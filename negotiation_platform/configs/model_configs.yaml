# GPU configuration for model comparison: SWAPPED - 8B vs 3B Llama models
#mistralai/Voxtral-Mini-3B-2507
#mistralai/Mistral-7B-Instruct-v0.3
#meta-llama/Llama-3.1-8B-Instruct
#meta-llama/Llama-3.2-3B-Instruct
#Qwen/Qwen2.5-3B
#Qwen/Qwen2.5-7B
# Testing for positional bias by swapping model assignments
model_a:
  type: "huggingface"
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  config:
    device: "cuda"
    device_map: "auto"
    temperature: 0.7
    max_length: 16
    do_sample: true
    trust_remote_code: true
    load_in_8bit: false
    

# 7B Mistral model for comparison (now in model_b position)
model_b:
  type: "huggingface"
  model_name: "Qwen/Qwen2.5-7B"
  config:
    device: "cuda"
    device_map: "auto"
    temperature: 0.7
    max_length: 16
    do_sample: true
    trust_remote_code: true
    load_in_8bit: false

model_c:
  type: "huggingface"
  model_name: "meta-llama/Llama-3.2-3B-Instruct"
  config:
    device: "cuda"
    device_map: "auto"
    temperature: 0.0
    max_length: 16
    do_sample: false
    pad_token_id: 50256
    trust_remote_code: true
    load_in_8bit: false