model_a:
  type: "huggingface"
  model_name: "meta-llama/Llama-3.1-8B-Instruct"  # Perfect for L40 GPUs
  config:
    device: "auto"  # Will use GPU automatically
    temperature: 0.0
    max_length: 512
    do_sample: false
    pad_token_id: 50256
    trust_remote_code: true
    load_in_8bit: true  # Should work properly on GPU
    api_token: "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"  # Add your HF token here

model_b:
  type: "huggingface"
  model_name: "meta-llama/Llama-3.1-8B-Instruct"  # Same model as A, different personality
  config:
    device: "auto"  # Will use GPU automatically
    temperature: 0.0  # Very conservative negotiator
    max_length: 512
    do_sample: false
    pad_token_id: 50256
    trust_remote_code: true
    load_in_8bit: true  # GPU quantization
    api_token: "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"  # Add your HF token here


model_c:
  type: "huggingface"
  model_name: "meta-llama/Llama-3.1-8B-Instruct"  # Same model as A, different personality
  config:
    device: "auto"  # Will use GPU automatically
    temperature: 0.0  # Very conservative negotiator
    max_length: 512
    do_sample: false
    pad_token_id: 50256
    trust_remote_code: true
    load_in_8bit: true  # GPU quantization
    api_token: "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"  # Add your HF token here
