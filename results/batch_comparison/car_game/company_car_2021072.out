=== Company Car Game - 10 Iterations ===
Job ID: 2021072, Node: jn117
=== Environment Check ===
CUDA: True, GPU: NVIDIA L40
NVIDIA L40, 49140 MiB, 1 MiB
=== Verifying Torch Scope Fix ===
âœ… Torch scope fix is present
=== Running Company Car Game - 10 Iterations ===
Game: Employee vs Manager company car negotiation

=== Iteration 1/300 ===
Thu Nov 13 01:10:17 PM CET 2025: Starting company car negotiation iteration 1 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 2.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 45.14s (total since entry 47.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.63s (total since entry 9.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,100 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 279.22 (saved â‚¬279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1661.22 (profit â‚¬1661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40058.45, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=279.22, model_a surplus=1661.22
âœ… Calculated utility_surplus: {'model_a': 1661.2249999999985, 'model_b': 279.22499999999854}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 40058.45, 'model_a': 39500.0}
Metrics: {'utility_surplus': {'model_a': 1661.2249999999985, 'model_b': 279.22499999999854}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 1 completed successfully
Thu Nov 13 01:11:22 PM CET 2025: Completed iteration 1

=== Iteration 2/300 ===
Thu Nov 13 01:11:22 PM CET 2025: Starting company car negotiation iteration 2 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.03s (total since entry 11.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,200 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 2 completed successfully
Thu Nov 13 01:11:52 PM CET 2025: Completed iteration 2

=== Iteration 3/300 ===
Thu Nov 13 01:11:52 PM CET 2025: Starting company car negotiation iteration 3 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ’¡ Player model_b made offer â‚¬38,200 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,900 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37900
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 694.80 (saved â‚¬694.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1187.87 (profit â‚¬1187.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39289.60, seller=37900.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=694.80, model_b surplus=1187.87
âœ… Calculated utility_surplus: {'model_a': 694.7985756249982, 'model_b': 1187.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39289.59715125, 'model_b': 37900.0}
Metrics: {'utility_surplus': {'model_a': 694.7985756249982, 'model_b': 1187.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 3 completed successfully
Thu Nov 13 01:12:22 PM CET 2025: Completed iteration 3

=== Iteration 4/300 ===
Thu Nov 13 01:12:22 PM CET 2025: Starting company car negotiation iteration 4 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.07s (total since entry 10.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.31s (total since entry 8.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38200}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38200}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38200}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,200 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 4 completed successfully
Thu Nov 13 01:12:51 PM CET 2025: Completed iteration 4

=== Iteration 5/300 ===
Thu Nov 13 01:12:51 PM CET 2025: Starting company car negotiation iteration 5 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38300}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38300}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38300}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,300 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,300 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38300
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 294.80 (saved â‚¬294.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1587.87 (profit â‚¬1587.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38889.60, seller=38300.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=294.80, model_b surplus=1587.87
âœ… Calculated utility_surplus: {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38889.59715125, 'model_b': 38300.0}
Metrics: {'utility_surplus': {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 5 completed successfully
Thu Nov 13 01:13:21 PM CET 2025: Completed iteration 5

=== Iteration 6/300 ===
Thu Nov 13 01:13:21 PM CET 2025: Starting company car negotiation iteration 6 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.43s (total since entry 10.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 6 completed successfully
Thu Nov 13 01:13:48 PM CET 2025: Completed iteration 6

=== Iteration 7/300 ===
Thu Nov 13 01:13:48 PM CET 2025: Starting company car negotiation iteration 7 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.18s (total since entry 10.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 37500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 37500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 37500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬37,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 7 completed successfully
Thu Nov 13 01:14:17 PM CET 2025: Completed iteration 7

=== Iteration 8/300 ===
Thu Nov 13 01:14:17 PM CET 2025: Starting company car negotiation iteration 8 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39779.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38900}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38900}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38900}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 8 completed successfully
Thu Nov 13 01:14:45 PM CET 2025: Completed iteration 8

=== Iteration 9/300 ===
Thu Nov 13 01:14:45 PM CET 2025: Starting company car negotiation iteration 9 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,450 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 9 completed successfully
Thu Nov 13 01:15:14 PM CET 2025: Completed iteration 9

=== Iteration 10/300 ===
Thu Nov 13 01:15:14 PM CET 2025: Starting company car negotiation iteration 10 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 10 completed successfully
Thu Nov 13 01:15:42 PM CET 2025: Completed iteration 10

=== Iteration 11/300 ===
Thu Nov 13 01:15:42 PM CET 2025: Starting company car negotiation iteration 11 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1379.22, model_a surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41158.45, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 11 completed successfully
Thu Nov 13 01:16:10 PM CET 2025: Completed iteration 11

=== Iteration 12/300 ===
Thu Nov 13 01:16:10 PM CET 2025: Starting company car negotiation iteration 12 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.05s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
âœ… Player model_a accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 12 completed successfully
Thu Nov 13 01:16:39 PM CET 2025: Completed iteration 12

=== Iteration 13/300 ===
Thu Nov 13 01:16:39 PM CET 2025: Starting company car negotiation iteration 13 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.48s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 13 completed successfully
Thu Nov 13 01:17:10 PM CET 2025: Completed iteration 13

=== Iteration 14/300 ===
Thu Nov 13 01:17:10 PM CET 2025: Starting company car negotiation iteration 14 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 14 completed successfully
Thu Nov 13 01:17:38 PM CET 2025: Completed iteration 14

=== Iteration 15/300 ===
Thu Nov 13 01:17:38 PM CET 2025: Starting company car negotiation iteration 15 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1279.22, model_a surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 661.2249999999985, 'model_b': 1279.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41058.45, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 661.2249999999985, 'model_b': 1279.2249999999985}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 15 completed successfully
Thu Nov 13 01:18:06 PM CET 2025: Completed iteration 15

=== Iteration 16/300 ===
Thu Nov 13 01:18:06 PM CET 2025: Starting company car negotiation iteration 16 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 16 completed successfully
Thu Nov 13 01:18:35 PM CET 2025: Completed iteration 16

=== Iteration 17/300 ===
Thu Nov 13 01:18:35 PM CET 2025: Starting company car negotiation iteration 17 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.62s (total since entry 9.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 17 completed successfully
Thu Nov 13 01:19:05 PM CET 2025: Completed iteration 17

=== Iteration 18/300 ===
Thu Nov 13 01:19:05 PM CET 2025: Starting company car negotiation iteration 18 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 18 completed successfully
Thu Nov 13 01:19:32 PM CET 2025: Completed iteration 18

=== Iteration 19/300 ===
Thu Nov 13 01:19:32 PM CET 2025: Starting company car negotiation iteration 19 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 19 completed successfully
Thu Nov 13 01:19:59 PM CET 2025: Completed iteration 19

=== Iteration 20/300 ===
Thu Nov 13 01:19:59 PM CET 2025: Starting company car negotiation iteration 20 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 20 completed successfully
Thu Nov 13 01:20:27 PM CET 2025: Completed iteration 20

=== Iteration 21/300 ===
Thu Nov 13 01:20:27 PM CET 2025: Starting company car negotiation iteration 21 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 21 completed successfully
Thu Nov 13 01:20:56 PM CET 2025: Completed iteration 21

=== Iteration 22/300 ===
Thu Nov 13 01:20:56 PM CET 2025: Starting company car negotiation iteration 22 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.24s (total since entry 10.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 22 completed successfully
Thu Nov 13 01:21:27 PM CET 2025: Completed iteration 22

=== Iteration 23/300 ===
Thu Nov 13 01:21:27 PM CET 2025: Starting company car negotiation iteration 23 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 23 completed successfully
Thu Nov 13 01:21:56 PM CET 2025: Completed iteration 23

=== Iteration 24/300 ===
Thu Nov 13 01:21:56 PM CET 2025: Starting company car negotiation iteration 24 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 0.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 24 completed successfully
Thu Nov 13 01:22:25 PM CET 2025: Completed iteration 24

=== Iteration 25/300 ===
Thu Nov 13 01:22:25 PM CET 2025: Starting company car negotiation iteration 25 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38200}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38200}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38200}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,200 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,200 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38200
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 394.80 (saved â‚¬394.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1487.87 (profit â‚¬1487.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38989.60, seller=38200.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=394.80, model_b surplus=1487.87
âœ… Calculated utility_surplus: {'model_a': 394.7985756249982, 'model_b': 1487.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38989.59715125, 'model_b': 38200.0}
Metrics: {'utility_surplus': {'model_a': 394.7985756249982, 'model_b': 1487.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 25 completed successfully
Thu Nov 13 01:22:54 PM CET 2025: Completed iteration 25

=== Iteration 26/300 ===
Thu Nov 13 01:22:54 PM CET 2025: Starting company car negotiation iteration 26 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.09s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,750 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -155.20 (saved â‚¬-155.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2037.87 (profit â‚¬2037.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38439.60, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-155.20, model_a surplus=2037.87
âœ… Calculated utility_surplus: {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38439.59715125, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 26 completed successfully
Thu Nov 13 01:23:23 PM CET 2025: Completed iteration 26

=== Iteration 27/300 ===
Thu Nov 13 01:23:23 PM CET 2025: Starting company car negotiation iteration 27 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 27 completed successfully
Thu Nov 13 01:23:54 PM CET 2025: Completed iteration 27

=== Iteration 28/300 ===
Thu Nov 13 01:23:54 PM CET 2025: Starting company car negotiation iteration 28 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 28 completed successfully
Thu Nov 13 01:24:23 PM CET 2025: Completed iteration 28

=== Iteration 29/300 ===
Thu Nov 13 01:24:23 PM CET 2025: Starting company car negotiation iteration 29 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 29 completed successfully
Thu Nov 13 01:24:53 PM CET 2025: Completed iteration 29

=== Iteration 30/300 ===
Thu Nov 13 01:24:53 PM CET 2025: Starting company car negotiation iteration 30 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 30 completed successfully
Thu Nov 13 01:25:21 PM CET 2025: Completed iteration 30

=== Iteration 31/300 ===
Thu Nov 13 01:25:21 PM CET 2025: Starting company car negotiation iteration 31 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.48s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 31 completed successfully
Thu Nov 13 01:25:50 PM CET 2025: Completed iteration 31

=== Iteration 32/300 ===
Thu Nov 13 01:25:50 PM CET 2025: Starting company car negotiation iteration 32 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 37750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 37750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 37750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37750.0}
ğŸ’¡ Player model_b made offer â‚¬37,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬36,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=36000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2594.80 (saved â‚¬2594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: -712.13 (profit â‚¬-712.13 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41189.60, seller=36000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=2594.80, model_b surplus=-712.13
âœ… Calculated utility_surplus: {'model_a': 2594.798575624998, 'model_b': -712.1254743750033}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 41189.59715125, 'model_b': 36000.0}
Metrics: {'utility_surplus': {'model_a': 2594.798575624998, 'model_b': -712.1254743750033}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 32 completed successfully
Thu Nov 13 01:26:20 PM CET 2025: Completed iteration 32

=== Iteration 33/300 ===
Thu Nov 13 01:26:20 PM CET 2025: Starting company car negotiation iteration 33 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39750.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 33 completed successfully
Thu Nov 13 01:26:51 PM CET 2025: Completed iteration 33

=== Iteration 34/300 ===
Thu Nov 13 01:26:51 PM CET 2025: Starting company car negotiation iteration 34 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 37800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 37800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 37800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37800.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬37,800 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 34 completed successfully
Thu Nov 13 01:27:20 PM CET 2025: Completed iteration 34

=== Iteration 35/300 ===
Thu Nov 13 01:27:20 PM CET 2025: Starting company car negotiation iteration 35 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 35 completed successfully
Thu Nov 13 01:27:49 PM CET 2025: Completed iteration 35

=== Iteration 36/300 ===
Thu Nov 13 01:27:49 PM CET 2025: Starting company car negotiation iteration 36 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.50s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 36 completed successfully
Thu Nov 13 01:28:16 PM CET 2025: Completed iteration 36

=== Iteration 37/300 ===
Thu Nov 13 01:28:16 PM CET 2025: Starting company car negotiation iteration 37 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.10s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,100 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 37 completed successfully
Thu Nov 13 01:28:45 PM CET 2025: Completed iteration 37

=== Iteration 38/300 ===
Thu Nov 13 01:28:45 PM CET 2025: Starting company car negotiation iteration 38 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.10s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38250\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38250\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38250\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,250 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,499 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38499
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 95.80 (saved â‚¬95.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1786.87 (profit â‚¬1786.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38690.60, seller=38499.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=95.80, model_b surplus=1786.87
âœ… Calculated utility_surplus: {'model_a': 95.7985756249982, 'model_b': 1786.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38690.59715125, 'model_b': 38499.0}
Metrics: {'utility_surplus': {'model_a': 95.7985756249982, 'model_b': 1786.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 38 completed successfully
Thu Nov 13 01:29:15 PM CET 2025: Completed iteration 38

=== Iteration 39/300 ===
Thu Nov 13 01:29:15 PM CET 2025: Starting company car negotiation iteration 39 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 39 completed successfully
Thu Nov 13 01:29:41 PM CET 2025: Completed iteration 39

=== Iteration 40/300 ===
Thu Nov 13 01:29:41 PM CET 2025: Starting company car negotiation iteration 40 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40385\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40385\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40385\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40385}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40385}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40385.0}
ğŸ’¡ Player model_b made offer â‚¬40,385 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39779.0}
ğŸ’¡ Player model_b made offer â‚¬39,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1094.80, model_a surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 787.8745256249967, 'model_b': 1094.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 39689.59715125, 'model_a': 37500.0}
Metrics: {'utility_surplus': {'model_a': 787.8745256249967, 'model_b': 1094.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 40 completed successfully
Thu Nov 13 01:30:10 PM CET 2025: Completed iteration 40

=== Iteration 41/300 ===
Thu Nov 13 01:30:10 PM CET 2025: Starting company car negotiation iteration 41 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 41 completed successfully
Thu Nov 13 01:30:39 PM CET 2025: Completed iteration 41

=== Iteration 42/300 ===
Thu Nov 13 01:30:39 PM CET 2025: Starting company car negotiation iteration 42 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.28s (total since entry 11.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39700}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39700}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39700}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39700.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,700 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38900}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38900}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38900}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 42 completed successfully
Thu Nov 13 01:31:08 PM CET 2025: Completed iteration 42

=== Iteration 43/300 ===
Thu Nov 13 01:31:08 PM CET 2025: Starting company car negotiation iteration 43 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.19s (total since entry 10.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38761}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38761}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38761}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38761}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38761}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38761.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,761 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 43 completed successfully
Thu Nov 13 01:31:37 PM CET 2025: Completed iteration 43

=== Iteration 44/300 ===
Thu Nov 13 01:31:37 PM CET 2025: Starting company car negotiation iteration 44 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.85s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 44 completed successfully
Thu Nov 13 01:32:05 PM CET 2025: Completed iteration 44

=== Iteration 45/300 ===
Thu Nov 13 01:32:05 PM CET 2025: Starting company car negotiation iteration 45 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.17s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 37999\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 37999\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 37999\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬37,999 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 45 completed successfully
Thu Nov 13 01:32:35 PM CET 2025: Completed iteration 45

=== Iteration 46/300 ===
Thu Nov 13 01:32:35 PM CET 2025: Starting company car negotiation iteration 46 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.10s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38450\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38450\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38450\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,450 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 46 completed successfully
Thu Nov 13 01:33:03 PM CET 2025: Completed iteration 46

=== Iteration 47/300 ===
Thu Nov 13 01:33:03 PM CET 2025: Starting company car negotiation iteration 47 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.56s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 47 completed successfully
Thu Nov 13 01:33:33 PM CET 2025: Completed iteration 47

=== Iteration 48/300 ===
Thu Nov 13 01:33:33 PM CET 2025: Starting company car negotiation iteration 48 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.08s (total since entry 10.79s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.34s (total since entry 8.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 48 completed successfully
Thu Nov 13 01:34:01 PM CET 2025: Completed iteration 48

=== Iteration 49/300 ===
Thu Nov 13 01:34:01 PM CET 2025: Starting company car negotiation iteration 49 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬39,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=682.54, model_a surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39865.07324999999, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 49 completed successfully
Thu Nov 13 01:34:30 PM CET 2025: Completed iteration 49

=== Iteration 50/300 ===
Thu Nov 13 01:34:30 PM CET 2025: Starting company car negotiation iteration 50 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.38s (total since entry 9.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 50 completed successfully
Thu Nov 13 01:35:00 PM CET 2025: Completed iteration 50

=== Iteration 51/300 ===
Thu Nov 13 01:35:00 PM CET 2025: Starting company car negotiation iteration 51 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.10s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 51 completed successfully
Thu Nov 13 01:35:29 PM CET 2025: Completed iteration 51

=== Iteration 52/300 ===
Thu Nov 13 01:35:29 PM CET 2025: Starting company car negotiation iteration 52 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.19s (total since entry 10.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 52 completed successfully
Thu Nov 13 01:35:59 PM CET 2025: Completed iteration 52

=== Iteration 53/300 ===
Thu Nov 13 01:35:59 PM CET 2025: Starting company car negotiation iteration 53 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 53 completed successfully
Thu Nov 13 01:36:26 PM CET 2025: Completed iteration 53

=== Iteration 54/300 ===
Thu Nov 13 01:36:26 PM CET 2025: Starting company car negotiation iteration 54 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.87s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.02s (total since entry 10.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.20s (total since entry 9.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38495}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38495}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38495}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38495}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38495}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38495.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,495 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,495 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38495
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 99.80 (saved â‚¬99.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1782.87 (profit â‚¬1782.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38694.60, seller=38495.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=99.80, model_b surplus=1782.87
âœ… Calculated utility_surplus: {'model_a': 99.7985756249982, 'model_b': 1782.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38694.59715125, 'model_b': 38495.0}
Metrics: {'utility_surplus': {'model_a': 99.7985756249982, 'model_b': 1782.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 54 completed successfully
Thu Nov 13 01:36:57 PM CET 2025: Completed iteration 54

=== Iteration 55/300 ===
Thu Nov 13 01:36:57 PM CET 2025: Starting company car negotiation iteration 55 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.23s (total since entry 10.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.34s (total since entry 8.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 55 completed successfully
Thu Nov 13 01:37:26 PM CET 2025: Completed iteration 55

=== Iteration 56/300 ===
Thu Nov 13 01:37:26 PM CET 2025: Starting company car negotiation iteration 56 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,200 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 56 completed successfully
Thu Nov 13 01:37:55 PM CET 2025: Completed iteration 56

=== Iteration 57/300 ===
Thu Nov 13 01:37:55 PM CET 2025: Starting company car negotiation iteration 57 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.10s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 57 completed successfully
Thu Nov 13 01:38:24 PM CET 2025: Completed iteration 57

=== Iteration 58/300 ===
Thu Nov 13 01:38:24 PM CET 2025: Starting company car negotiation iteration 58 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 9.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38200}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38200}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38200}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,200 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 58 completed successfully
Thu Nov 13 01:38:52 PM CET 2025: Completed iteration 58

=== Iteration 59/300 ===
Thu Nov 13 01:38:52 PM CET 2025: Starting company car negotiation iteration 59 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37,500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37,500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37,500}'
âš ï¸ [DEBUG] [model_a] JSON decode error for start match: Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] JSON decode error for '{"type": "offer", "price": 37,500}...': Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] JSON decode error for '{"type": "offer", "price": 37,500}...': Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] Extracted JSON decode error: Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âœ… [DEBUG] [model_a] Manually extracted action: {'type': 'offer'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 59 completed successfully
Thu Nov 13 01:39:22 PM CET 2025: Completed iteration 59

=== Iteration 60/300 ===
Thu Nov 13 01:39:22 PM CET 2025: Starting company car negotiation iteration 60 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.09s (total since entry 10.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-5.20, model_b surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38589.59715125, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 60 completed successfully
Thu Nov 13 01:39:51 PM CET 2025: Completed iteration 60

=== Iteration 61/300 ===
Thu Nov 13 01:39:51 PM CET 2025: Starting company car negotiation iteration 61 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,900 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37900
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 694.80 (saved â‚¬694.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1187.87 (profit â‚¬1187.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39289.60, seller=37900.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=694.80, model_b surplus=1187.87
âœ… Calculated utility_surplus: {'model_a': 694.7985756249982, 'model_b': 1187.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39289.59715125, 'model_b': 37900.0}
Metrics: {'utility_surplus': {'model_a': 694.7985756249982, 'model_b': 1187.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 61 completed successfully
Thu Nov 13 01:40:21 PM CET 2025: Completed iteration 61

=== Iteration 62/300 ===
Thu Nov 13 01:40:21 PM CET 2025: Starting company car negotiation iteration 62 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.09s (total since entry 10.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.38s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 62 completed successfully
Thu Nov 13 01:40:49 PM CET 2025: Completed iteration 62

=== Iteration 63/300 ===
Thu Nov 13 01:40:49 PM CET 2025: Starting company car negotiation iteration 63 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1379.22, model_a surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41158.45, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 63 completed successfully
Thu Nov 13 01:41:17 PM CET 2025: Completed iteration 63

=== Iteration 64/300 ===
Thu Nov 13 01:41:17 PM CET 2025: Starting company car negotiation iteration 64 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 64 completed successfully
Thu Nov 13 01:41:45 PM CET 2025: Completed iteration 64

=== Iteration 65/300 ===
Thu Nov 13 01:41:45 PM CET 2025: Starting company car negotiation iteration 65 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,500 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -905.20 (saved â‚¬-905.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2787.87 (profit â‚¬2787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37689.60, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-905.20, model_b surplus=2787.87
âœ… Calculated utility_surplus: {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 37689.59715125, 'model_b': 39500.0}
Metrics: {'utility_surplus': {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 65 completed successfully
Thu Nov 13 01:42:14 PM CET 2025: Completed iteration 65

=== Iteration 66/300 ===
Thu Nov 13 01:42:14 PM CET 2025: Starting company car negotiation iteration 66 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38999\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38999\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38999\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38999}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38999.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,999 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,750 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -155.20 (saved â‚¬-155.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2037.87 (profit â‚¬2037.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38439.60, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-155.20, model_a surplus=2037.87
âœ… Calculated utility_surplus: {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38439.59715125, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 66 completed successfully
Thu Nov 13 01:42:42 PM CET 2025: Completed iteration 66

=== Iteration 67/300 ===
Thu Nov 13 01:42:42 PM CET 2025: Starting company car negotiation iteration 67 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.07s (total since entry 10.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 67 completed successfully
Thu Nov 13 01:43:09 PM CET 2025: Completed iteration 67

=== Iteration 68/300 ===
Thu Nov 13 01:43:09 PM CET 2025: Starting company car negotiation iteration 68 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.17s (total since entry 10.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 68 completed successfully
Thu Nov 13 01:43:38 PM CET 2025: Completed iteration 68

=== Iteration 69/300 ===
Thu Nov 13 01:43:38 PM CET 2025: Starting company car negotiation iteration 69 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38888\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38888\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38888\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38888}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38888}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38888.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,888 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "reject"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "reject"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "reject"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'reject'}
[DEBUG] Parsed JSON: {'type': 'reject'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'reject'}
âŒ Player model_b rejected after reaching proposal limit (4/4)
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 69 completed successfully
Thu Nov 13 01:44:06 PM CET 2025: Completed iteration 69

=== Iteration 70/300 ===
Thu Nov 13 01:44:06 PM CET 2025: Starting company car negotiation iteration 70 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.13s (total since entry 10.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.53s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 70 completed successfully
Thu Nov 13 01:44:35 PM CET 2025: Completed iteration 70

=== Iteration 71/300 ===
Thu Nov 13 01:44:35 PM CET 2025: Starting company car negotiation iteration 71 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1379.22, model_a surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41158.45, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 71 completed successfully
Thu Nov 13 01:45:04 PM CET 2025: Completed iteration 71

=== Iteration 72/300 ===
Thu Nov 13 01:45:04 PM CET 2025: Starting company car negotiation iteration 72 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 72 completed successfully
Thu Nov 13 01:45:31 PM CET 2025: Completed iteration 72

=== Iteration 73/300 ===
Thu Nov 13 01:45:31 PM CET 2025: Starting company car negotiation iteration 73 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.19s (total since entry 10.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39250\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39250\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39250\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39250.0}
ğŸ’¡ Player model_b made offer â‚¬39,250 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38300}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38300}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38300}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,300 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,300 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38300
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 294.80 (saved â‚¬294.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1587.87 (profit â‚¬1587.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38889.60, seller=38300.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=294.80, model_b surplus=1587.87
âœ… Calculated utility_surplus: {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38889.59715125, 'model_b': 38300.0}
Metrics: {'utility_surplus': {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 73 completed successfully
Thu Nov 13 01:46:01 PM CET 2025: Completed iteration 73

=== Iteration 74/300 ===
Thu Nov 13 01:46:01 PM CET 2025: Starting company car negotiation iteration 74 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬37,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 74 completed successfully
Thu Nov 13 01:46:29 PM CET 2025: Completed iteration 74

=== Iteration 75/300 ===
Thu Nov 13 01:46:29 PM CET 2025: Starting company car negotiation iteration 75 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 885.00 (saved â‚¬885.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1085.00 (profit â‚¬1085.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41270.00, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=885.00, model_a surplus=1085.00
âœ… Calculated utility_surplus: {'model_a': 1085.0, 'model_b': 885.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41270.0, 'model_a': 39500.0}
Metrics: {'utility_surplus': {'model_a': 1085.0, 'model_b': 885.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 75 completed successfully
Thu Nov 13 01:46:56 PM CET 2025: Completed iteration 75

=== Iteration 76/300 ===
Thu Nov 13 01:46:56 PM CET 2025: Starting company car negotiation iteration 76 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬36,900 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,100 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 76 completed successfully
Thu Nov 13 01:47:25 PM CET 2025: Completed iteration 76

=== Iteration 77/300 ===
Thu Nov 13 01:47:25 PM CET 2025: Starting company car negotiation iteration 77 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.09s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 77 completed successfully
Thu Nov 13 01:47:54 PM CET 2025: Completed iteration 77

=== Iteration 78/300 ===
Thu Nov 13 01:47:54 PM CET 2025: Starting company car negotiation iteration 78 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.38s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38601\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38601\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38601\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38601}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38601}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38601.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,601 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38550\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38550\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38550\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 78 completed successfully
Thu Nov 13 01:48:23 PM CET 2025: Completed iteration 78

=== Iteration 79/300 ===
Thu Nov 13 01:48:23 PM CET 2025: Starting company car negotiation iteration 79 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.85s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-405.20, model_b surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38189.59715125, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 79 completed successfully
Thu Nov 13 01:48:52 PM CET 2025: Completed iteration 79

=== Iteration 80/300 ===
Thu Nov 13 01:48:52 PM CET 2025: Starting company car negotiation iteration 80 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.12s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 1: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=41000.00, seller=39000.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2000.00 (saved â‚¬2000.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 0.00 (profit â‚¬0.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=43000.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=2000.00, model_a surplus=0.00
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 2000.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 43000.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 2000.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 80 completed successfully
Thu Nov 13 01:49:18 PM CET 2025: Completed iteration 80

=== Iteration 81/300 ===
Thu Nov 13 01:49:18 PM CET 2025: Starting company car negotiation iteration 81 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 16.66s (total since entry 17.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 39.68s (total since entry 40.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 81 completed successfully
Thu Nov 13 01:50:23 PM CET 2025: Completed iteration 81

=== Iteration 82/300 ===
Thu Nov 13 01:50:23 PM CET 2025: Starting company car negotiation iteration 82 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.90s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 25.66s (total since entry 26.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.39s (total since entry 10.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 82 completed successfully
Thu Nov 13 01:51:13 PM CET 2025: Completed iteration 82

=== Iteration 83/300 ===
Thu Nov 13 01:51:13 PM CET 2025: Starting company car negotiation iteration 83 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 1.07s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.48s (total since entry 11.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-405.20, model_b surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38189.59715125, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 83 completed successfully
Thu Nov 13 01:51:43 PM CET 2025: Completed iteration 83

=== Iteration 84/300 ===
Thu Nov 13 01:51:43 PM CET 2025: Starting company car negotiation iteration 84 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 2/4)
âœ… Player model_b accepted offer of â‚¬38,450 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38450
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 144.80 (saved â‚¬144.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1737.87 (profit â‚¬1737.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38739.60, seller=38450.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=144.80, model_b surplus=1737.87
âœ… Calculated utility_surplus: {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38739.59715125, 'model_b': 38450.0}
Metrics: {'utility_surplus': {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 84 completed successfully
Thu Nov 13 01:52:11 PM CET 2025: Completed iteration 84

=== Iteration 85/300 ===
Thu Nov 13 01:52:11 PM CET 2025: Starting company car negotiation iteration 85 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 85 completed successfully
Thu Nov 13 01:52:40 PM CET 2025: Completed iteration 85

=== Iteration 86/300 ===
Thu Nov 13 01:52:40 PM CET 2025: Starting company car negotiation iteration 86 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,200 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-5.20, model_b surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38589.59715125, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 86 completed successfully
Thu Nov 13 01:53:07 PM CET 2025: Completed iteration 86

=== Iteration 87/300 ===
Thu Nov 13 01:53:07 PM CET 2025: Starting company car negotiation iteration 87 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,450 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 87 completed successfully
Thu Nov 13 01:53:36 PM CET 2025: Completed iteration 87

=== Iteration 88/300 ===
Thu Nov 13 01:53:36 PM CET 2025: Starting company car negotiation iteration 88 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 88 completed successfully
Thu Nov 13 01:54:06 PM CET 2025: Completed iteration 88

=== Iteration 89/300 ===
Thu Nov 13 01:54:06 PM CET 2025: Starting company car negotiation iteration 89 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38100.0}
ğŸ’¡ Player model_b made offer â‚¬38,100 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38271\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38271\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38271\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38271}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38271}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38271.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,271 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 89 completed successfully
Thu Nov 13 01:54:34 PM CET 2025: Completed iteration 89

=== Iteration 90/300 ===
Thu Nov 13 01:54:34 PM CET 2025: Starting company car negotiation iteration 90 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1379.22, model_b surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41158.45, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 90 completed successfully
Thu Nov 13 01:55:01 PM CET 2025: Completed iteration 90

=== Iteration 91/300 ===
Thu Nov 13 01:55:01 PM CET 2025: Starting company car negotiation iteration 91 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.89s (total since entry 10.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 91 completed successfully
Thu Nov 13 01:55:28 PM CET 2025: Completed iteration 91

=== Iteration 92/300 ===
Thu Nov 13 01:55:28 PM CET 2025: Starting company car negotiation iteration 92 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 8.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,750 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 432.54 (saved â‚¬432.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1478.81 (profit â‚¬1478.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39615.07, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=432.54, model_b surplus=1478.81
âœ… Calculated utility_surplus: {'model_a': 432.53662499999336, 'model_b': 1478.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39615.07324999999, 'model_b': 38750.0}
Metrics: {'utility_surplus': {'model_a': 432.53662499999336, 'model_b': 1478.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 92 completed successfully
Thu Nov 13 01:55:57 PM CET 2025: Completed iteration 92

=== Iteration 93/300 ===
Thu Nov 13 01:55:57 PM CET 2025: Starting company car negotiation iteration 93 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 93 completed successfully
Thu Nov 13 01:56:25 PM CET 2025: Completed iteration 93

=== Iteration 94/300 ===
Thu Nov 13 01:56:25 PM CET 2025: Starting company car negotiation iteration 94 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 94 completed successfully
Thu Nov 13 01:56:53 PM CET 2025: Completed iteration 94

=== Iteration 95/300 ===
Thu Nov 13 01:56:53 PM CET 2025: Starting company car negotiation iteration 95 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.92s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.59s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 95 completed successfully
Thu Nov 13 01:57:21 PM CET 2025: Completed iteration 95

=== Iteration 96/300 ===
Thu Nov 13 01:57:21 PM CET 2025: Starting company car negotiation iteration 96 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 8.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38401}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38401}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38401}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38401}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38401}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38401.0}
ğŸ’¡ Player model_b made offer â‚¬38,401 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 96 completed successfully
Thu Nov 13 01:57:49 PM CET 2025: Completed iteration 96

=== Iteration 97/300 ===
Thu Nov 13 01:57:49 PM CET 2025: Starting company car negotiation iteration 97 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39750.0}
ğŸ’¡ Player model_b made offer â‚¬39,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39200.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,200 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 97 completed successfully
Thu Nov 13 01:58:17 PM CET 2025: Completed iteration 97

=== Iteration 98/300 ===
Thu Nov 13 01:58:17 PM CET 2025: Starting company car negotiation iteration 98 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.88s (total since entry 10.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.48s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 98 completed successfully
Thu Nov 13 01:58:46 PM CET 2025: Completed iteration 98

=== Iteration 99/300 ===
Thu Nov 13 01:58:46 PM CET 2025: Starting company car negotiation iteration 99 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 99 completed successfully
Thu Nov 13 01:59:14 PM CET 2025: Completed iteration 99

=== Iteration 100/300 ===
Thu Nov 13 01:59:14 PM CET 2025: Starting company car negotiation iteration 100 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 100 completed successfully
Thu Nov 13 01:59:43 PM CET 2025: Completed iteration 100

=== Iteration 101/300 ===
Thu Nov 13 01:59:43 PM CET 2025: Starting company car negotiation iteration 101 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.76s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 101 completed successfully
Thu Nov 13 02:00:11 PM CET 2025: Completed iteration 101

=== Iteration 102/300 ===
Thu Nov 13 02:00:11 PM CET 2025: Starting company car negotiation iteration 102 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.66s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38200}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38200}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38200}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,200 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37800}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37800}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37800}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37800}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,800 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬37,800 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37800
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 794.80 (saved â‚¬794.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1087.87 (profit â‚¬1087.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39389.60, seller=37800.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=794.80, model_b surplus=1087.87
âœ… Calculated utility_surplus: {'model_a': 794.7985756249982, 'model_b': 1087.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39389.59715125, 'model_b': 37800.0}
Metrics: {'utility_surplus': {'model_a': 794.7985756249982, 'model_b': 1087.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 102 completed successfully
Thu Nov 13 02:00:40 PM CET 2025: Completed iteration 102

=== Iteration 103/300 ===
Thu Nov 13 02:00:40 PM CET 2025: Starting company car negotiation iteration 103 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 1.02s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=682.54, model_b surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39865.07324999999, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 103 completed successfully
Thu Nov 13 02:01:09 PM CET 2025: Completed iteration 103

=== Iteration 104/300 ===
Thu Nov 13 02:01:09 PM CET 2025: Starting company car negotiation iteration 104 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.99s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,450 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38450
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 144.80 (saved â‚¬144.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1737.87 (profit â‚¬1737.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38739.60, seller=38450.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=144.80, model_b surplus=1737.87
âœ… Calculated utility_surplus: {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38739.59715125, 'model_b': 38450.0}
Metrics: {'utility_surplus': {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 104 completed successfully
Thu Nov 13 02:01:39 PM CET 2025: Completed iteration 104

=== Iteration 105/300 ===
Thu Nov 13 02:01:39 PM CET 2025: Starting company car negotiation iteration 105 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 8.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 105 completed successfully
Thu Nov 13 02:02:06 PM CET 2025: Completed iteration 105

=== Iteration 106/300 ===
Thu Nov 13 02:02:06 PM CET 2025: Starting company car negotiation iteration 106 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 106 completed successfully
Thu Nov 13 02:02:35 PM CET 2025: Completed iteration 106

=== Iteration 107/300 ===
Thu Nov 13 02:02:35 PM CET 2025: Starting company car negotiation iteration 107 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.34s (total since entry 8.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38762\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38762\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38762\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38762}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38762}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38762.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,762 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 107 completed successfully
Thu Nov 13 02:03:02 PM CET 2025: Completed iteration 107

=== Iteration 108/300 ===
Thu Nov 13 02:03:02 PM CET 2025: Starting company car negotiation iteration 108 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 108 completed successfully
Thu Nov 13 02:03:29 PM CET 2025: Completed iteration 108

=== Iteration 109/300 ===
Thu Nov 13 02:03:29 PM CET 2025: Starting company car negotiation iteration 109 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.84s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38425}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38425}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38425}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38425}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38425}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38425.0}
ğŸ’¡ Player model_b made offer â‚¬38,425 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 109 completed successfully
Thu Nov 13 02:03:58 PM CET 2025: Completed iteration 109

=== Iteration 110/300 ===
Thu Nov 13 02:03:58 PM CET 2025: Starting company car negotiation iteration 110 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.80s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.62s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 110 completed successfully
Thu Nov 13 02:04:28 PM CET 2025: Completed iteration 110

=== Iteration 111/300 ===
Thu Nov 13 02:04:28 PM CET 2025: Starting company car negotiation iteration 111 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.91s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39250\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39250\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39250\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39250.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,250 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38100.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1379.22, model_b surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41158.45, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 111 completed successfully
Thu Nov 13 02:04:57 PM CET 2025: Completed iteration 111

=== Iteration 112/300 ===
Thu Nov 13 02:04:57 PM CET 2025: Starting company car negotiation iteration 112 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.89s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38,750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38,750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38,750\n}'
âš ï¸ [DEBUG] [model_b] JSON decode error for start match: Expecting property name enclosed in double quotes: line 3 column 15 (char 35)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
  "type": "offer",
  "price": 38,750
}...': Expecting property name enclosed in double quotes: line 3 column 15 (char 35)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
  "type": "offer",
  "price": 38,750
}...': Expecting property name enclosed in double quotes: line 3 column 15 (char 35)
âš ï¸ [DEBUG] [model_b] Extracted JSON decode error: Expecting property name enclosed in double quotes: line 3 column 15 (char 35)
âœ… [DEBUG] [model_b] Manually extracted action: {'type': 'offer'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 112 completed successfully
Thu Nov 13 02:05:27 PM CET 2025: Completed iteration 112

=== Iteration 113/300 ===
Thu Nov 13 02:05:27 PM CET 2025: Starting company car negotiation iteration 113 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 113 completed successfully
Thu Nov 13 02:05:55 PM CET 2025: Completed iteration 113

=== Iteration 114/300 ===
Thu Nov 13 02:05:55 PM CET 2025: Starting company car negotiation iteration 114 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.90s (total since entry 10.62s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,750 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 432.54 (saved â‚¬432.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1478.81 (profit â‚¬1478.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39615.07, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=432.54, model_a surplus=1478.81
âœ… Calculated utility_surplus: {'model_a': 1478.8066250000047, 'model_b': 432.53662499999336}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39615.07324999999, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 1478.8066250000047, 'model_b': 432.53662499999336}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 114 completed successfully
Thu Nov 13 02:06:25 PM CET 2025: Completed iteration 114

=== Iteration 115/300 ===
Thu Nov 13 02:06:25 PM CET 2025: Starting company car negotiation iteration 115 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38100}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38100}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38100}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,100 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 115 completed successfully
Thu Nov 13 02:06:54 PM CET 2025: Completed iteration 115

=== Iteration 116/300 ===
Thu Nov 13 02:06:54 PM CET 2025: Starting company car negotiation iteration 116 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 116 completed successfully
Thu Nov 13 02:07:23 PM CET 2025: Completed iteration 116

=== Iteration 117/300 ===
Thu Nov 13 02:07:23 PM CET 2025: Starting company car negotiation iteration 117 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,750 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1635.00 (saved â‚¬1635.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 335.00 (profit â‚¬335.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=42020.00, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1635.00, model_a surplus=335.00
âœ… Calculated utility_surplus: {'model_a': 335.0, 'model_b': 1635.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 42020.0, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 335.0, 'model_b': 1635.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 117 completed successfully
Thu Nov 13 02:07:49 PM CET 2025: Completed iteration 117

=== Iteration 118/300 ===
Thu Nov 13 02:07:49 PM CET 2025: Starting company car negotiation iteration 118 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 118 completed successfully
Thu Nov 13 02:08:18 PM CET 2025: Completed iteration 118

=== Iteration 119/300 ===
Thu Nov 13 02:08:18 PM CET 2025: Starting company car negotiation iteration 119 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.90s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.99s (total since entry 10.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37999}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37999}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37999}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,999 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,999 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37999
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 595.80 (saved â‚¬595.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1286.87 (profit â‚¬1286.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39190.60, seller=37999.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=595.80, model_b surplus=1286.87
âœ… Calculated utility_surplus: {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39190.59715125, 'model_b': 37999.0}
Metrics: {'utility_surplus': {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 119 completed successfully
Thu Nov 13 02:08:48 PM CET 2025: Completed iteration 119

=== Iteration 120/300 ===
Thu Nov 13 02:08:48 PM CET 2025: Starting company car negotiation iteration 120 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 120 completed successfully
Thu Nov 13 02:09:14 PM CET 2025: Completed iteration 120

=== Iteration 121/300 ===
Thu Nov 13 02:09:14 PM CET 2025: Starting company car negotiation iteration 121 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 121 completed successfully
Thu Nov 13 02:09:42 PM CET 2025: Completed iteration 121

=== Iteration 122/300 ===
Thu Nov 13 02:09:42 PM CET 2025: Starting company car negotiation iteration 122 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.99s (total since entry 10.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 122 completed successfully
Thu Nov 13 02:10:10 PM CET 2025: Completed iteration 122

=== Iteration 123/300 ===
Thu Nov 13 02:10:10 PM CET 2025: Starting company car negotiation iteration 123 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.85s (total since entry 10.68s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.59s (total since entry 9.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 123 completed successfully
Thu Nov 13 02:10:38 PM CET 2025: Completed iteration 123

=== Iteration 124/300 ===
Thu Nov 13 02:10:38 PM CET 2025: Starting company car negotiation iteration 124 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 124 completed successfully
Thu Nov 13 02:11:05 PM CET 2025: Completed iteration 124

=== Iteration 125/300 ===
Thu Nov 13 02:11:05 PM CET 2025: Starting company car negotiation iteration 125 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38764}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38764}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38764}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38764}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38764}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38764.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,764 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38,600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38,600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38,600\n}'
âš ï¸ [DEBUG] [model_b] JSON decode error for start match: Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
    "type": "offer",
    "price": 38,600
}...': Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
    "type": "offer",
    "price": 38,600
}...': Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] Extracted JSON decode error: Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âœ… [DEBUG] [model_b] Manually extracted action: {'type': 'offer'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 125 completed successfully
Thu Nov 13 02:11:34 PM CET 2025: Completed iteration 125

=== Iteration 126/300 ===
Thu Nov 13 02:11:34 PM CET 2025: Starting company car negotiation iteration 126 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.90s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38250\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38250\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38250\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,250 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 126 completed successfully
Thu Nov 13 02:12:03 PM CET 2025: Completed iteration 126

=== Iteration 127/300 ===
Thu Nov 13 02:12:03 PM CET 2025: Starting company car negotiation iteration 127 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.40s (total since entry 11.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.41s (total since entry 10.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-5.20, model_b surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38589.59715125, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 127 completed successfully
Thu Nov 13 02:12:34 PM CET 2025: Completed iteration 127

=== Iteration 128/300 ===
Thu Nov 13 02:12:34 PM CET 2025: Starting company car negotiation iteration 128 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.68s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 128 completed successfully
Thu Nov 13 02:13:03 PM CET 2025: Completed iteration 128

=== Iteration 129/300 ===
Thu Nov 13 02:13:03 PM CET 2025: Starting company car negotiation iteration 129 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 129 completed successfully
Thu Nov 13 02:13:29 PM CET 2025: Completed iteration 129

=== Iteration 130/300 ===
Thu Nov 13 02:13:29 PM CET 2025: Starting company car negotiation iteration 130 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.84s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 130 completed successfully
Thu Nov 13 02:13:58 PM CET 2025: Completed iteration 130

=== Iteration 131/300 ===
Thu Nov 13 02:13:58 PM CET 2025: Starting company car negotiation iteration 131 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 37500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 37500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 37500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬37,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,200 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 131 completed successfully
Thu Nov 13 02:14:27 PM CET 2025: Completed iteration 131

=== Iteration 132/300 ===
Thu Nov 13 02:14:27 PM CET 2025: Starting company car negotiation iteration 132 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.62s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 132 completed successfully
Thu Nov 13 02:14:57 PM CET 2025: Completed iteration 132

=== Iteration 133/300 ===
Thu Nov 13 02:14:57 PM CET 2025: Starting company car negotiation iteration 133 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.54s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39779.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 133 completed successfully
Thu Nov 13 02:15:24 PM CET 2025: Completed iteration 133

=== Iteration 134/300 ===
Thu Nov 13 02:15:24 PM CET 2025: Starting company car negotiation iteration 134 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-405.20, model_b surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38189.59715125, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': -405.2014243750018, 'model_b': 2287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 134 completed successfully
Thu Nov 13 02:15:52 PM CET 2025: Completed iteration 134

=== Iteration 135/300 ===
Thu Nov 13 02:15:52 PM CET 2025: Starting company car negotiation iteration 135 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.87s (total since entry 10.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.59s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1379.22, model_a surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41158.45, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 135 completed successfully
Thu Nov 13 02:16:20 PM CET 2025: Completed iteration 135

=== Iteration 136/300 ===
Thu Nov 13 02:16:20 PM CET 2025: Starting company car negotiation iteration 136 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 136 completed successfully
Thu Nov 13 02:16:48 PM CET 2025: Completed iteration 136

=== Iteration 137/300 ===
Thu Nov 13 02:16:48 PM CET 2025: Starting company car negotiation iteration 137 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.67s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37250}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 37250}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 37250}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 37250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 37250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬37,250 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1594.80, model_a surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 287.87452562499675, 'model_b': 1594.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 40189.59715125, 'model_a': 37000.0}
Metrics: {'utility_surplus': {'model_a': 287.87452562499675, 'model_b': 1594.7985756249982}, 'risk_minimization': {'model_a': 0.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 137 completed successfully
Thu Nov 13 02:17:16 PM CET 2025: Completed iteration 137

=== Iteration 138/300 ===
Thu Nov 13 02:17:16 PM CET 2025: Starting company car negotiation iteration 138 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 138 completed successfully
Thu Nov 13 02:17:44 PM CET 2025: Completed iteration 138

=== Iteration 139/300 ===
Thu Nov 13 02:17:44 PM CET 2025: Starting company car negotiation iteration 139 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.62s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 139 completed successfully
Thu Nov 13 02:18:10 PM CET 2025: Completed iteration 139

=== Iteration 140/300 ===
Thu Nov 13 02:18:10 PM CET 2025: Starting company car negotiation iteration 140 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38700}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38700}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38700}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 140 completed successfully
Thu Nov 13 02:18:39 PM CET 2025: Completed iteration 140

=== Iteration 141/300 ===
Thu Nov 13 02:18:39 PM CET 2025: Starting company car negotiation iteration 141 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.94s (total since entry 10.70s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 141 completed successfully
Thu Nov 13 02:19:08 PM CET 2025: Completed iteration 141

=== Iteration 142/300 ===
Thu Nov 13 02:19:08 PM CET 2025: Starting company car negotiation iteration 142 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.56s (total since entry 9.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 142 completed successfully
Thu Nov 13 02:19:35 PM CET 2025: Completed iteration 142

=== Iteration 143/300 ===
Thu Nov 13 02:19:35 PM CET 2025: Starting company car negotiation iteration 143 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,750 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -155.20 (saved â‚¬-155.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2037.87 (profit â‚¬2037.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38439.60, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-155.20, model_b surplus=2037.87
âœ… Calculated utility_surplus: {'model_a': -155.2014243750018, 'model_b': 2037.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38439.59715125, 'model_b': 38750.0}
Metrics: {'utility_surplus': {'model_a': -155.2014243750018, 'model_b': 2037.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 143 completed successfully
Thu Nov 13 02:20:03 PM CET 2025: Completed iteration 143

=== Iteration 144/300 ===
Thu Nov 13 02:20:03 PM CET 2025: Starting company car negotiation iteration 144 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 144 completed successfully
Thu Nov 13 02:20:32 PM CET 2025: Completed iteration 144

=== Iteration 145/300 ===
Thu Nov 13 02:20:32 PM CET 2025: Starting company car negotiation iteration 145 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 37,500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 37,500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 37,500\n}'
âš ï¸ [DEBUG] [model_b] JSON decode error for start match: Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
    "type": "offer",
    "price": 37,500
}...': Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] JSON decode error for '{
    "type": "offer",
    "price": 37,500
}...': Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âš ï¸ [DEBUG] [model_b] Extracted JSON decode error: Expecting property name enclosed in double quotes: line 3 column 17 (char 39)
âœ… [DEBUG] [model_b] Manually extracted action: {'type': 'offer'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38300\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38300\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38300\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,300 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38425}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38425}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38425}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38425}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38425}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38425.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,425 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 145 completed successfully
Thu Nov 13 02:21:02 PM CET 2025: Completed iteration 145

=== Iteration 146/300 ===
Thu Nov 13 02:21:02 PM CET 2025: Starting company car negotiation iteration 146 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.84s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 146 completed successfully
Thu Nov 13 02:21:32 PM CET 2025: Completed iteration 146

=== Iteration 147/300 ===
Thu Nov 13 02:21:32 PM CET 2025: Starting company car negotiation iteration 147 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬39,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 4: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 182.54 (saved â‚¬182.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1728.81 (profit â‚¬1728.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39365.07, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=182.54, model_a surplus=1728.81
âœ… Calculated utility_surplus: {'model_a': 1728.8066250000047, 'model_b': 182.53662499999336}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39365.07324999999, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1728.8066250000047, 'model_b': 182.53662499999336}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 147 completed successfully
Thu Nov 13 02:22:00 PM CET 2025: Completed iteration 147

=== Iteration 148/300 ===
Thu Nov 13 02:22:00 PM CET 2025: Starting company car negotiation iteration 148 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.77s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38700}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38700}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38700}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,499 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38499
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 95.80 (saved â‚¬95.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1786.87 (profit â‚¬1786.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38690.60, seller=38499.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=95.80, model_a surplus=1786.87
âœ… Calculated utility_surplus: {'model_a': 1786.8745256249967, 'model_b': 95.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38690.59715125, 'model_a': 38499.0}
Metrics: {'utility_surplus': {'model_a': 1786.8745256249967, 'model_b': 95.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 148 completed successfully
Thu Nov 13 02:22:30 PM CET 2025: Completed iteration 148

=== Iteration 149/300 ===
Thu Nov 13 02:22:30 PM CET 2025: Starting company car negotiation iteration 149 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.88s (total since entry 10.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39765\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39765\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39765\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39765}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39765}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39765.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,765 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "reject"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "reject"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "reject"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'reject'}
[DEBUG] Parsed JSON: {'type': 'reject'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'reject'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
âŒ Player model_b rejected after reaching proposal limit (4/4)
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 149 completed successfully
Thu Nov 13 02:22:58 PM CET 2025: Completed iteration 149

=== Iteration 150/300 ===
Thu Nov 13 02:22:58 PM CET 2025: Starting company car negotiation iteration 150 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 150 completed successfully
Thu Nov 13 02:23:26 PM CET 2025: Completed iteration 150

=== Iteration 151/300 ===
Thu Nov 13 02:23:26 PM CET 2025: Starting company car negotiation iteration 151 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.90s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.79s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.54s (total since entry 9.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1279.22, model_b surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41058.45, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 151 completed successfully
Thu Nov 13 02:23:55 PM CET 2025: Completed iteration 151

=== Iteration 152/300 ===
Thu Nov 13 02:23:55 PM CET 2025: Starting company car negotiation iteration 152 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.00s (total since entry 10.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=682.54, model_a surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39865.07324999999, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 152 completed successfully
Thu Nov 13 02:24:24 PM CET 2025: Completed iteration 152

=== Iteration 153/300 ===
Thu Nov 13 02:24:24 PM CET 2025: Starting company car negotiation iteration 153 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 153 completed successfully
Thu Nov 13 02:24:50 PM CET 2025: Completed iteration 153

=== Iteration 154/300 ===
Thu Nov 13 02:24:50 PM CET 2025: Starting company car negotiation iteration 154 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 154 completed successfully
Thu Nov 13 02:25:18 PM CET 2025: Completed iteration 154

=== Iteration 155/300 ===
Thu Nov 13 02:25:18 PM CET 2025: Starting company car negotiation iteration 155 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.87s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 155 completed successfully
Thu Nov 13 02:25:45 PM CET 2025: Completed iteration 155

=== Iteration 156/300 ===
Thu Nov 13 02:25:45 PM CET 2025: Starting company car negotiation iteration 156 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 156 completed successfully
Thu Nov 13 02:26:13 PM CET 2025: Completed iteration 156

=== Iteration 157/300 ===
Thu Nov 13 02:26:13 PM CET 2025: Starting company car negotiation iteration 157 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_a made offer â‚¬39,100 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "reject"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "reject"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "reject"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'reject'}
[DEBUG] Parsed JSON: {'type': 'reject'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'reject'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "reject"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "reject"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "reject"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'reject'}
[DEBUG] Parsed JSON: {'type': 'reject'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'reject'}
âŒ Player model_b rejected after reaching proposal limit (4/4)
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 157 completed successfully
Thu Nov 13 02:26:45 PM CET 2025: Completed iteration 157

=== Iteration 158/300 ===
Thu Nov 13 02:26:45 PM CET 2025: Starting company car negotiation iteration 158 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 1: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=41000.00, seller=39000.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2000.00 (saved â‚¬2000.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 0.00 (profit â‚¬0.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=43000.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=2000.00, model_a surplus=0.00
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 2000.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 43000.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 2000.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 158 completed successfully
Thu Nov 13 02:27:10 PM CET 2025: Completed iteration 158

=== Iteration 159/300 ===
Thu Nov 13 02:27:10 PM CET 2025: Starting company car negotiation iteration 159 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 159 completed successfully
Thu Nov 13 02:27:39 PM CET 2025: Completed iteration 159

=== Iteration 160/300 ===
Thu Nov 13 02:27:39 PM CET 2025: Starting company car negotiation iteration 160 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.98s (total since entry 10.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 41000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 41000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 41000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,500 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -905.20 (saved â‚¬-905.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2787.87 (profit â‚¬2787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37689.60, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-905.20, model_b surplus=2787.87
âœ… Calculated utility_surplus: {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 37689.59715125, 'model_b': 39500.0}
Metrics: {'utility_surplus': {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 160 completed successfully
Thu Nov 13 02:28:07 PM CET 2025: Completed iteration 160

=== Iteration 161/300 ===
Thu Nov 13 02:28:07 PM CET 2025: Starting company car negotiation iteration 161 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.80s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39200.0}
ğŸ’¡ Player model_b made offer â‚¬39,200 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38300\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38300\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38300\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,300 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 161 completed successfully
Thu Nov 13 02:28:37 PM CET 2025: Completed iteration 161

=== Iteration 162/300 ===
Thu Nov 13 02:28:37 PM CET 2025: Starting company car negotiation iteration 162 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.60s (total since entry 9.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,600 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬37,600 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 994.80 (saved â‚¬994.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 887.87 (profit â‚¬887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39589.60, seller=37600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=994.80, model_b surplus=887.87
âœ… Calculated utility_surplus: {'model_a': 994.7985756249982, 'model_b': 887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39589.59715125, 'model_b': 37600.0}
Metrics: {'utility_surplus': {'model_a': 994.7985756249982, 'model_b': 887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 162 completed successfully
Thu Nov 13 02:29:07 PM CET 2025: Completed iteration 162

=== Iteration 163/300 ===
Thu Nov 13 02:29:07 PM CET 2025: Starting company car negotiation iteration 163 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.91s (total since entry 10.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 163 completed successfully
Thu Nov 13 02:29:34 PM CET 2025: Completed iteration 163

=== Iteration 164/300 ===
Thu Nov 13 02:29:34 PM CET 2025: Starting company car negotiation iteration 164 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.85s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=682.54, model_b surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39865.07324999999, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 164 completed successfully
Thu Nov 13 02:30:02 PM CET 2025: Completed iteration 164

=== Iteration 165/300 ===
Thu Nov 13 02:30:02 PM CET 2025: Starting company car negotiation iteration 165 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.78s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 165 completed successfully
Thu Nov 13 02:30:30 PM CET 2025: Completed iteration 165

=== Iteration 166/300 ===
Thu Nov 13 02:30:30 PM CET 2025: Starting company car negotiation iteration 166 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.78s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.85s (total since entry 10.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.48s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 1: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=41000.00, seller=39000.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2000.00 (saved â‚¬2000.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 0.00 (profit â‚¬0.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=43000.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=2000.00, model_a surplus=0.00
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 2000.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 43000.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 2000.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 166 completed successfully
Thu Nov 13 02:30:55 PM CET 2025: Completed iteration 166

=== Iteration 167/300 ===
Thu Nov 13 02:30:55 PM CET 2025: Starting company car negotiation iteration 167 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.80s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 167 completed successfully
Thu Nov 13 02:31:23 PM CET 2025: Completed iteration 167

=== Iteration 168/300 ===
Thu Nov 13 02:31:23 PM CET 2025: Starting company car negotiation iteration 168 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,450 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 168 completed successfully
Thu Nov 13 02:31:51 PM CET 2025: Completed iteration 168

=== Iteration 169/300 ===
Thu Nov 13 02:31:51 PM CET 2025: Starting company car negotiation iteration 169 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 169 completed successfully
Thu Nov 13 02:32:20 PM CET 2025: Completed iteration 169

=== Iteration 170/300 ===
Thu Nov 13 02:32:20 PM CET 2025: Starting company car negotiation iteration 170 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.99s (total since entry 10.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -905.20 (saved â‚¬-905.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2787.87 (profit â‚¬2787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37689.60, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-905.20, model_a surplus=2787.87
âœ… Calculated utility_surplus: {'model_a': 2787.8745256249967, 'model_b': -905.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 37689.59715125, 'model_a': 39500.0}
Metrics: {'utility_surplus': {'model_a': 2787.8745256249967, 'model_b': -905.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 170 completed successfully
Thu Nov 13 02:32:50 PM CET 2025: Completed iteration 170

=== Iteration 171/300 ===
Thu Nov 13 02:32:50 PM CET 2025: Starting company car negotiation iteration 171 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.85s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.53s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,500 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -905.20 (saved â‚¬-905.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2787.87 (profit â‚¬2787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37689.60, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-905.20, model_b surplus=2787.87
âœ… Calculated utility_surplus: {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 37689.59715125, 'model_b': 39500.0}
Metrics: {'utility_surplus': {'model_a': -905.2014243750018, 'model_b': 2787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 171 completed successfully
Thu Nov 13 02:33:18 PM CET 2025: Completed iteration 171

=== Iteration 172/300 ===
Thu Nov 13 02:33:18 PM CET 2025: Starting company car negotiation iteration 172 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 172 completed successfully
Thu Nov 13 02:33:47 PM CET 2025: Completed iteration 172

=== Iteration 173/300 ===
Thu Nov 13 02:33:47 PM CET 2025: Starting company car negotiation iteration 173 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.45s (total since entry 11.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.24s (total since entry 9.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 173 completed successfully
Thu Nov 13 02:34:14 PM CET 2025: Completed iteration 173

=== Iteration 174/300 ===
Thu Nov 13 02:34:14 PM CET 2025: Starting company car negotiation iteration 174 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.85s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 174 completed successfully
Thu Nov 13 02:34:42 PM CET 2025: Completed iteration 174

=== Iteration 175/300 ===
Thu Nov 13 02:34:42 PM CET 2025: Starting company car negotiation iteration 175 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1379.22, model_a surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 41158.45, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 561.2249999999985, 'model_b': 1379.2249999999985}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 175 completed successfully
Thu Nov 13 02:35:09 PM CET 2025: Completed iteration 175

=== Iteration 176/300 ===
Thu Nov 13 02:35:09 PM CET 2025: Starting company car negotiation iteration 176 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 176 completed successfully
Thu Nov 13 02:35:38 PM CET 2025: Completed iteration 176

=== Iteration 177/300 ===
Thu Nov 13 02:35:38 PM CET 2025: Starting company car negotiation iteration 177 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 177 completed successfully
Thu Nov 13 02:36:07 PM CET 2025: Completed iteration 177

=== Iteration 178/300 ===
Thu Nov 13 02:36:07 PM CET 2025: Starting company car negotiation iteration 178 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 178 completed successfully
Thu Nov 13 02:36:36 PM CET 2025: Completed iteration 178

=== Iteration 179/300 ===
Thu Nov 13 02:36:36 PM CET 2025: Starting company car negotiation iteration 179 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,450 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38450
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 144.80 (saved â‚¬144.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1737.87 (profit â‚¬1737.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38739.60, seller=38450.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=144.80, model_b surplus=1737.87
âœ… Calculated utility_surplus: {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38739.59715125, 'model_b': 38450.0}
Metrics: {'utility_surplus': {'model_a': 144.7985756249982, 'model_b': 1737.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 179 completed successfully
Thu Nov 13 02:37:05 PM CET 2025: Completed iteration 179

=== Iteration 180/300 ===
Thu Nov 13 02:37:05 PM CET 2025: Starting company car negotiation iteration 180 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.89s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 180 completed successfully
Thu Nov 13 02:37:34 PM CET 2025: Completed iteration 180

=== Iteration 181/300 ===
Thu Nov 13 02:37:34 PM CET 2025: Starting company car negotiation iteration 181 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.02s (total since entry 10.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 181 completed successfully
Thu Nov 13 02:38:00 PM CET 2025: Completed iteration 181

=== Iteration 182/300 ===
Thu Nov 13 02:38:00 PM CET 2025: Starting company car negotiation iteration 182 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 182 completed successfully
Thu Nov 13 02:38:27 PM CET 2025: Completed iteration 182

=== Iteration 183/300 ===
Thu Nov 13 02:38:27 PM CET 2025: Starting company car negotiation iteration 183 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 1: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=41000.00, seller=39000.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2000.00 (saved â‚¬2000.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 0.00 (profit â‚¬0.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=43000.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=2000.00, model_a surplus=0.00
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 2000.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 43000.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 2000.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 183 completed successfully
Thu Nov 13 02:38:53 PM CET 2025: Completed iteration 183

=== Iteration 184/300 ===
Thu Nov 13 02:38:53 PM CET 2025: Starting company car negotiation iteration 184 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 184 completed successfully
Thu Nov 13 02:39:19 PM CET 2025: Completed iteration 184

=== Iteration 185/300 ===
Thu Nov 13 02:39:19 PM CET 2025: Starting company car negotiation iteration 185 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.43s (total since entry 11.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.25s (total since entry 9.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 185 completed successfully
Thu Nov 13 02:39:47 PM CET 2025: Completed iteration 185

=== Iteration 186/300 ===
Thu Nov 13 02:39:47 PM CET 2025: Starting company car negotiation iteration 186 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 8.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 1: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=41000.00, seller=39000.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2000.00 (saved â‚¬2000.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 0.00 (profit â‚¬0.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=43000.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=2000.00, model_b surplus=0.00
âœ… Calculated utility_surplus: {'model_a': 2000.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_a': 43000.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2000.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 186 completed successfully
Thu Nov 13 02:40:12 PM CET 2025: Completed iteration 186

=== Iteration 187/300 ===
Thu Nov 13 02:40:12 PM CET 2025: Starting company car negotiation iteration 187 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39200.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,200 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬36,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=36000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 2594.80 (saved â‚¬2594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: -712.13 (profit â‚¬-712.13 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41189.60, seller=36000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=2594.80, model_b surplus=-712.13
âœ… Calculated utility_surplus: {'model_a': 2594.798575624998, 'model_b': -712.1254743750033}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 41189.59715125, 'model_b': 36000.0}
Metrics: {'utility_surplus': {'model_a': 2594.798575624998, 'model_b': -712.1254743750033}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 187 completed successfully
Thu Nov 13 02:40:41 PM CET 2025: Completed iteration 187

=== Iteration 188/300 ===
Thu Nov 13 02:40:41 PM CET 2025: Starting company car negotiation iteration 188 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 188 completed successfully
Thu Nov 13 02:41:08 PM CET 2025: Completed iteration 188

=== Iteration 189/300 ===
Thu Nov 13 02:41:08 PM CET 2025: Starting company car negotiation iteration 189 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37999}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37999}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37999}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,999 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 189 completed successfully
Thu Nov 13 02:41:37 PM CET 2025: Completed iteration 189

=== Iteration 190/300 ===
Thu Nov 13 02:41:37 PM CET 2025: Starting company car negotiation iteration 190 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.81s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=682.54, model_a surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39865.07324999999, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1228.8066250000047, 'model_b': 682.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 190 completed successfully
Thu Nov 13 02:42:04 PM CET 2025: Completed iteration 190

=== Iteration 191/300 ===
Thu Nov 13 02:42:04 PM CET 2025: Starting company car negotiation iteration 191 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.76s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 0.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 191 completed successfully
Thu Nov 13 02:42:33 PM CET 2025: Completed iteration 191

=== Iteration 192/300 ===
Thu Nov 13 02:42:33 PM CET 2025: Starting company car negotiation iteration 192 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.37s (total since entry 9.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-5.20, model_b surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38589.59715125, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 192 completed successfully
Thu Nov 13 02:43:01 PM CET 2025: Completed iteration 192

=== Iteration 193/300 ===
Thu Nov 13 02:43:01 PM CET 2025: Starting company car negotiation iteration 193 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 193 completed successfully
Thu Nov 13 02:43:28 PM CET 2025: Completed iteration 193

=== Iteration 194/300 ===
Thu Nov 13 02:43:28 PM CET 2025: Starting company car negotiation iteration 194 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 194 completed successfully
Thu Nov 13 02:43:56 PM CET 2025: Completed iteration 194

=== Iteration 195/300 ===
Thu Nov 13 02:43:56 PM CET 2025: Starting company car negotiation iteration 195 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37999}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37999}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37999}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,999 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,999 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37999
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 595.80 (saved â‚¬595.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1286.87 (profit â‚¬1286.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39190.60, seller=37999.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=595.80, model_b surplus=1286.87
âœ… Calculated utility_surplus: {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39190.59715125, 'model_b': 37999.0}
Metrics: {'utility_surplus': {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 195 completed successfully
Thu Nov 13 02:44:25 PM CET 2025: Completed iteration 195

=== Iteration 196/300 ===
Thu Nov 13 02:44:25 PM CET 2025: Starting company car negotiation iteration 196 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.38s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=682.54, model_b surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39865.07324999999, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 196 completed successfully
Thu Nov 13 02:44:54 PM CET 2025: Completed iteration 196

=== Iteration 197/300 ===
Thu Nov 13 02:44:54 PM CET 2025: Starting company car negotiation iteration 197 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.82s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 197 completed successfully
Thu Nov 13 02:45:24 PM CET 2025: Completed iteration 197

=== Iteration 198/300 ===
Thu Nov 13 02:45:24 PM CET 2025: Starting company car negotiation iteration 198 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 41000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 41000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 41000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 198 completed successfully
Thu Nov 13 02:45:52 PM CET 2025: Completed iteration 198

=== Iteration 199/300 ===
Thu Nov 13 02:45:52 PM CET 2025: Starting company car negotiation iteration 199 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 199 completed successfully
Thu Nov 13 02:46:18 PM CET 2025: Completed iteration 199

=== Iteration 200/300 ===
Thu Nov 13 02:46:18 PM CET 2025: Starting company car negotiation iteration 200 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 21.07s (total since entry 21.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 200 completed successfully
Thu Nov 13 02:46:58 PM CET 2025: Completed iteration 200

=== Iteration 201/300 ===
Thu Nov 13 02:46:58 PM CET 2025: Starting company car negotiation iteration 201 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.77s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 45.82s (total since entry 46.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.86s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.61s (total since entry 9.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,100 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 201 completed successfully
Thu Nov 13 02:48:07 PM CET 2025: Completed iteration 201

=== Iteration 202/300 ===
Thu Nov 13 02:48:07 PM CET 2025: Starting company car negotiation iteration 202 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.42s (total since entry 11.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 202 completed successfully
Thu Nov 13 02:48:37 PM CET 2025: Completed iteration 202

=== Iteration 203/300 ===
Thu Nov 13 02:48:37 PM CET 2025: Starting company car negotiation iteration 203 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 203 completed successfully
Thu Nov 13 02:49:06 PM CET 2025: Completed iteration 203

=== Iteration 204/300 ===
Thu Nov 13 02:49:06 PM CET 2025: Starting company car negotiation iteration 204 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.59s (total since entry 10.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.84s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 204 completed successfully
Thu Nov 13 02:49:34 PM CET 2025: Completed iteration 204

=== Iteration 205/300 ===
Thu Nov 13 02:49:34 PM CET 2025: Starting company car negotiation iteration 205 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.64s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 205 completed successfully
Thu Nov 13 02:50:01 PM CET 2025: Completed iteration 205

=== Iteration 206/300 ===
Thu Nov 13 02:50:01 PM CET 2025: Starting company car negotiation iteration 206 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.89s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.76s (total since entry 10.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38300}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38300}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38300}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,300 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,300 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38300
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 294.80 (saved â‚¬294.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1587.87 (profit â‚¬1587.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38889.60, seller=38300.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=294.80, model_b surplus=1587.87
âœ… Calculated utility_surplus: {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38889.59715125, 'model_b': 38300.0}
Metrics: {'utility_surplus': {'model_a': 294.7985756249982, 'model_b': 1587.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 206 completed successfully
Thu Nov 13 02:50:30 PM CET 2025: Completed iteration 206

=== Iteration 207/300 ===
Thu Nov 13 02:50:30 PM CET 2025: Starting company car negotiation iteration 207 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.48s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 207 completed successfully
Thu Nov 13 02:50:57 PM CET 2025: Completed iteration 207

=== Iteration 208/300 ===
Thu Nov 13 02:50:57 PM CET 2025: Starting company car negotiation iteration 208 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.73s (total since entry 10.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.47s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 208 completed successfully
Thu Nov 13 02:51:26 PM CET 2025: Completed iteration 208

=== Iteration 209/300 ===
Thu Nov 13 02:51:26 PM CET 2025: Starting company car negotiation iteration 209 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40100.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 209 completed successfully
Thu Nov 13 02:51:55 PM CET 2025: Completed iteration 209

=== Iteration 210/300 ===
Thu Nov 13 02:51:55 PM CET 2025: Starting company car negotiation iteration 210 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.95s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 41000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 41000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1279.22, model_b surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41058.45, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 210 completed successfully
Thu Nov 13 02:52:22 PM CET 2025: Completed iteration 210

=== Iteration 211/300 ===
Thu Nov 13 02:52:22 PM CET 2025: Starting company car negotiation iteration 211 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39200\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39200\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39200}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39200.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,200 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 211 completed successfully
Thu Nov 13 02:52:53 PM CET 2025: Completed iteration 211

=== Iteration 212/300 ===
Thu Nov 13 02:52:53 PM CET 2025: Starting company car negotiation iteration 212 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,400 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1379.22 (saved â‚¬1379.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 561.22 (profit â‚¬561.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41158.45, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1379.22, model_b surplus=561.22
âœ… Calculated utility_surplus: {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41158.45, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1379.2249999999985, 'model_b': 561.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 212 completed successfully
Thu Nov 13 02:53:20 PM CET 2025: Completed iteration 212

=== Iteration 213/300 ===
Thu Nov 13 02:53:20 PM CET 2025: Starting company car negotiation iteration 213 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 213 completed successfully
Thu Nov 13 02:53:48 PM CET 2025: Completed iteration 213

=== Iteration 214/300 ===
Thu Nov 13 02:53:48 PM CET 2025: Starting company car negotiation iteration 214 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.36s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 214 completed successfully
Thu Nov 13 02:54:14 PM CET 2025: Completed iteration 214

=== Iteration 215/300 ===
Thu Nov 13 02:54:14 PM CET 2025: Starting company car negotiation iteration 215 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.61s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,750 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1635.00 (saved â‚¬1635.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 335.00 (profit â‚¬335.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=42020.00, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1635.00, model_a surplus=335.00
âœ… Calculated utility_surplus: {'model_a': 335.0, 'model_b': 1635.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 42020.0, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 335.0, 'model_b': 1635.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 215 completed successfully
Thu Nov 13 02:54:41 PM CET 2025: Completed iteration 215

=== Iteration 216/300 ===
Thu Nov 13 02:54:41 PM CET 2025: Starting company car negotiation iteration 216 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 216 completed successfully
Thu Nov 13 02:55:11 PM CET 2025: Completed iteration 216

=== Iteration 217/300 ===
Thu Nov 13 02:55:11 PM CET 2025: Starting company car negotiation iteration 217 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 217 completed successfully
Thu Nov 13 02:55:37 PM CET 2025: Completed iteration 217

=== Iteration 218/300 ===
Thu Nov 13 02:55:37 PM CET 2025: Starting company car negotiation iteration 218 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38425\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38425\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38425\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38425}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38425}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38425.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,425 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 218 completed successfully
Thu Nov 13 02:56:06 PM CET 2025: Completed iteration 218

=== Iteration 219/300 ===
Thu Nov 13 02:56:06 PM CET 2025: Starting company car negotiation iteration 219 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 219 completed successfully
Thu Nov 13 02:56:33 PM CET 2025: Completed iteration 219

=== Iteration 220/300 ===
Thu Nov 13 02:56:33 PM CET 2025: Starting company car negotiation iteration 220 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.64s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 220 completed successfully
Thu Nov 13 02:57:02 PM CET 2025: Completed iteration 220

=== Iteration 221/300 ===
Thu Nov 13 02:57:02 PM CET 2025: Starting company car negotiation iteration 221 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.51s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 221 completed successfully
Thu Nov 13 02:57:31 PM CET 2025: Completed iteration 221

=== Iteration 222/300 ===
Thu Nov 13 02:57:31 PM CET 2025: Starting company car negotiation iteration 222 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 222 completed successfully
Thu Nov 13 02:57:59 PM CET 2025: Completed iteration 222

=== Iteration 223/300 ===
Thu Nov 13 02:57:59 PM CET 2025: Starting company car negotiation iteration 223 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 223 completed successfully
Thu Nov 13 02:58:28 PM CET 2025: Completed iteration 223

=== Iteration 224/300 ===
Thu Nov 13 02:58:28 PM CET 2025: Starting company car negotiation iteration 224 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1279.22, model_b surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41058.45, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 224 completed successfully
Thu Nov 13 02:58:55 PM CET 2025: Completed iteration 224

=== Iteration 225/300 ===
Thu Nov 13 02:58:55 PM CET 2025: Starting company car negotiation iteration 225 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.90s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "accept"\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "accept"\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 225 completed successfully
Thu Nov 13 02:59:24 PM CET 2025: Completed iteration 225

=== Iteration 226/300 ===
Thu Nov 13 02:59:24 PM CET 2025: Starting company car negotiation iteration 226 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=-5.20, model_b surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38589.59715125, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': -5.201424375001807, 'model_b': 1887.8745256249967}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 0.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 226 completed successfully
Thu Nov 13 02:59:53 PM CET 2025: Completed iteration 226

=== Iteration 227/300 ===
Thu Nov 13 02:59:53 PM CET 2025: Starting company car negotiation iteration 227 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 227 completed successfully
Thu Nov 13 03:00:22 PM CET 2025: Completed iteration 227

=== Iteration 228/300 ===
Thu Nov 13 03:00:22 PM CET 2025: Starting company car negotiation iteration 228 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.90s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.61s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 8.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 228 completed successfully
Thu Nov 13 03:00:51 PM CET 2025: Completed iteration 228

=== Iteration 229/300 ===
Thu Nov 13 03:00:51 PM CET 2025: Starting company car negotiation iteration 229 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.81s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 229 completed successfully
Thu Nov 13 03:01:19 PM CET 2025: Completed iteration 229

=== Iteration 230/300 ===
Thu Nov 13 03:01:19 PM CET 2025: Starting company car negotiation iteration 230 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37999}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37999}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37999}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,999 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 230 completed successfully
Thu Nov 13 03:01:48 PM CET 2025: Completed iteration 230

=== Iteration 231/300 ===
Thu Nov 13 03:01:48 PM CET 2025: Starting company car negotiation iteration 231 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 231 completed successfully
Thu Nov 13 03:02:14 PM CET 2025: Completed iteration 231

=== Iteration 232/300 ===
Thu Nov 13 03:02:14 PM CET 2025: Starting company car negotiation iteration 232 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬39,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 232 completed successfully
Thu Nov 13 03:02:42 PM CET 2025: Completed iteration 232

=== Iteration 233/300 ===
Thu Nov 13 03:02:42 PM CET 2025: Starting company car negotiation iteration 233 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38601}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38601}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38601}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38601}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38601}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38601.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,601 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 233 completed successfully
Thu Nov 13 03:03:10 PM CET 2025: Completed iteration 233

=== Iteration 234/300 ===
Thu Nov 13 03:03:10 PM CET 2025: Starting company car negotiation iteration 234 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬39,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 4: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 182.54 (saved â‚¬182.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1728.81 (profit â‚¬1728.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39365.07, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=182.54, model_b surplus=1728.81
âœ… Calculated utility_surplus: {'model_a': 182.53662499999336, 'model_b': 1728.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39365.07324999999, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 182.53662499999336, 'model_b': 1728.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 234 completed successfully
Thu Nov 13 03:03:39 PM CET 2025: Completed iteration 234

=== Iteration 235/300 ===
Thu Nov 13 03:03:39 PM CET 2025: Starting company car negotiation iteration 235 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 235 completed successfully
Thu Nov 13 03:04:06 PM CET 2025: Completed iteration 235

=== Iteration 236/300 ===
Thu Nov 13 03:04:06 PM CET 2025: Starting company car negotiation iteration 236 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a utility=0, model_b utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 236 completed successfully
Thu Nov 13 03:04:35 PM CET 2025: Completed iteration 236

=== Iteration 237/300 ===
Thu Nov 13 03:04:35 PM CET 2025: Starting company car negotiation iteration 237 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.55s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 237 completed successfully
Thu Nov 13 03:05:01 PM CET 2025: Completed iteration 237

=== Iteration 238/300 ===
Thu Nov 13 03:05:01 PM CET 2025: Starting company car negotiation iteration 238 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.70s (total since entry 10.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 238 completed successfully
Thu Nov 13 03:05:31 PM CET 2025: Completed iteration 238

=== Iteration 239/300 ===
Thu Nov 13 03:05:31 PM CET 2025: Starting company car negotiation iteration 239 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.63s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 239 completed successfully
Thu Nov 13 03:05:59 PM CET 2025: Completed iteration 239

=== Iteration 240/300 ===
Thu Nov 13 03:05:59 PM CET 2025: Starting company car negotiation iteration 240 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 240 completed successfully
Thu Nov 13 03:06:26 PM CET 2025: Completed iteration 240

=== Iteration 241/300 ===
Thu Nov 13 03:06:26 PM CET 2025: Starting company car negotiation iteration 241 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 241 completed successfully
Thu Nov 13 03:06:52 PM CET 2025: Completed iteration 241

=== Iteration 242/300 ===
Thu Nov 13 03:06:52 PM CET 2025: Starting company car negotiation iteration 242 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38100.0}
ğŸ’¡ Player model_b made offer â‚¬38,100 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38300}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38300}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38300}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,300 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 1)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 242 completed successfully
Thu Nov 13 03:07:20 PM CET 2025: Completed iteration 242

=== Iteration 243/300 ===
Thu Nov 13 03:07:20 PM CET 2025: Starting company car negotiation iteration 243 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37999}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37999}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37999}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37999}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37999.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,999 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "reject"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "reject"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "reject"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'reject'}
[DEBUG] Parsed JSON: {'type': 'reject'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'reject'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âš ï¸ Player model_a rejected but still has proposals remaining (3/4). Continuing negotiation.
âœ… Player model_b accepted offer of â‚¬37,999 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37999
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 595.80 (saved â‚¬595.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1286.87 (profit â‚¬1286.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39190.60, seller=37999.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=595.80, model_b surplus=1286.87
âœ… Calculated utility_surplus: {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39190.59715125, 'model_b': 37999.0}
Metrics: {'utility_surplus': {'model_a': 595.7985756249982, 'model_b': 1286.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 243 completed successfully
Thu Nov 13 03:07:52 PM CET 2025: Completed iteration 243

=== Iteration 244/300 ===
Thu Nov 13 03:07:52 PM CET 2025: Starting company car negotiation iteration 244 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.46s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 244 completed successfully
Thu Nov 13 03:08:21 PM CET 2025: Completed iteration 244

=== Iteration 245/300 ===
Thu Nov 13 03:08:21 PM CET 2025: Starting company car negotiation iteration 245 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬39,100 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,100 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=39100
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -505.20 (saved â‚¬-505.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2387.87 (profit â‚¬2387.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38089.60, seller=39100.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-505.20, model_a surplus=2387.87
âœ… Calculated utility_surplus: {'model_a': 2387.8745256249967, 'model_b': -505.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38089.59715125, 'model_a': 39100.0}
Metrics: {'utility_surplus': {'model_a': 2387.8745256249967, 'model_b': -505.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 245 completed successfully
Thu Nov 13 03:08:49 PM CET 2025: Completed iteration 245

=== Iteration 246/300 ===
Thu Nov 13 03:08:49 PM CET 2025: Starting company car negotiation iteration 246 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.63s (total since entry 10.34s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.57s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38601\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38601\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38601\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38601}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38601}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38601.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,601 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 246 completed successfully
Thu Nov 13 03:09:18 PM CET 2025: Completed iteration 246

=== Iteration 247/300 ===
Thu Nov 13 03:09:18 PM CET 2025: Starting company car negotiation iteration 247 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38839\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38839\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38839\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38839}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38839}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38839.0}
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,839 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37600.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,600 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 994.80 (saved â‚¬994.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 887.87 (profit â‚¬887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39589.60, seller=37600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=994.80, model_b surplus=887.87
âœ… Calculated utility_surplus: {'model_a': 994.7985756249982, 'model_b': 887.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39589.59715125, 'model_b': 37600.0}
Metrics: {'utility_surplus': {'model_a': 994.7985756249982, 'model_b': 887.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 247 completed successfully
Thu Nov 13 03:09:47 PM CET 2025: Completed iteration 247

=== Iteration 248/300 ===
Thu Nov 13 03:09:47 PM CET 2025: Starting company car negotiation iteration 248 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,499 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 248 completed successfully
Thu Nov 13 03:10:16 PM CET 2025: Completed iteration 248

=== Iteration 249/300 ===
Thu Nov 13 03:10:16 PM CET 2025: Starting company car negotiation iteration 249 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39250\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39250\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39250\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39250}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,250 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 249 completed successfully
Thu Nov 13 03:10:45 PM CET 2025: Completed iteration 249

=== Iteration 250/300 ===
Thu Nov 13 03:10:45 PM CET 2025: Starting company car negotiation iteration 250 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 250 completed successfully
Thu Nov 13 03:11:16 PM CET 2025: Completed iteration 250

=== Iteration 251/300 ===
Thu Nov 13 03:11:16 PM CET 2025: Starting company car negotiation iteration 251 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.89s (total since entry 10.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=582.54, model_b surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39765.07324999999, 'model_b': 38600.0}
Metrics: {'utility_surplus': {'model_a': 582.5366249999934, 'model_b': 1328.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 251 completed successfully
Thu Nov 13 03:11:45 PM CET 2025: Completed iteration 251

=== Iteration 252/300 ===
Thu Nov 13 03:11:45 PM CET 2025: Starting company car negotiation iteration 252 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬36,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1279.22, model_b surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41058.45, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 252 completed successfully
Thu Nov 13 03:12:12 PM CET 2025: Completed iteration 252

=== Iteration 253/300 ===
Thu Nov 13 03:12:12 PM CET 2025: Starting company car negotiation iteration 253 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,750 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38999}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38999}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38999}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38999}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38999}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38999.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,999 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 253 completed successfully
Thu Nov 13 03:12:42 PM CET 2025: Completed iteration 253

=== Iteration 254/300 ===
Thu Nov 13 03:12:42 PM CET 2025: Starting company car negotiation iteration 254 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 254 completed successfully
Thu Nov 13 03:13:10 PM CET 2025: Completed iteration 254

=== Iteration 255/300 ===
Thu Nov 13 03:13:10 PM CET 2025: Starting company car negotiation iteration 255 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38762\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38762\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38762\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38762}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38762}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38762.0}
ğŸ’¡ Player model_b made offer â‚¬38,762 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38595}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38595}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38595}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38595}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38595}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38595.0}
âš ï¸ Player model_b tried to make offer but exceeded proposal limit (4/4)
ğŸ² [ROLE DEBUG] No Agreement - Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 0.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 255 completed successfully
Thu Nov 13 03:13:38 PM CET 2025: Completed iteration 255

=== Iteration 256/300 ===
Thu Nov 13 03:13:38 PM CET 2025: Starting company car negotiation iteration 256 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of â‚¬38,900 (made in round 3)
ğŸ” [BATNA DEBUG] Round 4: price=38900
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 282.54 (saved â‚¬282.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1628.81 (profit â‚¬1628.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39465.07, seller=38900.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=282.54, model_a surplus=1628.81
âœ… Calculated utility_surplus: {'model_a': 1628.8066250000047, 'model_b': 282.53662499999336}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39465.07324999999, 'model_a': 38900.0}
Metrics: {'utility_surplus': {'model_a': 1628.8066250000047, 'model_b': 282.53662499999336}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 256 completed successfully
Thu Nov 13 03:14:06 PM CET 2025: Completed iteration 256

=== Iteration 257/300 ===
Thu Nov 13 03:14:06 PM CET 2025: Starting company car negotiation iteration 257 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 2)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 257 completed successfully
Thu Nov 13 03:14:34 PM CET 2025: Completed iteration 257

=== Iteration 258/300 ===
Thu Nov 13 03:14:34 PM CET 2025: Starting company car negotiation iteration 258 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.70s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 258 completed successfully
Thu Nov 13 03:15:04 PM CET 2025: Completed iteration 258

=== Iteration 259/300 ===
Thu Nov 13 03:15:04 PM CET 2025: Starting company car negotiation iteration 259 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40100\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40100\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40100\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40100}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40100.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38700\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38700\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38700\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38700}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38700}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38700.0}
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,700 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,750 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -155.20 (saved â‚¬-155.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2037.87 (profit â‚¬2037.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38439.60, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-155.20, model_a surplus=2037.87
âœ… Calculated utility_surplus: {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38439.59715125, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 2037.8745256249967, 'model_b': -155.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 259 completed successfully
Thu Nov 13 03:15:32 PM CET 2025: Completed iteration 259

=== Iteration 260/300 ===
Thu Nov 13 03:15:32 PM CET 2025: Starting company car negotiation iteration 260 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.87s (total since entry 10.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,750 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,750 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38750
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 432.54 (saved â‚¬432.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1478.81 (profit â‚¬1478.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39615.07, seller=38750.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=432.54, model_a surplus=1478.81
âœ… Calculated utility_surplus: {'model_a': 1478.8066250000047, 'model_b': 432.53662499999336}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39615.07324999999, 'model_a': 38750.0}
Metrics: {'utility_surplus': {'model_a': 1478.8066250000047, 'model_b': 432.53662499999336}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 260 completed successfully
Thu Nov 13 03:16:01 PM CET 2025: Completed iteration 260

=== Iteration 261/300 ===
Thu Nov 13 03:16:01 PM CET 2025: Starting company car negotiation iteration 261 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1885.00 (saved â‚¬1885.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 85.00 (profit â‚¬85.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=42270.00, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1885.00, model_a surplus=85.00
âœ… Calculated utility_surplus: {'model_a': 85.0, 'model_b': 1885.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 42270.0, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 85.0, 'model_b': 1885.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 261 completed successfully
Thu Nov 13 03:16:27 PM CET 2025: Completed iteration 261

=== Iteration 262/300 ===
Thu Nov 13 03:16:27 PM CET 2025: Starting company car negotiation iteration 262 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38499}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38499}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38499}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38499}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38499}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38499.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,499 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 262 completed successfully
Thu Nov 13 03:16:56 PM CET 2025: Completed iteration 262

=== Iteration 263/300 ===
Thu Nov 13 03:16:56 PM CET 2025: Starting company car negotiation iteration 263 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬40,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=40000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -1405.20 (saved â‚¬-1405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 3287.87 (profit â‚¬3287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37189.60, seller=40000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-1405.20, model_a surplus=3287.87
âœ… Calculated utility_surplus: {'model_a': 3287.8745256249967, 'model_b': -1405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 37189.59715125, 'model_a': 40000.0}
Metrics: {'utility_surplus': {'model_a': 3287.8745256249967, 'model_b': -1405.2014243750018}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 263 completed successfully
Thu Nov 13 03:17:25 PM CET 2025: Completed iteration 263

=== Iteration 264/300 ===
Thu Nov 13 03:17:25 PM CET 2025: Starting company car negotiation iteration 264 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬40,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=40000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -1405.20 (saved â‚¬-1405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 3287.87 (profit â‚¬3287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37189.60, seller=40000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-1405.20, model_a surplus=3287.87
âœ… Calculated utility_surplus: {'model_a': 3287.8745256249967, 'model_b': -1405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 37189.59715125, 'model_a': 40000.0}
Metrics: {'utility_surplus': {'model_a': 3287.8745256249967, 'model_b': -1405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 264 completed successfully
Thu Nov 13 03:17:53 PM CET 2025: Completed iteration 264

=== Iteration 265/300 ===
Thu Nov 13 03:17:53 PM CET 2025: Starting company car negotiation iteration 265 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.64s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 265 completed successfully
Thu Nov 13 03:18:22 PM CET 2025: Completed iteration 265

=== Iteration 266/300 ===
Thu Nov 13 03:18:22 PM CET 2025: Starting company car negotiation iteration 266 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39779.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,779 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 266 completed successfully
Thu Nov 13 03:18:50 PM CET 2025: Completed iteration 266

=== Iteration 267/300 ===
Thu Nov 13 03:18:50 PM CET 2025: Starting company car negotiation iteration 267 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1279.22 (saved â‚¬1279.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 661.22 (profit â‚¬661.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41058.45, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1279.22, model_b surplus=661.22
âœ… Calculated utility_surplus: {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 41058.45, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1279.2249999999985, 'model_b': 661.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 267 completed successfully
Thu Nov 13 03:19:18 PM CET 2025: Completed iteration 267

=== Iteration 268/300 ===
Thu Nov 13 03:19:18 PM CET 2025: Starting company car negotiation iteration 268 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ’¡ Player model_a made offer â‚¬38,550 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,550 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38550
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 632.54 (saved â‚¬632.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1278.81 (profit â‚¬1278.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39815.07, seller=38550.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=632.54, model_b surplus=1278.81
âœ… Calculated utility_surplus: {'model_a': 632.5366249999934, 'model_b': 1278.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39815.07324999999, 'model_b': 38550.0}
Metrics: {'utility_surplus': {'model_a': 632.5366249999934, 'model_b': 1278.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 268 completed successfully
Thu Nov 13 03:19:46 PM CET 2025: Completed iteration 268

=== Iteration 269/300 ===
Thu Nov 13 03:19:46 PM CET 2025: Starting company car negotiation iteration 269 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 269 completed successfully
Thu Nov 13 03:20:13 PM CET 2025: Completed iteration 269

=== Iteration 270/300 ===
Thu Nov 13 03:20:13 PM CET 2025: Starting company car negotiation iteration 270 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38475\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38475\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38475\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38475}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38475}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38475.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,475 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 270 completed successfully
Thu Nov 13 03:20:42 PM CET 2025: Completed iteration 270

=== Iteration 271/300 ===
Thu Nov 13 03:20:42 PM CET 2025: Starting company car negotiation iteration 271 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.50s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,800 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -905.20 (saved â‚¬-905.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2787.87 (profit â‚¬2787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=37689.60, seller=39500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-905.20, model_a surplus=2787.87
âœ… Calculated utility_surplus: {'model_a': 2787.8745256249967, 'model_b': -905.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 37689.59715125, 'model_a': 39500.0}
Metrics: {'utility_surplus': {'model_a': 2787.8745256249967, 'model_b': -905.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 271 completed successfully
Thu Nov 13 03:21:10 PM CET 2025: Completed iteration 271

=== Iteration 272/300 ===
Thu Nov 13 03:21:10 PM CET 2025: Starting company car negotiation iteration 272 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.86s (total since entry 10.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.49s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38888}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38888}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38888}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38888}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38888}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38888.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,888 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 272 completed successfully
Thu Nov 13 03:21:39 PM CET 2025: Completed iteration 272

=== Iteration 273/300 ===
Thu Nov 13 03:21:39 PM CET 2025: Starting company car negotiation iteration 273 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38450}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38450}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38450}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38450}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38450}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38450.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ’¡ Player model_a made offer â‚¬38,450 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 273 completed successfully
Thu Nov 13 03:22:07 PM CET 2025: Completed iteration 273

=== Iteration 274/300 ===
Thu Nov 13 03:22:07 PM CET 2025: Starting company car negotiation iteration 274 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=94.80, model_b surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38689.59715125, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 94.7985756249982, 'model_b': 1787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 274 completed successfully
Thu Nov 13 03:22:36 PM CET 2025: Completed iteration 274

=== Iteration 275/300 ===
Thu Nov 13 03:22:36 PM CET 2025: Starting company car negotiation iteration 275 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 275 completed successfully
Thu Nov 13 03:23:02 PM CET 2025: Completed iteration 275

=== Iteration 276/300 ===
Thu Nov 13 03:23:02 PM CET 2025: Starting company car negotiation iteration 276 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.56s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 276 completed successfully
Thu Nov 13 03:23:31 PM CET 2025: Completed iteration 276

=== Iteration 277/300 ===
Thu Nov 13 03:23:31 PM CET 2025: Starting company car negotiation iteration 277 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.45s (total since entry 9.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38750}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38750}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38750}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 277 completed successfully
Thu Nov 13 03:23:59 PM CET 2025: Completed iteration 277

=== Iteration 278/300 ===
Thu Nov 13 03:23:59 PM CET 2025: Starting company car negotiation iteration 278 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "accept"\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 278 completed successfully
Thu Nov 13 03:24:27 PM CET 2025: Completed iteration 278

=== Iteration 279/300 ===
Thu Nov 13 03:24:27 PM CET 2025: Starting company car negotiation iteration 279 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.77s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38888\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38888\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38888\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38888}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38888}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38888.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,888 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬39,000 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -405.20 (saved â‚¬-405.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 2287.87 (profit â‚¬2287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38189.60, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-405.20, model_a surplus=2287.87
âœ… Calculated utility_surplus: {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38189.59715125, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 2287.8745256249967, 'model_b': -405.2014243750018}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 279 completed successfully
Thu Nov 13 03:24:56 PM CET 2025: Completed iteration 279

=== Iteration 280/300 ===
Thu Nov 13 03:24:56 PM CET 2025: Starting company car negotiation iteration 280 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.54s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38550\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38550\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38550\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,000 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1594.80 (saved â‚¬1594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 287.87 (profit â‚¬287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40189.60, seller=37000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1594.80, model_b surplus=287.87
âœ… Calculated utility_surplus: {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 40189.59715125, 'model_b': 37000.0}
Metrics: {'utility_surplus': {'model_a': 1594.7985756249982, 'model_b': 287.87452562499675}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 280 completed successfully
Thu Nov 13 03:25:26 PM CET 2025: Completed iteration 280

=== Iteration 281/300 ===
Thu Nov 13 03:25:26 PM CET 2025: Starting company car negotiation iteration 281 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.64s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37,500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37,500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37,500}'
âš ï¸ [DEBUG] [model_a] JSON decode error for start match: Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] JSON decode error for '{"type": "offer", "price": 37,500}...': Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] JSON decode error for '{"type": "offer", "price": 37,500}...': Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âš ï¸ [DEBUG] [model_a] Extracted JSON decode error: Expecting property name enclosed in double quotes: line 1 column 31 (char 30)
âœ… [DEBUG] [model_a] Manually extracted action: {'type': 'offer'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬37,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=37500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1094.80 (saved â‚¬1094.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 787.87 (profit â‚¬787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39689.60, seller=37500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1094.80, model_b surplus=787.87
âœ… Calculated utility_surplus: {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39689.59715125, 'model_b': 37500.0}
Metrics: {'utility_surplus': {'model_a': 1094.7985756249982, 'model_b': 787.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 281 completed successfully
Thu Nov 13 03:25:55 PM CET 2025: Completed iteration 281

=== Iteration 282/300 ===
Thu Nov 13 03:25:55 PM CET 2025: Starting company car negotiation iteration 282 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 41000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 41000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 41000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 41000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 41000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 41000.0}
ğŸ’¡ Player model_b made offer â‚¬41,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38200}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38200}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38200}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38200}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38200}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38200.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,200 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,200 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38200
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 394.80 (saved â‚¬394.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1487.87 (profit â‚¬1487.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38989.60, seller=38200.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=394.80, model_b surplus=1487.87
âœ… Calculated utility_surplus: {'model_a': 394.7985756249982, 'model_b': 1487.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38989.59715125, 'model_b': 38200.0}
Metrics: {'utility_surplus': {'model_a': 394.7985756249982, 'model_b': 1487.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 282 completed successfully
Thu Nov 13 03:26:22 PM CET 2025: Completed iteration 282

=== Iteration 283/300 ===
Thu Nov 13 03:26:23 PM CET 2025: Starting company car negotiation iteration 283 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.42s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 283 completed successfully
Thu Nov 13 03:26:52 PM CET 2025: Completed iteration 283

=== Iteration 284/300 ===
Thu Nov 13 03:26:52 PM CET 2025: Starting company car negotiation iteration 284 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.89s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37800}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37800}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37800}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37800}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬37,800 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬37,800 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=37800
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 794.80 (saved â‚¬794.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1087.87 (profit â‚¬1087.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39389.60, seller=37800.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=794.80, model_b surplus=1087.87
âœ… Calculated utility_surplus: {'model_a': 794.7985756249982, 'model_b': 1087.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39389.59715125, 'model_b': 37800.0}
Metrics: {'utility_surplus': {'model_a': 794.7985756249982, 'model_b': 1087.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 284 completed successfully
Thu Nov 13 03:27:20 PM CET 2025: Completed iteration 284

=== Iteration 285/300 ===
Thu Nov 13 03:27:20 PM CET 2025: Starting company car negotiation iteration 285 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.63s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38900}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38900}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38900}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 285 completed successfully
Thu Nov 13 03:27:48 PM CET 2025: Completed iteration 285

=== Iteration 286/300 ===
Thu Nov 13 03:27:48 PM CET 2025: Starting company car negotiation iteration 286 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 3: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39779.22, seller=37838.78
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1179.22 (saved â‚¬1179.22 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 761.22 (profit â‚¬761.22 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=40958.45, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1179.22, model_a surplus=761.22
âœ… Calculated utility_surplus: {'model_a': 761.2249999999985, 'model_b': 1179.2249999999985}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 40958.45, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 761.2249999999985, 'model_b': 1179.2249999999985}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 286 completed successfully
Thu Nov 13 03:28:16 PM CET 2025: Completed iteration 286

=== Iteration 287/300 ===
Thu Nov 13 03:28:16 PM CET 2025: Starting company car negotiation iteration 287 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,500 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 682.54 (saved â‚¬682.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1228.81 (profit â‚¬1228.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39865.07, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=682.54, model_b surplus=1228.81
âœ… Calculated utility_surplus: {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 39865.07324999999, 'model_b': 38500.0}
Metrics: {'utility_surplus': {'model_a': 682.5366249999934, 'model_b': 1228.8066250000047}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 287 completed successfully
Thu Nov 13 03:28:44 PM CET 2025: Completed iteration 287

=== Iteration 288/300 ===
Thu Nov 13 03:28:44 PM CET 2025: Starting company car negotiation iteration 288 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.51s (total since entry 11.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.24s (total since entry 9.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 36000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 36000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 36000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 36000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 36000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 36000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬36,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37900}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37900}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37900}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37900}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37900.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬37,900 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38550}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38550}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38550}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38550}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38550}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38550.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,550 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 288 completed successfully
Thu Nov 13 03:29:15 PM CET 2025: Completed iteration 288

=== Iteration 289/300 ===
Thu Nov 13 03:29:15 PM CET 2025: Starting company car negotiation iteration 289 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.66s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 289 completed successfully
Thu Nov 13 03:29:42 PM CET 2025: Completed iteration 289

=== Iteration 290/300 ===
Thu Nov 13 03:29:42 PM CET 2025: Starting company car negotiation iteration 290 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 40000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬40,000 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 4/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 290 completed successfully
Thu Nov 13 03:30:11 PM CET 2025: Completed iteration 290

=== Iteration 291/300 ===
Thu Nov 13 03:30:11 PM CET 2025: Starting company car negotiation iteration 291 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.59s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.35s (total since entry 9.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39500}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39500}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39500}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38385}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38385}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38385}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38385}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38385}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38385.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬38,385 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,600 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: -5.20 (saved â‚¬-5.20 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1887.87 (profit â‚¬1887.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38589.60, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=-5.20, model_a surplus=1887.87
âœ… Calculated utility_surplus: {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38589.59715125, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1887.8745256249967, 'model_b': -5.201424375001807}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 291 completed successfully
Thu Nov 13 03:30:39 PM CET 2025: Completed iteration 291

=== Iteration 292/300 ===
Thu Nov 13 03:30:39 PM CET 2025: Starting company car negotiation iteration 292 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.41s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=1385.00, model_b surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 1385.0, 'model_b': 585.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 41770.0, 'model_b': 39000.0}
Metrics: {'utility_surplus': {'model_a': 1385.0, 'model_b': 585.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 292 completed successfully
Thu Nov 13 03:31:05 PM CET 2025: Completed iteration 292

=== Iteration 293/300 ===
Thu Nov 13 03:31:05 PM CET 2025: Starting company car negotiation iteration 293 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38800\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38800\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38800\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38800}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38800}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38800.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,800 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 293 completed successfully
Thu Nov 13 03:31:35 PM CET 2025: Completed iteration 293

=== Iteration 294/300 ===
Thu Nov 13 03:31:35 PM CET 2025: Starting company car negotiation iteration 294 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.67s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬37,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38779\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38779\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38779}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38779}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38779.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬38,779 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38600\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38600\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,600 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬38,600 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 4: price=38600
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=39182.54, seller=37271.19
ğŸ” [SURPLUS DEBUG] Buyer surplus: 582.54 (saved â‚¬582.54 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1328.81 (profit â‚¬1328.81 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39765.07, seller=38600.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=582.54, model_a surplus=1328.81
âœ… Calculated utility_surplus: {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 39765.07324999999, 'model_a': 38600.0}
Metrics: {'utility_surplus': {'model_a': 1328.8066250000047, 'model_b': 582.5366249999934}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 294 completed successfully
Thu Nov 13 03:32:03 PM CET 2025: Completed iteration 294

=== Iteration 295/300 ===
Thu Nov 13 03:32:03 PM CET 2025: Starting company car negotiation iteration 295 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 38900\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 38900\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 38900\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38900}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38900}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38900.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,900 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38750\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38750\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38750}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38750}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38750.0}
ğŸ’¡ Player model_b made offer â‚¬38,750 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,500 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38500
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 94.80 (saved â‚¬94.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1787.87 (profit â‚¬1787.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38689.60, seller=38500.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=94.80, model_a surplus=1787.87
âœ… Calculated utility_surplus: {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38689.59715125, 'model_a': 38500.0}
Metrics: {'utility_surplus': {'model_a': 1787.8745256249967, 'model_b': 94.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 295 completed successfully
Thu Nov 13 03:32:32 PM CET 2025: Completed iteration 295

=== Iteration 296/300 ===
Thu Nov 13 03:32:32 PM CET 2025: Starting company car negotiation iteration 296 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.44s (total since entry 9.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 37500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 37500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 37500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 37500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 37500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 37500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_a made offer â‚¬37,500 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬38,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38300\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 38300\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 38300\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38300}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38300}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38300.0}
ğŸ’¡ Player model_b made offer â‚¬38,300 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,000 (made in round 3)
ğŸ” [BATNA DEBUG] Round 5: price=38000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 594.80 (saved â‚¬594.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1287.87 (profit â‚¬1287.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=39189.60, seller=38000.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=594.80, model_b surplus=1287.87
âœ… Calculated utility_surplus: {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 39189.59715125, 'model_b': 38000.0}
Metrics: {'utility_surplus': {'model_a': 594.7985756249982, 'model_b': 1287.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 296 completed successfully
Thu Nov 13 03:33:02 PM CET 2025: Completed iteration 296

=== Iteration 297/300 ===
Thu Nov 13 03:33:02 PM CET 2025: Starting company car negotiation iteration 297 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered â‚¬39,000 - Creating automatic agreement!
ğŸ” [BATNA DEBUG] Round 2: price=39000
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=40385.00, seller=38415.00
ğŸ” [SURPLUS DEBUG] Buyer surplus: 1385.00 (saved â‚¬1385.00 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 585.00 (profit â‚¬585.00 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=41770.00, seller=39000.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=1385.00, model_a surplus=585.00
âœ… Calculated utility_surplus: {'model_a': 585.0, 'model_b': 1385.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 41770.0, 'model_a': 39000.0}
Metrics: {'utility_surplus': {'model_a': 585.0, 'model_b': 1385.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 297 completed successfully
Thu Nov 13 03:33:28 PM CET 2025: Completed iteration 297

=== Iteration 298/300 ===
Thu Nov 13 03:33:28 PM CET 2025: Starting company car negotiation iteration 298 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.40s (total since entry 9.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n"type": "offer",\n"price": 39000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n"type": "offer",\n"price": 39000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=194.80, model_b surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38789.59715125, 'model_b': 38400.0}
Metrics: {'utility_surplus': {'model_a': 194.7985756249982, 'model_b': 1687.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 298 completed successfully
Thu Nov 13 03:33:57 PM CET 2025: Completed iteration 298

=== Iteration 299/300 ===
Thu Nov 13 03:33:57 PM CET 2025: Starting company car negotiation iteration 299 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.43s (total since entry 9.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = BUYER, model_a = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 38400\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 38400\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39100}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39100}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39100}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39100}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39100}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39100.0}
ğŸ’¡ Player model_b made offer â‚¬38,400 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬39,100 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38600}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 38600}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 38600}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 38600}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38600}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 38600.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_a] Raw LLM response: ''
âŒ [DEBUG] [model_a] Could not parse action from response: 
ğŸ’¡ Player model_b made offer â‚¬38,600 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38400}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38400}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38400}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38400}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38400}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38400.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
ğŸ’¡ Player model_a made offer â‚¬38,400 (proposal 3/4)
âœ… Player model_b accepted offer of â‚¬38,400 (made in round 5)
ğŸ” [BATNA DEBUG] Round 5: price=38400
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 194.80 (saved â‚¬194.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1687.87 (profit â‚¬1687.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38789.60, seller=38400.00
ğŸ² [ROLE DEBUG] Buyer=model_b, Seller=model_a
ğŸ² [ROLE DEBUG] model_b surplus=194.80, model_a surplus=1687.87
âœ… Calculated utility_surplus: {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Ministral-8B-Instruct-2410 beat mistralai/Mistral-7B-Instruct-v0.3 (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 38789.59715125, 'model_a': 38400.0}
Metrics: {'utility_surplus': {'model_a': 1687.8745256249967, 'model_b': 194.7985756249982}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 299 completed successfully
Thu Nov 13 03:34:25 PM CET 2025: Completed iteration 299

=== Iteration 300/300 ===
Thu Nov 13 03:34:25 PM CET 2025: Starting company car negotiation iteration 300 on node jn117
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Ministral-8B-Instruct-2410', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for mistralai/Ministral-8B-Instruct-2410)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for mistralai/Mistral-7B-Instruct-v0.3)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model: model_c (new instance for meta-llama/Llama-3.2-3B-Instruct)
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
Loading mistralai/Ministral-8B-Instruct-2410...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Ministral-8B-Instruct-2410'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Ministral-8B-Instruct-2410', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.65s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17847
âœ… mistralai/Ministral-8B-Instruct-2410 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Loading mistralai/Mistral-7B-Instruct-v0.3...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='mistralai/Mistral-7B-Instruct-v0.3'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.39s (total since entry 9.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 29847
âœ… mistralai/Mistral-7B-Instruct-v0.3 loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = BUYER, model_b = SELLER
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 42000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 42000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 42000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 42000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 42000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ’¡ Player model_b made offer â‚¬42,000 (proposal 1/4)
ğŸ’¡ Player model_a made offer â‚¬39,000 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38500}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38500}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38500}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38500}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "price": 40000\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "price": 40000\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ’¡ Player model_a made offer â‚¬38,500 (proposal 2/4)
ğŸ’¡ Player model_b made offer â‚¬40,000 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "price": 39500\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "price": 39500\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39500}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39500}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39500.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38000}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ’¡ Player model_b made offer â‚¬39,500 (proposal 3/4)
ğŸ’¡ Player model_a made offer â‚¬38,000 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 39000}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "price": 39000}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "price": 39000}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'price': 39000}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 39000}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'offer', 'price': 39000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38250}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "price": 38250}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "price": 38250}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'price': 38250}
[DEBUG] Parsed JSON: {'type': 'offer', 'price': 38250}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'offer', 'price': 38250.0}
ğŸ’¡ Player model_b made offer â‚¬39,000 (proposal 4/4)
ğŸ’¡ Player model_a made offer â‚¬38,250 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of â‚¬38,250 (made in round 4)
ğŸ” [BATNA DEBUG] Round 5: price=38250
ğŸ” [BATNA DEBUG] Config BATNAs: buyer=41000, seller=39000
ğŸ” [BATNA DEBUG] Decay rate: {'buyer': 0.015, 'seller': 0.015}
ğŸ” [BATNA DEBUG] Calculated BATNAs: buyer=38594.80, seller=36712.13
ğŸ” [SURPLUS DEBUG] Buyer surplus: 344.80 (saved â‚¬344.80 vs BATNA)
ğŸ” [SURPLUS DEBUG] Seller surplus: 1537.87 (profit â‚¬1537.87 over BATNA)
ğŸ” [UTILITY DEBUG] Final utilities (for session manager): buyer=38939.60, seller=38250.00
ğŸ² [ROLE DEBUG] Buyer=model_a, Seller=model_b
ğŸ² [ROLE DEBUG] model_a surplus=344.80, model_b surplus=1537.87
âœ… Calculated utility_surplus: {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] mistralai/Mistral-7B-Instruct-v0.3 beat mistralai/Ministral-8B-Instruct-2410 (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Ministral-8B-Instruct-2410, model_b=Mistral-7B-Instruct-v0.3
ğŸ—‘ï¸  mistralai/Ministral-8B-Instruct-2410 unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: mistralai/Ministral-8B-Instruct-2410)
ğŸ—‘ï¸  mistralai/Mistral-7B-Instruct-v0.3 unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: mistralai/Mistral-7B-Instruct-v0.3)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 38939.59715125, 'model_b': 38250.0}
Metrics: {'utility_surplus': {'model_a': 344.7985756249982, 'model_b': 1537.8745256249967}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 300 completed successfully
Thu Nov 13 03:34:54 PM CET 2025: Completed iteration 300

=== Company Car Game Batch Complete ===
Successful iterations: 300/300
âœ… ALL ITERATIONS SUCCESSFUL: Platform working perfectly on node jn117
=== Company Car Run Complete ===
