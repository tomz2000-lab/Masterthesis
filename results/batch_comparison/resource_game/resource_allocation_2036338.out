=== Resource Allocation Game - Targeted Node Run ===
Job ID: 2036338, Node: jn116
=== Environment Check ===
CUDA: True, GPU: NVIDIA L40
NVIDIA L40, 49140 MiB, 1 MiB
=== Verifying Torch Scope Fix ===
âœ… Torch scope fix is present
=== Verifying Resource Allocation Game Updates ===
âœ… Role-based prompts are implemented
=== Running Resource Allocation Game - 10 Iterations ===
Game: Development Team vs Marketing Team resource negotiation
Utility Functions: Development=12x+3y+Îµ, Marketing=3x+12y+i

=== Iteration 1/300 ===
Thu Nov 20 10:28:03 AM CET 2025: Starting resource allocation negotiation iteration 1
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 14.76s (total since entry 15.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 29.89s (total since entry 30.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=424.00 - BATNA=286.70 = surplus=137.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=258.03 = surplus=157.97
âœ… Calculated utility_surplus: {'model_a': 137.29851250000002, 'model_b': 157.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 424, 'model_b': 416}
Metrics: {'utility_surplus': {'model_a': 137.29851250000002, 'model_b': 157.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 1 completed successfully
Thu Nov 20 10:29:16 AM CET 2025: Completed iteration 1

=== Iteration 2/300 ===
Thu Nov 20 10:29:16 AM CET 2025: Starting resource allocation negotiation iteration 2
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.91s (total since entry 5.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.45s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 2 completed successfully
Thu Nov 20 10:30:02 AM CET 2025: Completed iteration 2

=== Iteration 3/300 ===
Thu Nov 20 10:30:02 AM CET 2025: Starting resource allocation negotiation iteration 3
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.71s (total since entry 6.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.46s (total since entry 9.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=45, CPU=15 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=154.50, marketing=124.05
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=265.95 = surplus=124.05
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=295.50 = surplus=154.50
âœ… Calculated utility_surplus: {'model_a': 124.05000000000001, 'model_b': 154.5}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 124.05000000000001, 'model_b': 154.5}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 3 completed successfully
Thu Nov 20 10:30:29 AM CET 2025: Completed iteration 3

=== Iteration 4/300 ===
Thu Nov 20 10:30:29 AM CET 2025: Starting resource allocation negotiation iteration 4
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.13s (total since entry 9.67s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 50}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 50}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 50}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 50}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 50}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 50.0}
âš ï¸ Player model_b offer violates constraints: GPU=50, CPU=50
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=30, CPU=25 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=94.50, marketing=114.05
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=265.95 = surplus=114.05
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=295.50 = surplus=94.50
âœ… Calculated utility_surplus: {'model_a': 114.05000000000001, 'model_b': 94.5}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 114.05000000000001, 'model_b': 94.5}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 4 completed successfully
Thu Nov 20 10:31:01 AM CET 2025: Completed iteration 4

=== Iteration 5/300 ===
Thu Nov 20 10:31:01 AM CET 2025: Starting resource allocation negotiation iteration 5
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.30s (total since entry 4.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.93, marketing=116.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=261.96 = surplus=116.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=291.07 = surplus=100.93
âœ… Calculated utility_surplus: {'model_a': 116.03924999999998, 'model_b': 100.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 116.03924999999998, 'model_b': 100.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 5 completed successfully
Thu Nov 20 10:31:34 AM CET 2025: Completed iteration 5

=== Iteration 6/300 ===
Thu Nov 20 10:31:34 AM CET 2025: Starting resource allocation negotiation iteration 6
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.30s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 47, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 47, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 47, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 47, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 47, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 47.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=47, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=37, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=434.00, marketing=406.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=147.30, marketing=147.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=406.00 - BATNA=258.03 = surplus=147.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=434.00 - BATNA=286.70 = surplus=147.30
âœ… Calculated utility_surplus: {'model_a': 147.96866125000003, 'model_b': 147.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 434, 'model_a': 406}
Metrics: {'utility_surplus': {'model_a': 147.96866125000003, 'model_b': 147.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 6 completed successfully
Thu Nov 20 10:32:08 AM CET 2025: Completed iteration 6

=== Iteration 7/300 ===
Thu Nov 20 10:32:08 AM CET 2025: Starting resource allocation negotiation iteration 7
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.49s (total since entry 6.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.92s (total since entry 9.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=132.93, marketing=154.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=261.96 = surplus=154.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=424.00 - BATNA=291.07 = surplus=132.93
âœ… Calculated utility_surplus: {'model_a': 154.03924999999998, 'model_b': 132.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 424, 'model_a': 416}
Metrics: {'utility_surplus': {'model_a': 154.03924999999998, 'model_b': 132.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 7 completed successfully
Thu Nov 20 10:32:44 AM CET 2025: Completed iteration 7

=== Iteration 8/300 ===
Thu Nov 20 10:32:44 AM CET 2025: Starting resource allocation negotiation iteration 8
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.30s (total since entry 5.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.16s (total since entry 9.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 8 completed successfully
Thu Nov 20 10:33:12 AM CET 2025: Completed iteration 8

=== Iteration 9/300 ===
Thu Nov 20 10:33:12 AM CET 2025: Starting resource allocation negotiation iteration 9
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.32s (total since entry 10.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=10 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=45, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=420.00, marketing=350.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=128.93, marketing=88.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=350.00 - BATNA=261.96 = surplus=88.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=420.00 - BATNA=291.07 = surplus=128.93
âœ… Calculated utility_surplus: {'model_a': 88.03924999999998, 'model_b': 128.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 420, 'model_a': 350}
Metrics: {'utility_surplus': {'model_a': 88.03924999999998, 'model_b': 128.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 9 completed successfully
Thu Nov 20 10:33:48 AM CET 2025: Completed iteration 9

=== Iteration 10/300 ===
Thu Nov 20 10:33:48 AM CET 2025: Starting resource allocation negotiation iteration 10
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.69s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=416.00, marketing=396.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=124.93, marketing=134.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=291.07 = surplus=124.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=396.00 - BATNA=261.96 = surplus=134.04
âœ… Calculated utility_surplus: {'model_a': 124.9325, 'model_b': 134.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 416, 'model_b': 396}
Metrics: {'utility_surplus': {'model_a': 124.9325, 'model_b': 134.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 10 completed successfully
Thu Nov 20 10:34:21 AM CET 2025: Completed iteration 10

=== Iteration 11/300 ===
Thu Nov 20 10:34:21 AM CET 2025: Starting resource allocation negotiation iteration 11
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.30s (total since entry 4.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.70s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=10 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=380.00, marketing=320.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=88.93, marketing=58.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=291.07 = surplus=88.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=320.00 - BATNA=261.96 = surplus=58.04
âœ… Calculated utility_surplus: {'model_a': 88.9325, 'model_b': 58.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 380, 'model_b': 320}
Metrics: {'utility_surplus': {'model_a': 88.9325, 'model_b': 58.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 11 completed successfully
Thu Nov 20 10:34:48 AM CET 2025: Completed iteration 11

=== Iteration 12/300 ===
Thu Nov 20 10:34:48 AM CET 2025: Starting resource allocation negotiation iteration 12
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.08s (total since entry 5.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.15s (total since entry 10.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 12 completed successfully
Thu Nov 20 10:35:16 AM CET 2025: Completed iteration 12

=== Iteration 13/300 ===
Thu Nov 20 10:35:16 AM CET 2025: Starting resource allocation negotiation iteration 13
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.15s (total since entry 9.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=26 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=105.30, marketing=119.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=258.03 = surplus=119.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=286.70 = surplus=105.30
âœ… Calculated utility_surplus: {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 13 completed successfully
Thu Nov 20 10:35:49 AM CET 2025: Completed iteration 13

=== Iteration 14/300 ===
Thu Nov 20 10:35:49 AM CET 2025: Starting resource allocation negotiation iteration 14
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 29}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 29}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 29.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=29 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 14 completed successfully
Thu Nov 20 10:36:20 AM CET 2025: Completed iteration 14

=== Iteration 15/300 ===
Thu Nov 20 10:36:20 AM CET 2025: Starting resource allocation negotiation iteration 15
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.34s (total since entry 5.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.32s (total since entry 9.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 15 completed successfully
Thu Nov 20 10:36:53 AM CET 2025: Completed iteration 15

=== Iteration 16/300 ===
Thu Nov 20 10:36:53 AM CET 2025: Starting resource allocation negotiation iteration 16
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.61s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.06s (total since entry 5.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.69s (total since entry 9.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22.5 (made in round 2)
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=22.5
ğŸ” [BATNA DEBUG] Using BATNA from offer round 2 (offer made), agreement in round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=415.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=119.50, marketing=124.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=415.00 - BATNA=295.50 = surplus=119.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=265.95 = surplus=124.05
âœ… Calculated utility_surplus: {'model_a': 119.5, 'model_b': 124.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 415.0, 'model_b': 390.0}
Metrics: {'utility_surplus': {'model_a': 119.5, 'model_b': 124.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 16 completed successfully
Thu Nov 20 10:37:19 AM CET 2025: Completed iteration 16

=== Iteration 17/300 ===
Thu Nov 20 10:37:19 AM CET 2025: Starting resource allocation negotiation iteration 17
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.39s (total since entry 10.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=10 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=380.00, marketing=320.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=88.93, marketing=58.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=291.07 = surplus=88.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=320.00 - BATNA=261.96 = surplus=58.04
âœ… Calculated utility_surplus: {'model_a': 88.9325, 'model_b': 58.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 380, 'model_b': 320}
Metrics: {'utility_surplus': {'model_a': 88.9325, 'model_b': 58.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 17 completed successfully
Thu Nov 20 10:38:01 AM CET 2025: Completed iteration 17

=== Iteration 18/300 ===
Thu Nov 20 10:38:01 AM CET 2025: Starting resource allocation negotiation iteration 18
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.03s (total since entry 11.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 18 completed successfully
Thu Nov 20 10:38:33 AM CET 2025: Completed iteration 18

=== Iteration 19/300 ===
Thu Nov 20 10:38:33 AM CET 2025: Starting resource allocation negotiation iteration 19
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 5.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=5 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 60,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 60,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 60,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=60, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 19 completed successfully
Thu Nov 20 10:39:07 AM CET 2025: Completed iteration 19

=== Iteration 20/300 ===
Thu Nov 20 10:39:07 AM CET 2025: Starting resource allocation negotiation iteration 20
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.61s (total since entry 5.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.71s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=360.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=98.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=291.07 = surplus=118.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=261.96 = surplus=98.04
âœ… Calculated utility_surplus: {'model_a': 118.9325, 'model_b': 98.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 410, 'model_b': 360}
Metrics: {'utility_surplus': {'model_a': 118.9325, 'model_b': 98.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 20 completed successfully
Thu Nov 20 10:39:36 AM CET 2025: Completed iteration 20

=== Iteration 21/300 ===
Thu Nov 20 10:39:36 AM CET 2025: Starting resource allocation negotiation iteration 21
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.34s (total since entry 6.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.44s (total since entry 10.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 21 completed successfully
Thu Nov 20 10:40:18 AM CET 2025: Completed iteration 21

=== Iteration 22/300 ===
Thu Nov 20 10:40:18 AM CET 2025: Starting resource allocation negotiation iteration 22
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.71s (total since entry 6.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.14s (total since entry 10.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=291.07 = surplus=148.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 148.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 148.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 22 completed successfully
Thu Nov 20 10:40:48 AM CET 2025: Completed iteration 22

=== Iteration 23/300 ===
Thu Nov 20 10:40:48 AM CET 2025: Starting resource allocation negotiation iteration 23
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.28s (total since entry 11.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 23 completed successfully
Thu Nov 20 10:41:22 AM CET 2025: Completed iteration 23

=== Iteration 24/300 ===
Thu Nov 20 10:41:22 AM CET 2025: Starting resource allocation negotiation iteration 24
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=418.00, marketing=394.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=131.30, marketing=135.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=258.03 = surplus=135.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=418.00 - BATNA=286.70 = surplus=131.30
âœ… Calculated utility_surplus: {'model_a': 135.96866125000003, 'model_b': 131.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 418, 'model_a': 394}
Metrics: {'utility_surplus': {'model_a': 135.96866125000003, 'model_b': 131.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 24 completed successfully
Thu Nov 20 10:42:00 AM CET 2025: Completed iteration 24

=== Iteration 25/300 ===
Thu Nov 20 10:42:00 AM CET 2025: Starting resource allocation negotiation iteration 25
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.29s (total since entry 4.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.71s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 25 completed successfully
Thu Nov 20 10:42:36 AM CET 2025: Completed iteration 25

=== Iteration 26/300 ===
Thu Nov 20 10:42:36 AM CET 2025: Starting resource allocation negotiation iteration 26
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.09s (total since entry 5.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=34, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=388.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=123.30, marketing=129.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=388.00 - BATNA=258.03 = surplus=129.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=286.70 = surplus=123.30
âœ… Calculated utility_surplus: {'model_a': 129.96866125000003, 'model_b': 123.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 410, 'model_a': 388}
Metrics: {'utility_surplus': {'model_a': 129.96866125000003, 'model_b': 123.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 26 completed successfully
Thu Nov 20 10:43:10 AM CET 2025: Completed iteration 26

=== Iteration 27/300 ===
Thu Nov 20 10:43:10 AM CET 2025: Starting resource allocation negotiation iteration 27
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.33s (total since entry 5.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.66s (total since entry 11.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=300.00 = surplus=140.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=270.00 = surplus=130.00
âœ… Calculated utility_surplus: {'model_a': 140.0, 'model_b': 130.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 140.0, 'model_b': 130.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 27 completed successfully
Thu Nov 20 10:43:38 AM CET 2025: Completed iteration 27

=== Iteration 28/300 ===
Thu Nov 20 10:43:38 AM CET 2025: Starting resource allocation negotiation iteration 28
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=24 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=125.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=384.00 - BATNA=258.03 = surplus=125.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 125.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 384}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 125.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 28 completed successfully
Thu Nov 20 10:44:13 AM CET 2025: Completed iteration 28

=== Iteration 29/300 ===
Thu Nov 20 10:44:13 AM CET 2025: Starting resource allocation negotiation iteration 29
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 29}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 29}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 29.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=29 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=29 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=29
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=470.00, marketing=454.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=183.30, marketing=195.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=470.00 - BATNA=286.70 = surplus=183.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=454.00 - BATNA=258.03 = surplus=195.97
âœ… Calculated utility_surplus: {'model_a': 183.29851250000002, 'model_b': 195.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 470, 'model_b': 454}
Metrics: {'utility_surplus': {'model_a': 183.29851250000002, 'model_b': 195.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 29 completed successfully
Thu Nov 20 10:44:51 AM CET 2025: Completed iteration 29

=== Iteration 30/300 ===
Thu Nov 20 10:44:51 AM CET 2025: Starting resource allocation negotiation iteration 30
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.50s (total since entry 6.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.56s (total since entry 11.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 30 completed successfully
Thu Nov 20 10:45:27 AM CET 2025: Completed iteration 30

=== Iteration 31/300 ===
Thu Nov 20 10:45:27 AM CET 2025: Starting resource allocation negotiation iteration 31
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.52s (total since entry 5.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.27s (total since entry 11.77s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=26 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=291.07 = surplus=120.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 120.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 412, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 120.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 31 completed successfully
Thu Nov 20 10:45:58 AM CET 2025: Completed iteration 31

=== Iteration 32/300 ===
Thu Nov 20 10:45:58 AM CET 2025: Starting resource allocation negotiation iteration 32
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 32 completed successfully
Thu Nov 20 10:46:34 AM CET 2025: Completed iteration 32

=== Iteration 33/300 ===
Thu Nov 20 10:46:34 AM CET 2025: Starting resource allocation negotiation iteration 33
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.23s (total since entry 5.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 33 completed successfully
Thu Nov 20 10:47:11 AM CET 2025: Completed iteration 33

=== Iteration 34/300 ===
Thu Nov 20 10:47:11 AM CET 2025: Starting resource allocation negotiation iteration 34
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.68s (total since entry 6.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.92s (total since entry 10.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 34 completed successfully
Thu Nov 20 10:47:51 AM CET 2025: Completed iteration 34

=== Iteration 35/300 ===
Thu Nov 20 10:47:51 AM CET 2025: Starting resource allocation negotiation iteration 35
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.50s (total since entry 5.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.94s (total since entry 11.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=37, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=416.00, marketing=382.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=124.93, marketing=120.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=382.00 - BATNA=261.96 = surplus=120.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=291.07 = surplus=124.93
âœ… Calculated utility_surplus: {'model_a': 120.03924999999998, 'model_b': 124.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 416, 'model_a': 382}
Metrics: {'utility_surplus': {'model_a': 120.03924999999998, 'model_b': 124.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 35 completed successfully
Thu Nov 20 10:48:27 AM CET 2025: Completed iteration 35

=== Iteration 36/300 ===
Thu Nov 20 10:48:27 AM CET 2025: Starting resource allocation negotiation iteration 36
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 36 completed successfully
Thu Nov 20 10:49:02 AM CET 2025: Completed iteration 36

=== Iteration 37/300 ===
Thu Nov 20 10:49:02 AM CET 2025: Starting resource allocation negotiation iteration 37
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.91s (total since entry 9.34s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 22.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=416.00, marketing=382.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=129.30, marketing=123.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=286.70 = surplus=129.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=382.00 - BATNA=258.03 = surplus=123.97
âœ… Calculated utility_surplus: {'model_a': 129.29851250000002, 'model_b': 123.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 416, 'model_b': 382}
Metrics: {'utility_surplus': {'model_a': 129.29851250000002, 'model_b': 123.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 37 completed successfully
Thu Nov 20 10:49:42 AM CET 2025: Completed iteration 37

=== Iteration 38/300 ===
Thu Nov 20 10:49:42 AM CET 2025: Starting resource allocation negotiation iteration 38
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.30s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 27.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=27, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
â° Maximum rounds (5) reached - Ending negotiation
ğŸ² [ROLE DEBUG] No Agreement - Development=model_a, Marketing=model_b
ğŸ² [ROLE DEBUG] model_a utility=0, model_b utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 38 completed successfully
Thu Nov 20 10:50:26 AM CET 2025: Completed iteration 38

=== Iteration 39/300 ===
Thu Nov 20 10:50:26 AM CET 2025: Starting resource allocation negotiation iteration 39
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.04s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 39 completed successfully
Thu Nov 20 10:51:07 AM CET 2025: Completed iteration 39

=== Iteration 40/300 ===
Thu Nov 20 10:51:07 AM CET 2025: Starting resource allocation negotiation iteration 40
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 17}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 17}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 17.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=17 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 17}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 17}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 17}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 17.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=17 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 40 completed successfully
Thu Nov 20 10:51:47 AM CET 2025: Completed iteration 40

=== Iteration 41/300 ===
Thu Nov 20 10:51:47 AM CET 2025: Starting resource allocation negotiation iteration 41
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.70s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 28\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 28\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 28\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 41 completed successfully
Thu Nov 20 10:52:26 AM CET 2025: Completed iteration 41

=== Iteration 42/300 ===
Thu Nov 20 10:52:26 AM CET 2025: Starting resource allocation negotiation iteration 42
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 42 completed successfully
Thu Nov 20 10:53:01 AM CET 2025: Completed iteration 42

=== Iteration 43/300 ===
Thu Nov 20 10:53:01 AM CET 2025: Starting resource allocation negotiation iteration 43
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.66s (total since entry 6.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.93s (total since entry 10.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=107.30, marketing=117.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=286.70 = surplus=107.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=376.00 - BATNA=258.03 = surplus=117.97
âœ… Calculated utility_surplus: {'model_a': 107.29851250000002, 'model_b': 117.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 394, 'model_b': 376}
Metrics: {'utility_surplus': {'model_a': 107.29851250000002, 'model_b': 117.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 43 completed successfully
Thu Nov 20 10:53:35 AM CET 2025: Completed iteration 43

=== Iteration 44/300 ===
Thu Nov 20 10:53:35 AM CET 2025: Starting resource allocation negotiation iteration 44
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.42s (total since entry 4.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.33s (total since entry 11.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22.5 (made in round 2)
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=22.5
ğŸ” [BATNA DEBUG] Using BATNA from offer round 2 (offer made), agreement in round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=415.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=119.50, marketing=124.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=415.00 - BATNA=295.50 = surplus=119.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=265.95 = surplus=124.05
âœ… Calculated utility_surplus: {'model_a': 119.5, 'model_b': 124.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 415.0, 'model_b': 390.0}
Metrics: {'utility_surplus': {'model_a': 119.5, 'model_b': 124.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 44 completed successfully
Thu Nov 20 10:54:10 AM CET 2025: Completed iteration 44

=== Iteration 45/300 ===
Thu Nov 20 10:54:10 AM CET 2025: Starting resource allocation negotiation iteration 45
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 32.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=32 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=30 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=30
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=500.00, marketing=480.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=208.93, marketing=218.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=261.96 = surplus=218.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=500.00 - BATNA=291.07 = surplus=208.93
âœ… Calculated utility_surplus: {'model_a': 218.03924999999998, 'model_b': 208.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 500, 'model_a': 480}
Metrics: {'utility_surplus': {'model_a': 218.03924999999998, 'model_b': 208.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 45 completed successfully
Thu Nov 20 10:54:42 AM CET 2025: Completed iteration 45

=== Iteration 46/300 ===
Thu Nov 20 10:54:42 AM CET 2025: Starting resource allocation negotiation iteration 46
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.80s (total since entry 5.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 46 completed successfully
Thu Nov 20 10:55:25 AM CET 2025: Completed iteration 46

=== Iteration 47/300 ===
Thu Nov 20 10:55:25 AM CET 2025: Starting resource allocation negotiation iteration 47
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.38s (total since entry 6.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 47 completed successfully
Thu Nov 20 10:55:56 AM CET 2025: Completed iteration 47

=== Iteration 48/300 ===
Thu Nov 20 10:55:56 AM CET 2025: Starting resource allocation negotiation iteration 48
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.36s (total since entry 5.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.24s (total since entry 11.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 48 completed successfully
Thu Nov 20 10:56:29 AM CET 2025: Completed iteration 48

=== Iteration 49/300 ===
Thu Nov 20 10:56:29 AM CET 2025: Starting resource allocation negotiation iteration 49
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.86s (total since entry 9.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=258.03 = surplus=127.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=286.70 = surplus=125.30
âœ… Calculated utility_surplus: {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 49 completed successfully
Thu Nov 20 10:57:04 AM CET 2025: Completed iteration 49

=== Iteration 50/300 ===
Thu Nov 20 10:57:04 AM CET 2025: Starting resource allocation negotiation iteration 50
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 7,\n  "cpu_hours": 9\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 7,\n  "cpu_hours": 9\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 7,\n  "cpu_hours": 9\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 7, 'cpu_hours': 9}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 7, 'cpu_hours': 9}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 7.0, 'cpu_hours': 9.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=7, CPU=9 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 50 completed successfully
Thu Nov 20 10:57:35 AM CET 2025: Completed iteration 50

=== Iteration 51/300 ===
Thu Nov 20 10:57:35 AM CET 2025: Starting resource allocation negotiation iteration 51
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.68s (total since entry 9.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=488.00, marketing=464.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=201.30, marketing=205.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=488.00 - BATNA=286.70 = surplus=201.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=464.00 - BATNA=258.03 = surplus=205.97
âœ… Calculated utility_surplus: {'model_a': 201.29851250000002, 'model_b': 205.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 488, 'model_b': 464}
Metrics: {'utility_surplus': {'model_a': 201.29851250000002, 'model_b': 205.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 51 completed successfully
Thu Nov 20 10:58:17 AM CET 2025: Completed iteration 51

=== Iteration 52/300 ===
Thu Nov 20 10:58:17 AM CET 2025: Starting resource allocation negotiation iteration 52
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.61s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.65s (total since entry 6.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 29}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 29}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 29}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 29}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 29}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 29.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=29 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 52 completed successfully
Thu Nov 20 10:58:56 AM CET 2025: Completed iteration 52

=== Iteration 53/300 ===
Thu Nov 20 10:58:56 AM CET 2025: Starting resource allocation negotiation iteration 53
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.73s (total since entry 6.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.31s (total since entry 9.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 53 completed successfully
Thu Nov 20 10:59:39 AM CET 2025: Completed iteration 53

=== Iteration 54/300 ===
Thu Nov 20 10:59:39 AM CET 2025: Starting resource allocation negotiation iteration 54
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.65s (total since entry 6.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.04s (total since entry 9.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 32.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=32 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 45}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 45}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 45}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 45}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 45}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 45.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=45 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 54 completed successfully
Thu Nov 20 11:00:13 AM CET 2025: Completed iteration 54

=== Iteration 55/300 ===
Thu Nov 20 11:00:13 AM CET 2025: Starting resource allocation negotiation iteration 55
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.33s (total since entry 5.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.78s (total since entry 11.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=291.07 = surplus=148.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 148.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 148.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 55 completed successfully
Thu Nov 20 11:00:43 AM CET 2025: Completed iteration 55

=== Iteration 56/300 ===
Thu Nov 20 11:00:43 AM CET 2025: Starting resource allocation negotiation iteration 56
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.29s (total since entry 5.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.26s (total since entry 11.77s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 56 completed successfully
Thu Nov 20 11:01:26 AM CET 2025: Completed iteration 56

=== Iteration 57/300 ===
Thu Nov 20 11:01:26 AM CET 2025: Starting resource allocation negotiation iteration 57
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.56s (total since entry 5.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
8, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.31s (total since entry 11.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=258.03 = surplus=141.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=286.70 = surplus=153.30
âœ… Calculated utility_surplus: {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 57 completed successfully
Thu Nov 20 11:02:01 AM CET 2025: Completed iteration 57

=== Iteration 58/300 ===
Thu Nov 20 11:02:01 AM CET 2025: Starting resource allocation negotiation iteration 58
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=286.70 = surplus=143.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=258.03 = surplus=151.97
âœ… Calculated utility_surplus: {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 58 completed successfully
Thu Nov 20 11:02:36 AM CET 2025: Completed iteration 58

=== Iteration 59/300 ===
Thu Nov 20 11:02:36 AM CET 2025: Starting resource allocation negotiation iteration 59
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=258.03 = surplus=157.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=424.00 - BATNA=286.70 = surplus=137.30
âœ… Calculated utility_surplus: {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 424, 'model_a': 416}
Metrics: {'utility_surplus': {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 59 completed successfully
Thu Nov 20 11:03:15 AM CET 2025: Completed iteration 59

=== Iteration 60/300 ===
Thu Nov 20 11:03:15 AM CET 2025: Starting resource allocation negotiation iteration 60
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.71s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=102.93, marketing=114.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=376.00 - BATNA=261.96 = surplus=114.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=394.00 - BATNA=291.07 = surplus=102.93
âœ… Calculated utility_surplus: {'model_a': 114.03924999999998, 'model_b': 102.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 394, 'model_a': 376}
Metrics: {'utility_surplus': {'model_a': 114.03924999999998, 'model_b': 102.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 60 completed successfully
Thu Nov 20 11:03:47 AM CET 2025: Completed iteration 60

=== Iteration 61/300 ===
Thu Nov 20 11:03:47 AM CET 2025: Starting resource allocation negotiation iteration 61
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.66s (total since entry 6.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.99s (total since entry 9.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 50.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=50 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=38, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=38, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=436.00, marketing=404.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=149.30, marketing=145.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=436.00 - BATNA=286.70 = surplus=149.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=404.00 - BATNA=258.03 = surplus=145.97
âœ… Calculated utility_surplus: {'model_a': 149.29851250000002, 'model_b': 145.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 436, 'model_b': 404}
Metrics: {'utility_surplus': {'model_a': 149.29851250000002, 'model_b': 145.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 61 completed successfully
Thu Nov 20 11:04:24 AM CET 2025: Completed iteration 61

=== Iteration 62/300 ===
Thu Nov 20 11:04:24 AM CET 2025: Starting resource allocation negotiation iteration 62
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.35s (total since entry 5.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.53s (total since entry 11.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 62 completed successfully
Thu Nov 20 11:04:54 AM CET 2025: Completed iteration 62

=== Iteration 63/300 ===
Thu Nov 20 11:04:54 AM CET 2025: Starting resource allocation negotiation iteration 63
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.42s (total since entry 4.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.02s (total since entry 10.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=36, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=36, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=432.00, marketing=408.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.93, marketing=146.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=408.00 - BATNA=261.96 = surplus=146.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=432.00 - BATNA=291.07 = surplus=140.93
âœ… Calculated utility_surplus: {'model_a': 146.03924999999998, 'model_b': 140.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 432, 'model_a': 408}
Metrics: {'utility_surplus': {'model_a': 146.03924999999998, 'model_b': 140.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 63 completed successfully
Thu Nov 20 11:05:25 AM CET 2025: Completed iteration 63

=== Iteration 64/300 ===
Thu Nov 20 11:05:25 AM CET 2025: Starting resource allocation negotiation iteration 64
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.53s (total since entry 5.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.86s (total since entry 9.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=37, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=434.00, marketing=406.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=147.30, marketing=147.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=434.00 - BATNA=286.70 = surplus=147.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=406.00 - BATNA=258.03 = surplus=147.97
âœ… Calculated utility_surplus: {'model_a': 147.29851250000002, 'model_b': 147.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 434, 'model_b': 406}
Metrics: {'utility_surplus': {'model_a': 147.29851250000002, 'model_b': 147.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 64 completed successfully
Thu Nov 20 11:06:03 AM CET 2025: Completed iteration 64

=== Iteration 65/300 ===
Thu Nov 20 11:06:03 AM CET 2025: Starting resource allocation negotiation iteration 65
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=26 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=26 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=291.07 = surplus=120.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 120.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 412, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 120.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 65 completed successfully
Thu Nov 20 11:06:35 AM CET 2025: Completed iteration 65

=== Iteration 66/300 ===
Thu Nov 20 11:06:35 AM CET 2025: Starting resource allocation negotiation iteration 66
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.61s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.51s (total since entry 6.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.32s (total since entry 9.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 66 completed successfully
Thu Nov 20 11:07:07 AM CET 2025: Completed iteration 66

=== Iteration 67/300 ===
Thu Nov 20 11:07:07 AM CET 2025: Starting resource allocation negotiation iteration 67
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.56s (total since entry 6.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=388.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=126.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=388.00 - BATNA=261.96 = surplus=126.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=291.07 = surplus=118.93
âœ… Calculated utility_surplus: {'model_a': 126.03924999999998, 'model_b': 118.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 410, 'model_a': 388}
Metrics: {'utility_surplus': {'model_a': 126.03924999999998, 'model_b': 118.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 67 completed successfully
Thu Nov 20 11:07:43 AM CET 2025: Completed iteration 67

=== Iteration 68/300 ===
Thu Nov 20 11:07:43 AM CET 2025: Starting resource allocation negotiation iteration 68
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.60s (total since entry 6.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.42s (total since entry 9.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=370.00, marketing=330.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=83.30, marketing=71.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=286.70 = surplus=83.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=330.00 - BATNA=258.03 = surplus=71.97
âœ… Calculated utility_surplus: {'model_a': 83.29851250000002, 'model_b': 71.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 370, 'model_b': 330}
Metrics: {'utility_surplus': {'model_a': 83.29851250000002, 'model_b': 71.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 68 completed successfully
Thu Nov 20 11:08:18 AM CET 2025: Completed iteration 68

=== Iteration 69/300 ===
Thu Nov 20 11:08:18 AM CET 2025: Starting resource allocation negotiation iteration 69
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.71s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 69 completed successfully
Thu Nov 20 11:08:59 AM CET 2025: Completed iteration 69

=== Iteration 70/300 ===
Thu Nov 20 11:08:59 AM CET 2025: Starting resource allocation negotiation iteration 70
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.37s (total since entry 6.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27.5 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27.5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27.5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27.5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27.5 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=27.5 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=27.5
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=445.00, marketing=430.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=158.30, marketing=171.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=258.03 = surplus=171.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=445.00 - BATNA=286.70 = surplus=158.30
âœ… Calculated utility_surplus: {'model_a': 171.96866125000003, 'model_b': 158.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 445.0, 'model_a': 430.0}
Metrics: {'utility_surplus': {'model_a': 171.96866125000003, 'model_b': 158.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 70 completed successfully
Thu Nov 20 11:09:38 AM CET 2025: Completed iteration 70

=== Iteration 71/300 ===
Thu Nov 20 11:09:38 AM CET 2025: Starting resource allocation negotiation iteration 71
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.29s (total since entry 4.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.79s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 71 completed successfully
Thu Nov 20 11:10:14 AM CET 2025: Completed iteration 71

=== Iteration 72/300 ===
Thu Nov 20 11:10:14 AM CET 2025: Starting resource allocation negotiation iteration 72
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.72s (total since entry 5.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 75, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 75, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 75, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 75, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 75, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 75.0, 'cpu_hours': 25.0}
âš ï¸ Player model_b offer violates constraints: GPU=75, CPU=25
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=33, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=105.30, marketing=119.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=258.03 = surplus=119.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=286.70 = surplus=105.30
âœ… Calculated utility_surplus: {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 72 completed successfully
Thu Nov 20 11:10:53 AM CET 2025: Completed iteration 72

=== Iteration 73/300 ===
Thu Nov 20 11:10:53 AM CET 2025: Starting resource allocation negotiation iteration 73
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=36, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=36, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=36, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=426.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=134.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=291.07 = surplus=134.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 134.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 426, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 134.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 73 completed successfully
Thu Nov 20 11:11:26 AM CET 2025: Completed iteration 73

=== Iteration 74/300 ===
Thu Nov 20 11:11:26 AM CET 2025: Starting resource allocation negotiation iteration 74
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.63s (total since entry 5.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=258.03 = surplus=127.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=286.70 = surplus=125.30
âœ… Calculated utility_surplus: {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 74 completed successfully
Thu Nov 20 11:12:04 AM CET 2025: Completed iteration 74

=== Iteration 75/300 ===
Thu Nov 20 11:12:04 AM CET 2025: Starting resource allocation negotiation iteration 75
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=38, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=38, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=436.00, marketing=404.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=149.30, marketing=145.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=404.00 - BATNA=258.03 = surplus=145.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=436.00 - BATNA=286.70 = surplus=149.30
âœ… Calculated utility_surplus: {'model_a': 145.96866125000003, 'model_b': 149.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 436, 'model_a': 404}
Metrics: {'utility_surplus': {'model_a': 145.96866125000003, 'model_b': 149.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 75 completed successfully
Thu Nov 20 11:12:40 AM CET 2025: Completed iteration 75

=== Iteration 76/300 ===
Thu Nov 20 11:12:40 AM CET 2025: Starting resource allocation negotiation iteration 76
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.98s (total since entry 5.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 76 completed successfully
Thu Nov 20 11:13:18 AM CET 2025: Completed iteration 76

=== Iteration 77/300 ===
Thu Nov 20 11:13:18 AM CET 2025: Starting resource allocation negotiation iteration 77
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=105.30, marketing=119.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=258.03 = surplus=119.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=286.70 = surplus=105.30
âœ… Calculated utility_surplus: {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 119.96866125000003, 'model_b': 105.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 77 completed successfully
Thu Nov 20 11:13:51 AM CET 2025: Completed iteration 77

=== Iteration 78/300 ===
Thu Nov 20 11:13:51 AM CET 2025: Starting resource allocation negotiation iteration 78
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.27s (total since entry 5.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.15s (total since entry 11.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=270.00 = surplus=130.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=300.00 = surplus=140.00
âœ… Calculated utility_surplus: {'model_a': 130.0, 'model_b': 140.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 130.0, 'model_b': 140.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 78 completed successfully
Thu Nov 20 11:14:22 AM CET 2025: Completed iteration 78

=== Iteration 79/300 ===
Thu Nov 20 11:14:22 AM CET 2025: Starting resource allocation negotiation iteration 79
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.40s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 60,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 60,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 60,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=60, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=60, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=60, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=600.00, marketing=520.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=308.93, marketing=258.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=520.00 - BATNA=261.96 = surplus=258.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=600.00 - BATNA=291.07 = surplus=308.93
âœ… Calculated utility_surplus: {'model_a': 258.03925, 'model_b': 308.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 600, 'model_a': 520}
Metrics: {'utility_surplus': {'model_a': 258.03925, 'model_b': 308.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 79 completed successfully
Thu Nov 20 11:14:55 AM CET 2025: Completed iteration 79

=== Iteration 80/300 ===
Thu Nov 20 11:14:55 AM CET 2025: Starting resource allocation negotiation iteration 80
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=26 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=286.70 = surplus=143.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=258.03 = surplus=151.97
âœ… Calculated utility_surplus: {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 80 completed successfully
Thu Nov 20 11:15:29 AM CET 2025: Completed iteration 80

=== Iteration 81/300 ===
Thu Nov 20 11:15:29 AM CET 2025: Starting resource allocation negotiation iteration 81
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.62s (total since entry 5.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=416.00, marketing=382.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=129.30, marketing=123.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=382.00 - BATNA=258.03 = surplus=123.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=286.70 = surplus=129.30
âœ… Calculated utility_surplus: {'model_a': 123.96866125000003, 'model_b': 129.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 416, 'model_a': 382}
Metrics: {'utility_surplus': {'model_a': 123.96866125000003, 'model_b': 129.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 81 completed successfully
Thu Nov 20 11:16:13 AM CET 2025: Completed iteration 81

=== Iteration 82/300 ===
Thu Nov 20 11:16:13 AM CET 2025: Starting resource allocation negotiation iteration 82
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=258.03 = surplus=127.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=286.70 = surplus=125.30
âœ… Calculated utility_surplus: {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 82 completed successfully
Thu Nov 20 11:16:47 AM CET 2025: Completed iteration 82

=== Iteration 83/300 ===
Thu Nov 20 11:16:47 AM CET 2025: Starting resource allocation negotiation iteration 83
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.67s (total since entry 5.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.72s (total since entry 9.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 83 completed successfully
Thu Nov 20 11:17:25 AM CET 2025: Completed iteration 83

=== Iteration 84/300 ===
Thu Nov 20 11:17:25 AM CET 2025: Starting resource allocation negotiation iteration 84
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.32s (total since entry 5.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.94s (total since entry 11.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=122.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=384.00 - BATNA=261.96 = surplus=122.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 122.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 384}
Metrics: {'utility_surplus': {'model_a': 122.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 84 completed successfully
Thu Nov 20 11:17:59 AM CET 2025: Completed iteration 84

=== Iteration 85/300 ===
Thu Nov 20 11:17:59 AM CET 2025: Starting resource allocation negotiation iteration 85
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.62s (total since entry 6.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.44s (total since entry 11.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=27 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 85 completed successfully
Thu Nov 20 11:18:33 AM CET 2025: Completed iteration 85

=== Iteration 86/300 ===
Thu Nov 20 11:18:33 AM CET 2025: Starting resource allocation negotiation iteration 86
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=388.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=126.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=291.07 = surplus=118.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=388.00 - BATNA=261.96 = surplus=126.04
âœ… Calculated utility_surplus: {'model_a': 118.9325, 'model_b': 126.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 410, 'model_b': 388}
Metrics: {'utility_surplus': {'model_a': 118.9325, 'model_b': 126.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 86 completed successfully
Thu Nov 20 11:19:11 AM CET 2025: Completed iteration 86

=== Iteration 87/300 ===
Thu Nov 20 11:19:11 AM CET 2025: Starting resource allocation negotiation iteration 87
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.12s (total since entry 9.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 87 completed successfully
Thu Nov 20 11:19:46 AM CET 2025: Completed iteration 87

=== Iteration 88/300 ===
Thu Nov 20 11:19:46 AM CET 2025: Starting resource allocation negotiation iteration 88
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.28s (total since entry 4.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.71s (total since entry 9.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=26 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "accept"\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "accept"\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=418.00, marketing=394.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=126.93, marketing=132.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=261.96 = surplus=132.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=418.00 - BATNA=291.07 = surplus=126.93
âœ… Calculated utility_surplus: {'model_a': 132.03924999999998, 'model_b': 126.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 418, 'model_a': 394}
Metrics: {'utility_surplus': {'model_a': 132.03924999999998, 'model_b': 126.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 88 completed successfully
Thu Nov 20 11:20:22 AM CET 2025: Completed iteration 88

=== Iteration 89/300 ===
Thu Nov 20 11:20:22 AM CET 2025: Starting resource allocation negotiation iteration 89
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.66s (total since entry 6.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.17s (total since entry 9.67s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 89 completed successfully
Thu Nov 20 11:20:54 AM CET 2025: Completed iteration 89

=== Iteration 90/300 ===
Thu Nov 20 11:20:54 AM CET 2025: Starting resource allocation negotiation iteration 90
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.49s (total since entry 5.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.91s (total since entry 11.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 90 completed successfully
Thu Nov 20 11:21:37 AM CET 2025: Completed iteration 90

=== Iteration 91/300 ===
Thu Nov 20 11:21:37 AM CET 2025: Starting resource allocation negotiation iteration 91
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.05s (total since entry 5.70s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.70s (total since entry 9.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 39.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=37, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=398.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=136.93, marketing=136.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=398.00 - BATNA=261.96 = surplus=136.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=428.00 - BATNA=291.07 = surplus=136.93
âœ… Calculated utility_surplus: {'model_a': 136.03924999999998, 'model_b': 136.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 428, 'model_a': 398}
Metrics: {'utility_surplus': {'model_a': 136.03924999999998, 'model_b': 136.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 91 completed successfully
Thu Nov 20 11:22:08 AM CET 2025: Completed iteration 91

=== Iteration 92/300 ===
Thu Nov 20 11:22:08 AM CET 2025: Starting resource allocation negotiation iteration 92
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 4.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.87s (total since entry 10.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=26 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=412.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=136.93, marketing=150.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=261.96 = surplus=150.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=428.00 - BATNA=291.07 = surplus=136.93
âœ… Calculated utility_surplus: {'model_a': 150.03924999999998, 'model_b': 136.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 428, 'model_a': 412}
Metrics: {'utility_surplus': {'model_a': 150.03924999999998, 'model_b': 136.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 92 completed successfully
Thu Nov 20 11:22:45 AM CET 2025: Completed iteration 92

=== Iteration 93/300 ===
Thu Nov 20 11:22:45 AM CET 2025: Starting resource allocation negotiation iteration 93
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.05s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 93 completed successfully
Thu Nov 20 11:23:25 AM CET 2025: Completed iteration 93

=== Iteration 94/300 ===
Thu Nov 20 11:23:25 AM CET 2025: Starting resource allocation negotiation iteration 94
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.91s (total since entry 10.39s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=360.00, marketing=340.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=68.93, marketing=78.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=360.00 - BATNA=291.07 = surplus=68.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=340.00 - BATNA=261.96 = surplus=78.04
âœ… Calculated utility_surplus: {'model_a': 68.9325, 'model_b': 78.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 360, 'model_b': 340}
Metrics: {'utility_surplus': {'model_a': 68.9325, 'model_b': 78.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 94 completed successfully
Thu Nov 20 11:24:05 AM CET 2025: Completed iteration 94

=== Iteration 95/300 ===
Thu Nov 20 11:24:05 AM CET 2025: Starting resource allocation negotiation iteration 95
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.03s (total since entry 9.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 27\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 27\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 27\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=34, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=364.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=105.30, marketing=105.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=392.00 - BATNA=286.70 = surplus=105.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=364.00 - BATNA=258.03 = surplus=105.97
âœ… Calculated utility_surplus: {'model_a': 105.29851250000002, 'model_b': 105.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 392, 'model_b': 364}
Metrics: {'utility_surplus': {'model_a': 105.29851250000002, 'model_b': 105.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 95 completed successfully
Thu Nov 20 11:24:37 AM CET 2025: Completed iteration 95

=== Iteration 96/300 ===
Thu Nov 20 11:24:37 AM CET 2025: Starting resource allocation negotiation iteration 96
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.50s (total since entry 5.18s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 21}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 21}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 21}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 21}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 21}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 21.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=21 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=38, CPU=21 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=38, CPU=21
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=396.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=134.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=396.00 - BATNA=261.96 = surplus=134.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 134.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 430, 'model_b': 396}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 134.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 96 completed successfully
Thu Nov 20 11:25:10 AM CET 2025: Completed iteration 96

=== Iteration 97/300 ===
Thu Nov 20 11:25:10 AM CET 2025: Starting resource allocation negotiation iteration 97
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.64s (total since entry 6.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.51s (total since entry 10.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=124.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=291.07 = surplus=120.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=261.96 = surplus=124.04
âœ… Calculated utility_surplus: {'model_a': 120.9325, 'model_b': 124.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 120.9325, 'model_b': 124.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 97 completed successfully
Thu Nov 20 11:25:42 AM CET 2025: Completed iteration 97

=== Iteration 98/300 ===
Thu Nov 20 11:25:42 AM CET 2025: Starting resource allocation negotiation iteration 98
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.63s (total since entry 6.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.30s (total since entry 11.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=402.00, marketing=396.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=110.93, marketing=134.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=402.00 - BATNA=291.07 = surplus=110.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=396.00 - BATNA=261.96 = surplus=134.04
âœ… Calculated utility_surplus: {'model_a': 110.9325, 'model_b': 134.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 402, 'model_b': 396}
Metrics: {'utility_surplus': {'model_a': 110.9325, 'model_b': 134.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 98 completed successfully
Thu Nov 20 11:26:20 AM CET 2025: Completed iteration 98

=== Iteration 99/300 ===
Thu Nov 20 11:26:20 AM CET 2025: Starting resource allocation negotiation iteration 99
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.95s (total since entry 10.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=26 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=33, CPU=26 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=420.00, marketing=406.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=133.30, marketing=147.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=406.00 - BATNA=258.03 = surplus=147.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=420.00 - BATNA=286.70 = surplus=133.30
âœ… Calculated utility_surplus: {'model_a': 147.96866125000003, 'model_b': 133.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 420, 'model_a': 406}
Metrics: {'utility_surplus': {'model_a': 147.96866125000003, 'model_b': 133.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 99 completed successfully
Thu Nov 20 11:26:57 AM CET 2025: Completed iteration 99

=== Iteration 100/300 ===
Thu Nov 20 11:26:57 AM CET 2025: Starting resource allocation negotiation iteration 100
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.82s (total since entry 11.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=300.00 = surplus=140.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=270.00 = surplus=130.00
âœ… Calculated utility_surplus: {'model_a': 140.0, 'model_b': 130.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 140.0, 'model_b': 130.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 100 completed successfully
Thu Nov 20 11:27:22 AM CET 2025: Completed iteration 100

=== Iteration 101/300 ===
Thu Nov 20 11:27:22 AM CET 2025: Starting resource allocation negotiation iteration 101
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.93, marketing=116.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=261.96 = surplus=116.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=291.07 = surplus=100.93
âœ… Calculated utility_surplus: {'model_a': 116.03924999999998, 'model_b': 100.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 116.03924999999998, 'model_b': 100.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 101 completed successfully
Thu Nov 20 11:27:55 AM CET 2025: Completed iteration 101

=== Iteration 102/300 ===
Thu Nov 20 11:27:55 AM CET 2025: Starting resource allocation negotiation iteration 102
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.62s (total since entry 6.34s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.06s (total since entry 9.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 102 completed successfully
Thu Nov 20 11:28:29 AM CET 2025: Completed iteration 102

=== Iteration 103/300 ===
Thu Nov 20 11:28:29 AM CET 2025: Starting resource allocation negotiation iteration 103
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 5.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.33s (total since entry 10.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=398.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=141.30, marketing=139.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=398.00 - BATNA=258.03 = surplus=139.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=428.00 - BATNA=286.70 = surplus=141.30
âœ… Calculated utility_surplus: {'model_a': 139.96866125000003, 'model_b': 141.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 428, 'model_a': 398}
Metrics: {'utility_surplus': {'model_a': 139.96866125000003, 'model_b': 141.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 103 completed successfully
Thu Nov 20 11:29:13 AM CET 2025: Completed iteration 103

=== Iteration 104/300 ===
Thu Nov 20 11:29:13 AM CET 2025: Starting resource allocation negotiation iteration 104
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=102.93, marketing=114.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=291.07 = surplus=102.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=376.00 - BATNA=261.96 = surplus=114.04
âœ… Calculated utility_surplus: {'model_a': 102.9325, 'model_b': 114.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 394, 'model_b': 376}
Metrics: {'utility_surplus': {'model_a': 102.9325, 'model_b': 114.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 104 completed successfully
Thu Nov 20 11:29:45 AM CET 2025: Completed iteration 104

=== Iteration 105/300 ===
Thu Nov 20 11:29:45 AM CET 2025: Starting resource allocation negotiation iteration 105
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.39s (total since entry 4.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 105 completed successfully
Thu Nov 20 11:30:21 AM CET 2025: Completed iteration 105

=== Iteration 106/300 ===
Thu Nov 20 11:30:21 AM CET 2025: Starting resource allocation negotiation iteration 106
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.50s (total since entry 5.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.94s (total since entry 9.42s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=442.00, marketing=426.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=155.30, marketing=167.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=258.03 = surplus=167.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=442.00 - BATNA=286.70 = surplus=155.30
âœ… Calculated utility_surplus: {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 442, 'model_a': 426}
Metrics: {'utility_surplus': {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 106 completed successfully
Thu Nov 20 11:30:57 AM CET 2025: Completed iteration 106

=== Iteration 107/300 ===
Thu Nov 20 11:30:57 AM CET 2025: Starting resource allocation negotiation iteration 107
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.90s (total since entry 6.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.79s (total since entry 12.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=102.93, marketing=114.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=291.07 = surplus=102.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=376.00 - BATNA=261.96 = surplus=114.04
âœ… Calculated utility_surplus: {'model_a': 102.9325, 'model_b': 114.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 394, 'model_b': 376}
Metrics: {'utility_surplus': {'model_a': 102.9325, 'model_b': 114.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 107 completed successfully
Thu Nov 20 11:31:34 AM CET 2025: Completed iteration 107

=== Iteration 108/300 ===
Thu Nov 20 11:31:34 AM CET 2025: Starting resource allocation negotiation iteration 108
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.54s (total since entry 5.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.02s (total since entry 9.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 108 completed successfully
Thu Nov 20 11:32:11 AM CET 2025: Completed iteration 108

=== Iteration 109/300 ===
Thu Nov 20 11:32:11 AM CET 2025: Starting resource allocation negotiation iteration 109
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.48s (total since entry 5.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.22s (total since entry 9.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 109 completed successfully
Thu Nov 20 11:32:51 AM CET 2025: Completed iteration 109

=== Iteration 110/300 ===
Thu Nov 20 11:32:51 AM CET 2025: Starting resource allocation negotiation iteration 110
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.45s (total since entry 5.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.98s (total since entry 9.45s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=291.07 = surplus=148.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 148.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 148.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 110 completed successfully
Thu Nov 20 11:33:27 AM CET 2025: Completed iteration 110

=== Iteration 111/300 ===
Thu Nov 20 11:33:27 AM CET 2025: Starting resource allocation negotiation iteration 111
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.54s (total since entry 5.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.06s (total since entry 9.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=45, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=158.93, marketing=128.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=450.00 - BATNA=291.07 = surplus=158.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=261.96 = surplus=128.04
âœ… Calculated utility_surplus: {'model_a': 158.9325, 'model_b': 128.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 450, 'model_b': 390}
Metrics: {'utility_surplus': {'model_a': 158.9325, 'model_b': 128.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 111 completed successfully
Thu Nov 20 11:34:09 AM CET 2025: Completed iteration 111

=== Iteration 112/300 ===
Thu Nov 20 11:34:09 AM CET 2025: Starting resource allocation negotiation iteration 112
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.76s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.08s (total since entry 6.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.16s (total since entry 9.79s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 42, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 42, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 42, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 42, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 42, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 42.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=42, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 43, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 43, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 43, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 43, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 43, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 43.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=43, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 112 completed successfully
Thu Nov 20 11:34:54 AM CET 2025: Completed iteration 112

=== Iteration 113/300 ===
Thu Nov 20 11:34:54 AM CET 2025: Starting resource allocation negotiation iteration 113
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.65s (total since entry 5.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.51s (total since entry 10.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=38, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=38, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=436.00, marketing=404.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=149.30, marketing=145.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=436.00 - BATNA=286.70 = surplus=149.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=404.00 - BATNA=258.03 = surplus=145.97
âœ… Calculated utility_surplus: {'model_a': 149.29851250000002, 'model_b': 145.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 436, 'model_b': 404}
Metrics: {'utility_surplus': {'model_a': 149.29851250000002, 'model_b': 145.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 113 completed successfully
Thu Nov 20 11:35:31 AM CET 2025: Completed iteration 113

=== Iteration 114/300 ===
Thu Nov 20 11:35:31 AM CET 2025: Starting resource allocation negotiation iteration 114
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.68s (total since entry 5.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 114 completed successfully
Thu Nov 20 11:36:13 AM CET 2025: Completed iteration 114

=== Iteration 115/300 ===
Thu Nov 20 11:36:13 AM CET 2025: Starting resource allocation negotiation iteration 115
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.84s (total since entry 5.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.18s (total since entry 9.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 115 completed successfully
Thu Nov 20 11:36:50 AM CET 2025: Completed iteration 115

=== Iteration 116/300 ===
Thu Nov 20 11:36:50 AM CET 2025: Starting resource allocation negotiation iteration 116
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.86s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.20s (total since entry 7.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 116 completed successfully
Thu Nov 20 11:37:37 AM CET 2025: Completed iteration 116

=== Iteration 117/300 ===
Thu Nov 20 11:37:37 AM CET 2025: Starting resource allocation negotiation iteration 117
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.89s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.19s (total since entry 6.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.42s (total since entry 9.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 117 completed successfully
Thu Nov 20 11:38:26 AM CET 2025: Completed iteration 117

=== Iteration 118/300 ===
Thu Nov 20 11:38:26 AM CET 2025: Starting resource allocation negotiation iteration 118
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.65s (total since entry 5.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.17s (total since entry 9.68s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 27.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=27, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 32.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=32 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=28, CPU=32 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=28, CPU=32
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=416.00, marketing=424.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=124.93, marketing=162.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=424.00 - BATNA=261.96 = surplus=162.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=291.07 = surplus=124.93
âœ… Calculated utility_surplus: {'model_a': 162.03924999999998, 'model_b': 124.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 416, 'model_a': 424}
Metrics: {'utility_surplus': {'model_a': 162.03924999999998, 'model_b': 124.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 118 completed successfully
Thu Nov 20 11:39:04 AM CET 2025: Completed iteration 118

=== Iteration 119/300 ===
Thu Nov 20 11:39:04 AM CET 2025: Starting resource allocation negotiation iteration 119
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.85s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.27s (total since entry 7.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.34s (total since entry 10.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "accept"\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "accept"\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=33, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=33, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=396.00, marketing=374.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.93, marketing=112.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=396.00 - BATNA=291.07 = surplus=104.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=374.00 - BATNA=261.96 = surplus=112.04
âœ… Calculated utility_surplus: {'model_a': 104.9325, 'model_b': 112.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 396, 'model_b': 374}
Metrics: {'utility_surplus': {'model_a': 104.9325, 'model_b': 112.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 119 completed successfully
Thu Nov 20 11:39:35 AM CET 2025: Completed iteration 119

=== Iteration 120/300 ===
Thu Nov 20 11:39:35 AM CET 2025: Starting resource allocation negotiation iteration 120
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.72s (total since entry 5.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.89s (total since entry 11.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=265.95 = surplus=104.05
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=295.50 = surplus=104.50
âœ… Calculated utility_surplus: {'model_a': 104.05000000000001, 'model_b': 104.5}
âœ… Calculated risk_minimization: {'model_a': 50.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 104.05000000000001, 'model_b': 104.5}, 'risk_minimization': {'model_a': 50.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 120 completed successfully
Thu Nov 20 11:40:10 AM CET 2025: Completed iteration 120

=== Iteration 121/300 ===
Thu Nov 20 11:40:10 AM CET 2025: Starting resource allocation negotiation iteration 121
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.67s (total since entry 5.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.28s (total since entry 9.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 121 completed successfully
Thu Nov 20 11:40:55 AM CET 2025: Completed iteration 121

=== Iteration 122/300 ===
Thu Nov 20 11:40:55 AM CET 2025: Starting resource allocation negotiation iteration 122
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.73s (total since entry 5.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.28s (total since entry 9.79s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=10 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=380.00, marketing=320.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=93.30, marketing=61.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=286.70 = surplus=93.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=320.00 - BATNA=258.03 = surplus=61.97
âœ… Calculated utility_surplus: {'model_a': 93.29851250000002, 'model_b': 61.968661250000025}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 380, 'model_b': 320}
Metrics: {'utility_surplus': {'model_a': 93.29851250000002, 'model_b': 61.968661250000025}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 122 completed successfully
Thu Nov 20 11:41:27 AM CET 2025: Completed iteration 122

=== Iteration 123/300 ===
Thu Nov 20 11:41:27 AM CET 2025: Starting resource allocation negotiation iteration 123
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.33s (total since entry 6.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.59s (total since entry 13.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 123 completed successfully
Thu Nov 20 11:42:04 AM CET 2025: Completed iteration 123

=== Iteration 124/300 ===
Thu Nov 20 11:42:04 AM CET 2025: Starting resource allocation negotiation iteration 124
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.77s (total since entry 5.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.61s (total since entry 10.15s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=424.00 - BATNA=286.70 = surplus=137.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=258.03 = surplus=157.97
âœ… Calculated utility_surplus: {'model_a': 137.29851250000002, 'model_b': 157.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 424, 'model_b': 416}
Metrics: {'utility_surplus': {'model_a': 137.29851250000002, 'model_b': 157.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 124 completed successfully
Thu Nov 20 11:42:48 AM CET 2025: Completed iteration 124

=== Iteration 125/300 ===
Thu Nov 20 11:42:48 AM CET 2025: Starting resource allocation negotiation iteration 125
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 1.10s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.74s (total since entry 7.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 10.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 13.22s (total since entry 23.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 125 completed successfully
Thu Nov 20 11:43:52 AM CET 2025: Completed iteration 125

=== Iteration 126/300 ===
Thu Nov 20 11:43:52 AM CET 2025: Starting resource allocation negotiation iteration 126
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.94s (total since entry 5.66s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.17s (total since entry 12.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 126 completed successfully
Thu Nov 20 11:44:41 AM CET 2025: Completed iteration 126

=== Iteration 127/300 ===
Thu Nov 20 11:44:41 AM CET 2025: Starting resource allocation negotiation iteration 127
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.94s (total since entry 5.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.98s (total since entry 10.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 127 completed successfully
Thu Nov 20 11:45:30 AM CET 2025: Completed iteration 127

=== Iteration 128/300 ===
Thu Nov 20 11:45:30 AM CET 2025: Starting resource allocation negotiation iteration 128
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.88s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.91s (total since entry 5.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 37,\n    "cpu_hours": 23\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=38, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 39.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=39, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=258.03 = surplus=141.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=286.70 = surplus=153.30
âœ… Calculated utility_surplus: {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 128 completed successfully
Thu Nov 20 11:46:12 AM CET 2025: Completed iteration 128

=== Iteration 129/300 ===
Thu Nov 20 11:46:12 AM CET 2025: Starting resource allocation negotiation iteration 129
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.91s (total since entry 5.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=360.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=98.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=291.07 = surplus=118.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=261.96 = surplus=98.04
âœ… Calculated utility_surplus: {'model_a': 118.9325, 'model_b': 98.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 410, 'model_b': 360}
Metrics: {'utility_surplus': {'model_a': 118.9325, 'model_b': 98.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 129 completed successfully
Thu Nov 20 11:46:58 AM CET 2025: Completed iteration 129

=== Iteration 130/300 ===
Thu Nov 20 11:46:58 AM CET 2025: Starting resource allocation negotiation iteration 130
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.90s (total since entry 5.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.21s (total since entry 10.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=122.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=384.00 - BATNA=261.96 = surplus=122.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 122.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 400, 'model_b': 384}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 122.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 130 completed successfully
Thu Nov 20 11:47:44 AM CET 2025: Completed iteration 130

=== Iteration 131/300 ===
Thu Nov 20 11:47:44 AM CET 2025: Starting resource allocation negotiation iteration 131
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=261.96 = surplus=118.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=291.07 = surplus=98.93
âœ… Calculated utility_surplus: {'model_a': 118.03924999999998, 'model_b': 98.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 118.03924999999998, 'model_b': 98.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 131 completed successfully
Thu Nov 20 11:48:27 AM CET 2025: Completed iteration 131

=== Iteration 132/300 ===
Thu Nov 20 11:48:27 AM CET 2025: Starting resource allocation negotiation iteration 132
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.90s (total since entry 5.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.97s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=33, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=33, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=33, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=426.00, marketing=414.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=134.93, marketing=152.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=414.00 - BATNA=261.96 = surplus=152.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=426.00 - BATNA=291.07 = surplus=134.93
âœ… Calculated utility_surplus: {'model_a': 152.03924999999998, 'model_b': 134.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 426, 'model_a': 414}
Metrics: {'utility_surplus': {'model_a': 152.03924999999998, 'model_b': 134.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 132 completed successfully
Thu Nov 20 11:49:14 AM CET 2025: Completed iteration 132

=== Iteration 133/300 ===
Thu Nov 20 11:49:14 AM CET 2025: Starting resource allocation negotiation iteration 133
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.79s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 133 completed successfully
Thu Nov 20 11:50:00 AM CET 2025: Completed iteration 133

=== Iteration 134/300 ===
Thu Nov 20 11:50:00 AM CET 2025: Starting resource allocation negotiation iteration 134
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.87s (total since entry 5.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=258.03 = surplus=127.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=286.70 = surplus=125.30
âœ… Calculated utility_surplus: {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 134 completed successfully
Thu Nov 20 11:50:38 AM CET 2025: Completed iteration 134

=== Iteration 135/300 ===
Thu Nov 20 11:50:38 AM CET 2025: Starting resource allocation negotiation iteration 135
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.90s (total since entry 5.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.97s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=163.30, marketing=131.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=450.00 - BATNA=286.70 = surplus=163.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=258.03 = surplus=131.97
âœ… Calculated utility_surplus: {'model_a': 163.29851250000002, 'model_b': 131.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 450, 'model_b': 390}
Metrics: {'utility_surplus': {'model_a': 163.29851250000002, 'model_b': 131.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 135 completed successfully
Thu Nov 20 11:51:24 AM CET 2025: Completed iteration 135

=== Iteration 136/300 ===
Thu Nov 20 11:51:24 AM CET 2025: Starting resource allocation negotiation iteration 136
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.89s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.15s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.88s (total since entry 10.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 136 completed successfully
Thu Nov 20 11:52:12 AM CET 2025: Completed iteration 136

=== Iteration 137/300 ===
Thu Nov 20 11:52:12 AM CET 2025: Starting resource allocation negotiation iteration 137
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.34s (total since entry 7.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 13.48s (total since entry 14.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 32,\n  "cpu_hours": 24\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=261.96 = surplus=118.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=291.07 = surplus=98.93
âœ… Calculated utility_surplus: {'model_a': 118.03924999999998, 'model_b': 98.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 118.03924999999998, 'model_b': 98.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 137 completed successfully
Thu Nov 20 11:53:04 AM CET 2025: Completed iteration 137

=== Iteration 138/300 ===
Thu Nov 20 11:53:04 AM CET 2025: Starting resource allocation negotiation iteration 138
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.84s (total since entry 5.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.57s (total since entry 10.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=132.93, marketing=154.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=424.00 - BATNA=291.07 = surplus=132.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=261.96 = surplus=154.04
âœ… Calculated utility_surplus: {'model_a': 132.9325, 'model_b': 154.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 424, 'model_b': 416}
Metrics: {'utility_surplus': {'model_a': 132.9325, 'model_b': 154.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 138 completed successfully
Thu Nov 20 11:53:49 AM CET 2025: Completed iteration 138

=== Iteration 139/300 ===
Thu Nov 20 11:53:49 AM CET 2025: Starting resource allocation negotiation iteration 139
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.89s (total since entry 5.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 139 completed successfully
Thu Nov 20 11:54:27 AM CET 2025: Completed iteration 139

=== Iteration 140/300 ===
Thu Nov 20 11:54:27 AM CET 2025: Starting resource allocation negotiation iteration 140
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.77s (total since entry 5.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.49s (total since entry 10.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 140 completed successfully
Thu Nov 20 11:55:09 AM CET 2025: Completed iteration 140

=== Iteration 141/300 ===
Thu Nov 20 11:55:09 AM CET 2025: Starting resource allocation negotiation iteration 141
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.86s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.57s (total since entry 10.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=32, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=102.93, marketing=114.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=376.00 - BATNA=261.96 = surplus=114.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=394.00 - BATNA=291.07 = surplus=102.93
âœ… Calculated utility_surplus: {'model_a': 114.03924999999998, 'model_b': 102.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 394, 'model_a': 376}
Metrics: {'utility_surplus': {'model_a': 114.03924999999998, 'model_b': 102.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 141 completed successfully
Thu Nov 20 11:55:52 AM CET 2025: Completed iteration 141

=== Iteration 142/300 ===
Thu Nov 20 11:55:52 AM CET 2025: Starting resource allocation negotiation iteration 142
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.80s (total since entry 5.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.80s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 19}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 19}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 19}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 19}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 19}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 19.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=19 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 142 completed successfully
Thu Nov 20 11:56:33 AM CET 2025: Completed iteration 142

=== Iteration 143/300 ===
Thu Nov 20 11:56:33 AM CET 2025: Starting resource allocation negotiation iteration 143
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.94s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.69s (total since entry 7.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.93, marketing=116.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=392.00 - BATNA=291.07 = surplus=100.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=378.00 - BATNA=261.96 = surplus=116.04
âœ… Calculated utility_surplus: {'model_a': 100.9325, 'model_b': 116.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 392, 'model_b': 378}
Metrics: {'utility_surplus': {'model_a': 100.9325, 'model_b': 116.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 143 completed successfully
Thu Nov 20 11:57:12 AM CET 2025: Completed iteration 143

=== Iteration 144/300 ===
Thu Nov 20 11:57:12 AM CET 2025: Starting resource allocation negotiation iteration 144
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.22s (total since entry 6.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 13.08s (total since entry 13.77s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=37, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=37, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=434.00, marketing=406.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=142.93, marketing=144.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=434.00 - BATNA=291.07 = surplus=142.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=406.00 - BATNA=261.96 = surplus=144.04
âœ… Calculated utility_surplus: {'model_a': 142.9325, 'model_b': 144.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 434, 'model_b': 406}
Metrics: {'utility_surplus': {'model_a': 142.9325, 'model_b': 144.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 144 completed successfully
Thu Nov 20 11:57:53 AM CET 2025: Completed iteration 144

=== Iteration 145/300 ===
Thu Nov 20 11:57:53 AM CET 2025: Starting resource allocation negotiation iteration 145
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.82s (total since entry 5.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.61s (total since entry 11.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 32,\n    "cpu_hours": 23\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 32,\n    "cpu_hours": 23\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 32,\n    "cpu_hours": 23\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_b made offer GPU=32, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=33, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=426.00, marketing=414.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=139.30, marketing=155.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=286.70 = surplus=139.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=414.00 - BATNA=258.03 = surplus=155.97
âœ… Calculated utility_surplus: {'model_a': 139.29851250000002, 'model_b': 155.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 426, 'model_b': 414}
Metrics: {'utility_surplus': {'model_a': 139.29851250000002, 'model_b': 155.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 145 completed successfully
Thu Nov 20 11:58:40 AM CET 2025: Completed iteration 145

=== Iteration 146/300 ===
Thu Nov 20 11:58:40 AM CET 2025: Starting resource allocation negotiation iteration 146
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.84s (total since entry 5.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.35s (total since entry 11.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=394.00, marketing=376.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=102.93, marketing=114.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=394.00 - BATNA=291.07 = surplus=102.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=376.00 - BATNA=261.96 = surplus=114.04
âœ… Calculated utility_surplus: {'model_a': 102.9325, 'model_b': 114.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 394, 'model_b': 376}
Metrics: {'utility_surplus': {'model_a': 102.9325, 'model_b': 114.03924999999998}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 146 completed successfully
Thu Nov 20 11:59:23 AM CET 2025: Completed iteration 146

=== Iteration 147/300 ===
Thu Nov 20 11:59:23 AM CET 2025: Starting resource allocation negotiation iteration 147
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.76s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 147 completed successfully
Thu Nov 20 12:00:07 PM CET 2025: Completed iteration 147

=== Iteration 148/300 ===
Thu Nov 20 12:00:07 PM CET 2025: Starting resource allocation negotiation iteration 148
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.96s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=45, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=480.00, marketing=430.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=188.93, marketing=168.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=261.96 = surplus=168.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=480.00 - BATNA=291.07 = surplus=188.93
âœ… Calculated utility_surplus: {'model_a': 168.03924999999998, 'model_b': 188.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 480, 'model_a': 430}
Metrics: {'utility_surplus': {'model_a': 168.03924999999998, 'model_b': 188.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 148 completed successfully
Thu Nov 20 12:00:51 PM CET 2025: Completed iteration 148

=== Iteration 149/300 ===
Thu Nov 20 12:00:51 PM CET 2025: Starting resource allocation negotiation iteration 149
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 1.10s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.52s (total since entry 7.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.89s (total since entry 10.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 149 completed successfully
Thu Nov 20 12:01:36 PM CET 2025: Completed iteration 149

=== Iteration 150/300 ===
Thu Nov 20 12:01:36 PM CET 2025: Starting resource allocation negotiation iteration 150
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.88s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.11s (total since entry 7.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.86s (total since entry 12.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=45, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=45, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=510.00, marketing=470.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=218.93, marketing=208.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=470.00 - BATNA=261.96 = surplus=208.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=510.00 - BATNA=291.07 = surplus=218.93
âœ… Calculated utility_surplus: {'model_a': 208.03924999999998, 'model_b': 218.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 510, 'model_a': 470}
Metrics: {'utility_surplus': {'model_a': 208.03924999999998, 'model_b': 218.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 150 completed successfully
Thu Nov 20 12:02:18 PM CET 2025: Completed iteration 150

=== Iteration 151/300 ===
Thu Nov 20 12:02:18 PM CET 2025: Starting resource allocation negotiation iteration 151
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.03s (total since entry 6.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.87s (total since entry 13.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 45,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=258.03 = surplus=127.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=286.70 = surplus=125.30
âœ… Calculated utility_surplus: {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 127.96866125000003, 'model_b': 125.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 151 completed successfully
Thu Nov 20 12:02:59 PM CET 2025: Completed iteration 151

=== Iteration 152/300 ===
Thu Nov 20 12:02:59 PM CET 2025: Starting resource allocation negotiation iteration 152
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.88s (total since entry 5.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.82s (total since entry 13.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=360.00, marketing=340.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=73.30, marketing=81.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=360.00 - BATNA=286.70 = surplus=73.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=340.00 - BATNA=258.03 = surplus=81.97
âœ… Calculated utility_surplus: {'model_a': 73.29851250000002, 'model_b': 81.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 360, 'model_b': 340}
Metrics: {'utility_surplus': {'model_a': 73.29851250000002, 'model_b': 81.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 152 completed successfully
Thu Nov 20 12:03:44 PM CET 2025: Completed iteration 152

=== Iteration 153/300 ===
Thu Nov 20 12:03:44 PM CET 2025: Starting resource allocation negotiation iteration 153
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.85s (total since entry 5.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.94s (total since entry 12.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 153 completed successfully
Thu Nov 20 12:04:28 PM CET 2025: Completed iteration 153

=== Iteration 154/300 ===
Thu Nov 20 12:04:28 PM CET 2025: Starting resource allocation negotiation iteration 154
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.69s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.89s (total since entry 5.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.74s (total since entry 12.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 32.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=32 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 29.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=29, CPU=26 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 154 completed successfully
Thu Nov 20 12:05:21 PM CET 2025: Completed iteration 154

=== Iteration 155/300 ===
Thu Nov 20 12:05:21 PM CET 2025: Starting resource allocation negotiation iteration 155
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.80s (total since entry 7.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.40s (total since entry 12.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=158.93, marketing=128.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=261.96 = surplus=128.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=291.07 = surplus=158.93
âœ… Calculated utility_surplus: {'model_a': 128.03924999999998, 'model_b': 158.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 128.03924999999998, 'model_b': 158.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 155 completed successfully
Thu Nov 20 12:06:09 PM CET 2025: Completed iteration 155

=== Iteration 156/300 ===
Thu Nov 20 12:06:09 PM CET 2025: Starting resource allocation negotiation iteration 156
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.49s (total since entry 7.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.04s (total since entry 12.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 27.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=27, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=124.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=291.07 = surplus=120.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=261.96 = surplus=124.04
âœ… Calculated utility_surplus: {'model_a': 120.9325, 'model_b': 124.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 120.9325, 'model_b': 124.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 156 completed successfully
Thu Nov 20 12:06:50 PM CET 2025: Completed iteration 156

=== Iteration 157/300 ===
Thu Nov 20 12:06:50 PM CET 2025: Starting resource allocation negotiation iteration 157
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.09s (total since entry 6.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.91s (total since entry 13.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 157 completed successfully
Thu Nov 20 12:07:29 PM CET 2025: Completed iteration 157

=== Iteration 158/300 ===
Thu Nov 20 12:07:29 PM CET 2025: Starting resource allocation negotiation iteration 158
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.81s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.38s (total since entry 11.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=10 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=380.00, marketing=320.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=93.30, marketing=61.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=286.70 = surplus=93.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=320.00 - BATNA=258.03 = surplus=61.97
âœ… Calculated utility_surplus: {'model_a': 93.29851250000002, 'model_b': 61.968661250000025}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 380, 'model_b': 320}
Metrics: {'utility_surplus': {'model_a': 93.29851250000002, 'model_b': 61.968661250000025}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 158 completed successfully
Thu Nov 20 12:08:11 PM CET 2025: Completed iteration 158

=== Iteration 159/300 ===
Thu Nov 20 12:08:11 PM CET 2025: Starting resource allocation negotiation iteration 159
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.87s (total since entry 5.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=60, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=60, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=60, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=60, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=570.00, marketing=480.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=283.30, marketing=221.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=258.03 = surplus=221.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=570.00 - BATNA=286.70 = surplus=283.30
âœ… Calculated utility_surplus: {'model_a': 221.96866125000003, 'model_b': 283.2985125}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 570, 'model_a': 480}
Metrics: {'utility_surplus': {'model_a': 221.96866125000003, 'model_b': 283.2985125}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 159 completed successfully
Thu Nov 20 12:09:00 PM CET 2025: Completed iteration 159

=== Iteration 160/300 ===
Thu Nov 20 12:09:00 PM CET 2025: Starting resource allocation negotiation iteration 160
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.93s (total since entry 5.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.81s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=26 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=26 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=286.70 = surplus=125.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 125.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 125.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 160 completed successfully
Thu Nov 20 12:09:45 PM CET 2025: Completed iteration 160

=== Iteration 161/300 ===
Thu Nov 20 12:09:45 PM CET 2025: Starting resource allocation negotiation iteration 161
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.92s (total since entry 5.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.87s (total since entry 10.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=261.96 = surplus=118.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=291.07 = surplus=98.93
âœ… Calculated utility_surplus: {'model_a': 118.03924999999998, 'model_b': 98.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 118.03924999999998, 'model_b': 98.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 161 completed successfully
Thu Nov 20 12:10:28 PM CET 2025: Completed iteration 161

=== Iteration 162/300 ===
Thu Nov 20 12:10:28 PM CET 2025: Starting resource allocation negotiation iteration 162
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.81s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.60s (total since entry 7.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.68s (total since entry 10.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=291.07 = surplus=148.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 148.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 148.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 162 completed successfully
Thu Nov 20 12:11:08 PM CET 2025: Completed iteration 162

=== Iteration 163/300 ===
Thu Nov 20 12:11:08 PM CET 2025: Starting resource allocation negotiation iteration 163
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.17s (total since entry 6.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.99s (total since entry 13.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 163 completed successfully
Thu Nov 20 12:11:44 PM CET 2025: Completed iteration 163

=== Iteration 164/300 ===
Thu Nov 20 12:11:44 PM CET 2025: Starting resource allocation negotiation iteration 164
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.85s (total since entry 5.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.78s (total since entry 11.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 164 completed successfully
Thu Nov 20 12:12:25 PM CET 2025: Completed iteration 164

=== Iteration 165/300 ===
Thu Nov 20 12:12:25 PM CET 2025: Starting resource allocation negotiation iteration 165
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.88s (total since entry 5.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.84s (total since entry 10.37s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 5,\n    "cpu_hours": 5\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 60\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 60\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 60\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=30 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=30
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=500.00, marketing=480.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=213.30, marketing=221.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=258.03 = surplus=221.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=500.00 - BATNA=286.70 = surplus=213.30
âœ… Calculated utility_surplus: {'model_a': 221.96866125000003, 'model_b': 213.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 500, 'model_a': 480}
Metrics: {'utility_surplus': {'model_a': 221.96866125000003, 'model_b': 213.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 165 completed successfully
Thu Nov 20 12:13:12 PM CET 2025: Completed iteration 165

=== Iteration 166/300 ===
Thu Nov 20 12:13:12 PM CET 2025: Starting resource allocation negotiation iteration 166
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.66s (total since entry 6.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=286.70 = surplus=125.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=258.03 = surplus=127.97
âœ… Calculated utility_surplus: {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 166 completed successfully
Thu Nov 20 12:13:59 PM CET 2025: Completed iteration 166

=== Iteration 167/300 ===
Thu Nov 20 12:13:59 PM CET 2025: Starting resource allocation negotiation iteration 167
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.85s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.61s (total since entry 7.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.44s (total since entry 11.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 167 completed successfully
Thu Nov 20 12:14:34 PM CET 2025: Completed iteration 167

=== Iteration 168/300 ===
Thu Nov 20 12:14:34 PM CET 2025: Starting resource allocation negotiation iteration 168
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.87s (total since entry 5.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.97s (total since entry 11.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 18}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 18.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=18 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=18 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=18
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=136.93, marketing=122.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=384.00 - BATNA=261.96 = surplus=122.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=428.00 - BATNA=291.07 = surplus=136.93
âœ… Calculated utility_surplus: {'model_a': 122.03924999999998, 'model_b': 136.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 428, 'model_a': 384}
Metrics: {'utility_surplus': {'model_a': 122.03924999999998, 'model_b': 136.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 168 completed successfully
Thu Nov 20 12:15:12 PM CET 2025: Completed iteration 168

=== Iteration 169/300 ===
Thu Nov 20 12:15:12 PM CET 2025: Starting resource allocation negotiation iteration 169
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.88s (total since entry 5.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 29}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 29}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 29}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 29.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 169 completed successfully
Thu Nov 20 12:16:05 PM CET 2025: Completed iteration 169

=== Iteration 170/300 ===
Thu Nov 20 12:16:05 PM CET 2025: Starting resource allocation negotiation iteration 170
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.89s (total since entry 5.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=26 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=33, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 27\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 27\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 27\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=33, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=33, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=426.00, marketing=414.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=139.30, marketing=155.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=414.00 - BATNA=258.03 = surplus=155.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=426.00 - BATNA=286.70 = surplus=139.30
âœ… Calculated utility_surplus: {'model_a': 155.96866125000003, 'model_b': 139.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 426, 'model_a': 414}
Metrics: {'utility_surplus': {'model_a': 155.96866125000003, 'model_b': 139.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 170 completed successfully
Thu Nov 20 12:16:50 PM CET 2025: Completed iteration 170

=== Iteration 171/300 ===
Thu Nov 20 12:16:50 PM CET 2025: Starting resource allocation negotiation iteration 171
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.81s (total since entry 5.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.71s (total since entry 10.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 10\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 10\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 45,\n    "cpu_hours": 10\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=258.03 = surplus=141.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=286.70 = surplus=153.30
âœ… Calculated utility_surplus: {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 171 completed successfully
Thu Nov 20 12:17:28 PM CET 2025: Completed iteration 171

=== Iteration 172/300 ===
Thu Nov 20 12:17:28 PM CET 2025: Starting resource allocation negotiation iteration 172
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.72s (total since entry 7.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.85s (total since entry 11.44s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=33, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 172 completed successfully
Thu Nov 20 12:18:17 PM CET 2025: Completed iteration 172

=== Iteration 173/300 ===
Thu Nov 20 12:18:17 PM CET 2025: Starting resource allocation negotiation iteration 173
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.87s (total since entry 5.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.52s (total since entry 13.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=286.70 = surplus=143.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=258.03 = surplus=151.97
âœ… Calculated utility_surplus: {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 173 completed successfully
Thu Nov 20 12:19:00 PM CET 2025: Completed iteration 173

=== Iteration 174/300 ===
Thu Nov 20 12:19:00 PM CET 2025: Starting resource allocation negotiation iteration 174
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.12s (total since entry 5.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.37s (total since entry 11.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 22\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=45, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=163.30, marketing=131.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=258.03 = surplus=131.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=286.70 = surplus=163.30
âœ… Calculated utility_surplus: {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 174 completed successfully
Thu Nov 20 12:19:46 PM CET 2025: Completed iteration 174

=== Iteration 175/300 ===
Thu Nov 20 12:19:46 PM CET 2025: Starting resource allocation negotiation iteration 175
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.06s (total since entry 5.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.50s (total since entry 11.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 7,\n    "cpu_hours": 9\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 7,\n    "cpu_hours": 9\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 7,\n    "cpu_hours": 9\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 7, 'cpu_hours': 9}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 7, 'cpu_hours': 9}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 7.0, 'cpu_hours': 9.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=7, CPU=9 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=24 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=125.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=384.00 - BATNA=258.03 = surplus=125.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 125.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 384}
Metrics: {'utility_surplus': {'model_a': 125.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 175 completed successfully
Thu Nov 20 12:20:33 PM CET 2025: Completed iteration 175

=== Iteration 176/300 ===
Thu Nov 20 12:20:33 PM CET 2025: Starting resource allocation negotiation iteration 176
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.74s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.94s (total since entry 5.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.16s (total since entry 12.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 39}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 39}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 39}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 39}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 39}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 39.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=26 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=39 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=26 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=412.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=136.93, marketing=150.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=428.00 - BATNA=291.07 = surplus=136.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=261.96 = surplus=150.04
âœ… Calculated utility_surplus: {'model_a': 136.9325, 'model_b': 150.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 428, 'model_b': 412}
Metrics: {'utility_surplus': {'model_a': 136.9325, 'model_b': 150.03924999999998}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 176 completed successfully
Thu Nov 20 12:21:14 PM CET 2025: Completed iteration 176

=== Iteration 177/300 ===
Thu Nov 20 12:21:14 PM CET 2025: Starting resource allocation negotiation iteration 177
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.81s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.09s (total since entry 6.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.07s (total since entry 12.79s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 22.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=258.03 = surplus=151.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=430.00 - BATNA=286.70 = surplus=143.30
âœ… Calculated utility_surplus: {'model_a': 151.96866125000003, 'model_b': 143.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 430, 'model_a': 410}
Metrics: {'utility_surplus': {'model_a': 151.96866125000003, 'model_b': 143.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 177 completed successfully
Thu Nov 20 12:22:01 PM CET 2025: Completed iteration 177

=== Iteration 178/300 ===
Thu Nov 20 12:22:01 PM CET 2025: Starting resource allocation negotiation iteration 178
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.81s (total since entry 5.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "accept"\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "accept"\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=10 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=380.00, marketing=320.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=88.93, marketing=58.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=320.00 - BATNA=261.96 = surplus=58.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=291.07 = surplus=88.93
âœ… Calculated utility_surplus: {'model_a': 58.03924999999998, 'model_b': 88.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 380, 'model_a': 320}
Metrics: {'utility_surplus': {'model_a': 58.03924999999998, 'model_b': 88.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 178 completed successfully
Thu Nov 20 12:22:39 PM CET 2025: Completed iteration 178

=== Iteration 179/300 ===
Thu Nov 20 12:22:39 PM CET 2025: Starting resource allocation negotiation iteration 179
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.88s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.30s (total since entry 7.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.72s (total since entry 10.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 29}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 29}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 29}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 29.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=29 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=36, CPU=26 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=29 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=37, CPU=29
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=470.00, marketing=454.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=178.93, marketing=192.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=470.00 - BATNA=291.07 = surplus=178.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=454.00 - BATNA=261.96 = surplus=192.04
âœ… Calculated utility_surplus: {'model_a': 178.9325, 'model_b': 192.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 470, 'model_b': 454}
Metrics: {'utility_surplus': {'model_a': 178.9325, 'model_b': 192.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 179 completed successfully
Thu Nov 20 12:23:26 PM CET 2025: Completed iteration 179

=== Iteration 180/300 ===
Thu Nov 20 12:23:26 PM CET 2025: Starting resource allocation negotiation iteration 180
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.82s (total since entry 5.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.75s (total since entry 10.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 18}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 18}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 18.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=18 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 180 completed successfully
Thu Nov 20 12:24:09 PM CET 2025: Completed iteration 180

=== Iteration 181/300 ===
Thu Nov 20 12:24:09 PM CET 2025: Starting resource allocation negotiation iteration 181
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.84s (total since entry 5.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.93s (total since entry 10.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 181 completed successfully
Thu Nov 20 12:24:45 PM CET 2025: Completed iteration 181

=== Iteration 182/300 ===
Thu Nov 20 12:24:45 PM CET 2025: Starting resource allocation negotiation iteration 182
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.86s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.07s (total since entry 5.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=26 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=33, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=402.00, marketing=382.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=115.30, marketing=123.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=382.00 - BATNA=258.03 = surplus=123.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=402.00 - BATNA=286.70 = surplus=115.30
âœ… Calculated utility_surplus: {'model_a': 123.96866125000003, 'model_b': 115.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 402, 'model_a': 382}
Metrics: {'utility_surplus': {'model_a': 123.96866125000003, 'model_b': 115.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 182 completed successfully
Thu Nov 20 12:25:34 PM CET 2025: Completed iteration 182

=== Iteration 183/300 ===
Thu Nov 20 12:25:34 PM CET 2025: Starting resource allocation negotiation iteration 183
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.79s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.24s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=270.00 = surplus=130.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=300.00 = surplus=140.00
âœ… Calculated utility_surplus: {'model_a': 130.0, 'model_b': 140.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 130.0, 'model_b': 140.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 183 completed successfully
Thu Nov 20 12:25:59 PM CET 2025: Completed iteration 183

=== Iteration 184/300 ===
Thu Nov 20 12:25:59 PM CET 2025: Starting resource allocation negotiation iteration 184
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.11s (total since entry 5.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.90s (total since entry 13.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 184 completed successfully
Thu Nov 20 12:26:34 PM CET 2025: Completed iteration 184

=== Iteration 185/300 ===
Thu Nov 20 12:26:34 PM CET 2025: Starting resource allocation negotiation iteration 185
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.85s (total since entry 5.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.27s (total since entry 11.80s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=286.70 = surplus=125.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=258.03 = surplus=127.97
âœ… Calculated utility_surplus: {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 185 completed successfully
Thu Nov 20 12:27:18 PM CET 2025: Completed iteration 185

=== Iteration 186/300 ===
Thu Nov 20 12:27:18 PM CET 2025: Starting resource allocation negotiation iteration 186
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.78s (total since entry 10.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=24 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=31, CPU=24
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=392.00, marketing=378.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.93, marketing=116.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=378.00 - BATNA=261.96 = surplus=116.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=392.00 - BATNA=291.07 = surplus=100.93
âœ… Calculated utility_surplus: {'model_a': 116.03924999999998, 'model_b': 100.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 392, 'model_a': 378}
Metrics: {'utility_surplus': {'model_a': 116.03924999999998, 'model_b': 100.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 186 completed successfully
Thu Nov 20 12:27:59 PM CET 2025: Completed iteration 186

=== Iteration 187/300 ===
Thu Nov 20 12:27:59 PM CET 2025: Starting resource allocation negotiation iteration 187
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.83s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.10s (total since entry 5.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.74s (total since entry 10.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=270.00 = surplus=130.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=300.00 = surplus=140.00
âœ… Calculated utility_surplus: {'model_a': 130.0, 'model_b': 140.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 130.0, 'model_b': 140.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 187 completed successfully
Thu Nov 20 12:28:28 PM CET 2025: Completed iteration 187

=== Iteration 188/300 ===
Thu Nov 20 12:28:28 PM CET 2025: Starting resource allocation negotiation iteration 188
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.84s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.10s (total since entry 6.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.32s (total since entry 13.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 29,\n  "cpu_hours": 26\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 29,\n  "cpu_hours": 26\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 29,\n  "cpu_hours": 26\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 29.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=28, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=28, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=386.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=94.93, marketing=122.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=384.00 - BATNA=261.96 = surplus=122.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=291.07 = surplus=94.93
âœ… Calculated utility_surplus: {'model_a': 122.03924999999998, 'model_b': 94.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 386, 'model_a': 384}
Metrics: {'utility_surplus': {'model_a': 122.03924999999998, 'model_b': 94.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 188 completed successfully
Thu Nov 20 12:29:10 PM CET 2025: Completed iteration 188

=== Iteration 189/300 ===
Thu Nov 20 12:29:10 PM CET 2025: Starting resource allocation negotiation iteration 189
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.80s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.10s (total since entry 6.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 12.39s (total since entry 13.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=60, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=360.00, marketing=340.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=73.30, marketing=81.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=340.00 - BATNA=258.03 = surplus=81.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=286.70 = surplus=73.30
âœ… Calculated utility_surplus: {'model_a': 81.96866125000003, 'model_b': 73.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 360, 'model_a': 340}
Metrics: {'utility_surplus': {'model_a': 81.96866125000003, 'model_b': 73.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 189 completed successfully
Thu Nov 20 12:30:01 PM CET 2025: Completed iteration 189

=== Iteration 190/300 ===
Thu Nov 20 12:30:01 PM CET 2025: Starting resource allocation negotiation iteration 190
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.87s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 6.47s (total since entry 7.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.19s (total since entry 10.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=291.07 = surplus=138.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=261.96 = surplus=148.04
âœ… Calculated utility_surplus: {'model_a': 138.9325, 'model_b': 148.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 138.9325, 'model_b': 148.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 190 completed successfully
Thu Nov 20 12:30:34 PM CET 2025: Completed iteration 190

=== Iteration 191/300 ===
Thu Nov 20 12:30:34 PM CET 2025: Starting resource allocation negotiation iteration 191
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.48s (total since entry 5.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.38s (total since entry 11.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=300.00 = surplus=140.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=270.00 = surplus=130.00
âœ… Calculated utility_surplus: {'model_a': 140.0, 'model_b': 130.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 140.0, 'model_b': 130.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 191 completed successfully
Thu Nov 20 12:31:02 PM CET 2025: Completed iteration 191

=== Iteration 192/300 ===
Thu Nov 20 12:31:02 PM CET 2025: Starting resource allocation negotiation iteration 192
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.31s (total since entry 4.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.85s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 192 completed successfully
Thu Nov 20 12:31:35 PM CET 2025: Completed iteration 192

=== Iteration 193/300 ===
Thu Nov 20 12:31:35 PM CET 2025: Starting resource allocation negotiation iteration 193
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.42s (total since entry 4.98s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.84s (total since entry 9.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=20, CPU=40 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=20, CPU=40
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=440.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.00, marketing=170.00
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=300.00 = surplus=100.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=270.00 = surplus=170.00
âœ… Calculated utility_surplus: {'model_a': 100.0, 'model_b': 170.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_a': 400, 'model_b': 440}
Metrics: {'utility_surplus': {'model_a': 100.0, 'model_b': 170.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 193 completed successfully
Thu Nov 20 12:32:00 PM CET 2025: Completed iteration 193

=== Iteration 194/300 ===
Thu Nov 20 12:32:00 PM CET 2025: Starting resource allocation negotiation iteration 194
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.49s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.37s (total since entry 11.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=50, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=291.07 = surplus=148.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=261.96 = surplus=138.04
âœ… Calculated utility_surplus: {'model_a': 148.9325, 'model_b': 138.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 148.9325, 'model_b': 138.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 194 completed successfully
Thu Nov 20 12:32:35 PM CET 2025: Completed iteration 194

=== Iteration 195/300 ===
Thu Nov 20 12:32:35 PM CET 2025: Starting resource allocation negotiation iteration 195
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.30s (total since entry 4.83s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.61s (total since entry 11.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 21}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 21}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 21}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 21}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 21}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 21.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=21 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=21 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=34, CPU=21
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=398.00, marketing=372.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=106.93, marketing=110.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=372.00 - BATNA=261.96 = surplus=110.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=398.00 - BATNA=291.07 = surplus=106.93
âœ… Calculated utility_surplus: {'model_a': 110.03924999999998, 'model_b': 106.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 398, 'model_a': 372}
Metrics: {'utility_surplus': {'model_a': 110.03924999999998, 'model_b': 106.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 195 completed successfully
Thu Nov 20 12:33:06 PM CET 2025: Completed iteration 195

=== Iteration 196/300 ===
Thu Nov 20 12:33:06 PM CET 2025: Starting resource allocation negotiation iteration 196
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.85s (total since entry 9.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 196 completed successfully
Thu Nov 20 12:33:46 PM CET 2025: Completed iteration 196

=== Iteration 197/300 ===
Thu Nov 20 12:33:46 PM CET 2025: Starting resource allocation negotiation iteration 197
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.49s (total since entry 5.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 197 completed successfully
Thu Nov 20 12:34:20 PM CET 2025: Completed iteration 197

=== Iteration 198/300 ===
Thu Nov 20 12:34:20 PM CET 2025: Starting resource allocation negotiation iteration 198
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.06s (total since entry 5.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.61s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.92s (total since entry 9.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=50, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=50, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=50, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=520.00, marketing=460.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=228.93, marketing=198.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=520.00 - BATNA=291.07 = surplus=228.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=460.00 - BATNA=261.96 = surplus=198.04
âœ… Calculated utility_surplus: {'model_a': 228.9325, 'model_b': 198.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 520, 'model_b': 460}
Metrics: {'utility_surplus': {'model_a': 228.9325, 'model_b': 198.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 198 completed successfully
Thu Nov 20 12:35:00 PM CET 2025: Completed iteration 198

=== Iteration 199/300 ===
Thu Nov 20 12:35:00 PM CET 2025: Starting resource allocation negotiation iteration 199
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.48s (total since entry 6.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.73s (total since entry 11.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=25 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=134.50, marketing=144.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=295.50 = surplus=134.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=265.95 = surplus=144.05
âœ… Calculated utility_surplus: {'model_a': 134.5, 'model_b': 144.05}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 134.5, 'model_b': 144.05}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 199 completed successfully
Thu Nov 20 12:35:32 PM CET 2025: Completed iteration 199

=== Iteration 200/300 ===
Thu Nov 20 12:35:32 PM CET 2025: Starting resource allocation negotiation iteration 200
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.59s (total since entry 5.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.06s (total since entry 10.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 32.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=32 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 29.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=29, CPU=26 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 200 completed successfully
Thu Nov 20 12:36:12 PM CET 2025: Completed iteration 200

=== Iteration 201/300 ===
Thu Nov 20 12:36:12 PM CET 2025: Starting resource allocation negotiation iteration 201
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.42s (total since entry 6.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.08s (total since entry 11.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 5,\n  "cpu_hours": 5\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 5,\n  "cpu_hours": 5\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 5,\n  "cpu_hours": 5\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 21}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 21}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 21}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 21}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 21}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 21.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=21 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=488.00, marketing=464.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=196.93, marketing=202.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=464.00 - BATNA=261.96 = surplus=202.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=488.00 - BATNA=291.07 = surplus=196.93
âœ… Calculated utility_surplus: {'model_a': 202.03924999999998, 'model_b': 196.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 488, 'model_a': 464}
Metrics: {'utility_surplus': {'model_a': 202.03924999999998, 'model_b': 196.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 201 completed successfully
Thu Nov 20 12:36:48 PM CET 2025: Completed iteration 201

=== Iteration 202/300 ===
Thu Nov 20 12:36:48 PM CET 2025: Starting resource allocation negotiation iteration 202
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.71s (total since entry 6.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.28s (total since entry 10.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=258.03 = surplus=157.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=424.00 - BATNA=286.70 = surplus=137.30
âœ… Calculated utility_surplus: {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 424, 'model_a': 416}
Metrics: {'utility_surplus': {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 202 completed successfully
Thu Nov 20 12:37:39 PM CET 2025: Completed iteration 202

=== Iteration 203/300 ===
Thu Nov 20 12:37:39 PM CET 2025: Starting resource allocation negotiation iteration 203
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.05s (total since entry 5.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 203 completed successfully
Thu Nov 20 12:38:13 PM CET 2025: Completed iteration 203

=== Iteration 204/300 ===
Thu Nov 20 12:38:13 PM CET 2025: Starting resource allocation negotiation iteration 204
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.79s (total since entry 6.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.57s (total since entry 10.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=360.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=98.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=291.07 = surplus=118.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=261.96 = surplus=98.04
âœ… Calculated utility_surplus: {'model_a': 118.9325, 'model_b': 98.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 410, 'model_b': 360}
Metrics: {'utility_surplus': {'model_a': 118.9325, 'model_b': 98.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 204 completed successfully
Thu Nov 20 12:38:47 PM CET 2025: Completed iteration 204

=== Iteration 205/300 ===
Thu Nov 20 12:38:47 PM CET 2025: Starting resource allocation negotiation iteration 205
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.54s (total since entry 6.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.41s (total since entry 11.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=442.00, marketing=426.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=150.93, marketing=164.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=261.96 = surplus=164.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=442.00 - BATNA=291.07 = surplus=150.93
âœ… Calculated utility_surplus: {'model_a': 164.03924999999998, 'model_b': 150.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 442, 'model_a': 426}
Metrics: {'utility_surplus': {'model_a': 164.03924999999998, 'model_b': 150.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 205 completed successfully
Thu Nov 20 12:39:25 PM CET 2025: Completed iteration 205

=== Iteration 206/300 ===
Thu Nov 20 12:39:25 PM CET 2025: Starting resource allocation negotiation iteration 206
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.56s (total since entry 6.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.57s (total since entry 12.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 206 completed successfully
Thu Nov 20 12:40:05 PM CET 2025: Completed iteration 206

=== Iteration 207/300 ===
Thu Nov 20 12:40:05 PM CET 2025: Starting resource allocation negotiation iteration 207
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.50s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.61s (total since entry 12.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23.5 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=23.5 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=23.5
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=397.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=105.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=261.96 = surplus=118.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=397.00 - BATNA=291.07 = surplus=105.93
âœ… Calculated utility_surplus: {'model_a': 118.03924999999998, 'model_b': 105.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 397.0, 'model_a': 380.0}
Metrics: {'utility_surplus': {'model_a': 118.03924999999998, 'model_b': 105.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 207 completed successfully
Thu Nov 20 12:40:47 PM CET 2025: Completed iteration 207

=== Iteration 208/300 ===
Thu Nov 20 12:40:47 PM CET 2025: Starting resource allocation negotiation iteration 208
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.39s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.21s (total since entry 10.64s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 208 completed successfully
Thu Nov 20 12:41:26 PM CET 2025: Completed iteration 208

=== Iteration 209/300 ===
Thu Nov 20 12:41:26 PM CET 2025: Starting resource allocation negotiation iteration 209
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.52s (total since entry 9.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=124.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=386.00 - BATNA=261.96 = surplus=124.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=291.07 = surplus=120.93
âœ… Calculated utility_surplus: {'model_a': 124.03924999999998, 'model_b': 120.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 412, 'model_a': 386}
Metrics: {'utility_surplus': {'model_a': 124.03924999999998, 'model_b': 120.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 209 completed successfully
Thu Nov 20 12:42:02 PM CET 2025: Completed iteration 209

=== Iteration 210/300 ===
Thu Nov 20 12:42:02 PM CET 2025: Starting resource allocation negotiation iteration 210
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.73s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 5.11s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=45, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=158.93, marketing=128.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=261.96 = surplus=128.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=291.07 = surplus=158.93
âœ… Calculated utility_surplus: {'model_a': 128.03924999999998, 'model_b': 158.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 128.03924999999998, 'model_b': 158.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 210 completed successfully
Thu Nov 20 12:42:37 PM CET 2025: Completed iteration 210

=== Iteration 211/300 ===
Thu Nov 20 12:42:37 PM CET 2025: Starting resource allocation negotiation iteration 211
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.40s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.69s (total since entry 11.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=25 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=134.50, marketing=144.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=295.50 = surplus=134.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=265.95 = surplus=144.05
âœ… Calculated utility_surplus: {'model_a': 134.5, 'model_b': 144.05}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 134.5, 'model_b': 144.05}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 211 completed successfully
Thu Nov 20 12:43:04 PM CET 2025: Completed iteration 211

=== Iteration 212/300 ===
Thu Nov 20 12:43:04 PM CET 2025: Starting resource allocation negotiation iteration 212
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.36s (total since entry 9.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=442.00, marketing=426.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=155.30, marketing=167.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=442.00 - BATNA=286.70 = surplus=155.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=426.00 - BATNA=258.03 = surplus=167.97
âœ… Calculated utility_surplus: {'model_a': 155.29851250000002, 'model_b': 167.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 442, 'model_b': 426}
Metrics: {'utility_surplus': {'model_a': 155.29851250000002, 'model_b': 167.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 212 completed successfully
Thu Nov 20 12:43:37 PM CET 2025: Completed iteration 212

=== Iteration 213/300 ===
Thu Nov 20 12:43:37 PM CET 2025: Starting resource allocation negotiation iteration 213
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.80s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 5.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.59s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.79s (total since entry 9.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=442.00, marketing=426.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=155.30, marketing=167.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=258.03 = surplus=167.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=442.00 - BATNA=286.70 = surplus=155.30
âœ… Calculated utility_surplus: {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 442, 'model_a': 426}
Metrics: {'utility_surplus': {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 213 completed successfully
Thu Nov 20 12:44:11 PM CET 2025: Completed iteration 213

=== Iteration 214/300 ===
Thu Nov 20 12:44:11 PM CET 2025: Starting resource allocation negotiation iteration 214
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.09s (total since entry 5.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.79s (total since entry 9.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=124.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=291.07 = surplus=120.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=261.96 = surplus=124.04
âœ… Calculated utility_surplus: {'model_a': 120.9325, 'model_b': 124.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 120.9325, 'model_b': 124.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 214 completed successfully
Thu Nov 20 12:44:49 PM CET 2025: Completed iteration 214

=== Iteration 215/300 ===
Thu Nov 20 12:44:49 PM CET 2025: Starting resource allocation negotiation iteration 215
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.38s (total since entry 6.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.39s (total since entry 11.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type" : "accept"\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type" : "accept"\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type" : "accept"\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 215 completed successfully
Thu Nov 20 12:45:27 PM CET 2025: Completed iteration 215

=== Iteration 216/300 ===
Thu Nov 20 12:45:27 PM CET 2025: Starting resource allocation negotiation iteration 216
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.41s (total since entry 9.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=258.03 = surplus=141.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=286.70 = surplus=153.30
âœ… Calculated utility_surplus: {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 216 completed successfully
Thu Nov 20 12:46:08 PM CET 2025: Completed iteration 216

=== Iteration 217/300 ===
Thu Nov 20 12:46:08 PM CET 2025: Starting resource allocation negotiation iteration 217
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 5.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 217 completed successfully
Thu Nov 20 12:46:42 PM CET 2025: Completed iteration 217

=== Iteration 218/300 ===
Thu Nov 20 12:46:42 PM CET 2025: Starting resource allocation negotiation iteration 218
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 218 completed successfully
Thu Nov 20 12:47:14 PM CET 2025: Completed iteration 218

=== Iteration 219/300 ===
Thu Nov 20 12:47:14 PM CET 2025: Starting resource allocation negotiation iteration 219
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.20s (total since entry 5.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.97s (total since entry 9.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=480.00, marketing=430.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=193.30, marketing=171.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=258.03 = surplus=171.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=480.00 - BATNA=286.70 = surplus=193.30
âœ… Calculated utility_surplus: {'model_a': 171.96866125000003, 'model_b': 193.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 480, 'model_a': 430}
Metrics: {'utility_surplus': {'model_a': 171.96866125000003, 'model_b': 193.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 219 completed successfully
Thu Nov 20 12:47:51 PM CET 2025: Completed iteration 219

=== Iteration 220/300 ===
Thu Nov 20 12:47:51 PM CET 2025: Starting resource allocation negotiation iteration 220
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.94s (total since entry 6.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.88s (total since entry 9.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 18.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=18 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 18.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=37, CPU=18 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=37, CPU=18
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=404.00, marketing=366.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=112.93, marketing=104.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=366.00 - BATNA=261.96 = surplus=104.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=404.00 - BATNA=291.07 = surplus=112.93
âœ… Calculated utility_surplus: {'model_a': 104.03924999999998, 'model_b': 112.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 404, 'model_a': 366}
Metrics: {'utility_surplus': {'model_a': 104.03924999999998, 'model_b': 112.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 220 completed successfully
Thu Nov 20 12:48:27 PM CET 2025: Completed iteration 220

=== Iteration 221/300 ===
Thu Nov 20 12:48:27 PM CET 2025: Starting resource allocation negotiation iteration 221
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.43s (total since entry 6.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.51s (total since entry 12.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=31, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=31, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=386.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=99.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=286.70 = surplus=99.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 99.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 386, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 99.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 221 completed successfully
Thu Nov 20 12:49:06 PM CET 2025: Completed iteration 221

=== Iteration 222/300 ===
Thu Nov 20 12:49:06 PM CET 2025: Starting resource allocation negotiation iteration 222
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.46s (total since entry 5.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.18s (total since entry 10.63s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 222 completed successfully
Thu Nov 20 12:49:42 PM CET 2025: Completed iteration 222

=== Iteration 223/300 ===
Thu Nov 20 12:49:42 PM CET 2025: Starting resource allocation negotiation iteration 223
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.42s (total since entry 4.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.84s (total since entry 9.55s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=24 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 27, "cpu_hours": 32}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 27, 'cpu_hours': 32}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 27.0, 'cpu_hours': 32.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=27, CPU=32 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=430.00 - BATNA=286.70 = surplus=143.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=258.03 = surplus=151.97
âœ… Calculated utility_surplus: {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 430, 'model_b': 410}
Metrics: {'utility_surplus': {'model_a': 143.29851250000002, 'model_b': 151.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 223 completed successfully
Thu Nov 20 12:50:21 PM CET 2025: Completed iteration 223

=== Iteration 224/300 ===
Thu Nov 20 12:50:21 PM CET 2025: Starting resource allocation negotiation iteration 224
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.45s (total since entry 4.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=402.00, marketing=396.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=110.93, marketing=134.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=402.00 - BATNA=291.07 = surplus=110.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=396.00 - BATNA=261.96 = surplus=134.04
âœ… Calculated utility_surplus: {'model_a': 110.9325, 'model_b': 134.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 402, 'model_b': 396}
Metrics: {'utility_surplus': {'model_a': 110.9325, 'model_b': 134.03924999999998}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 224 completed successfully
Thu Nov 20 12:50:54 PM CET 2025: Completed iteration 224

=== Iteration 225/300 ===
Thu Nov 20 12:50:54 PM CET 2025: Starting resource allocation negotiation iteration 225
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.59s (total since entry 6.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.02s (total since entry 10.54s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
â° Maximum rounds (5) reached - Ending negotiation
ğŸ² [ROLE DEBUG] No Agreement - Development=model_b, Marketing=model_a
ğŸ² [ROLE DEBUG] model_b utility=0, model_a utility=0
âœ… Calculated utility_surplus: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 0.0, 'model_b': 0.0}
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: False
Metrics: {'utility_surplus': {'model_a': 0.0, 'model_b': 0.0}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 0.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 225 completed successfully
Thu Nov 20 12:51:39 PM CET 2025: Completed iteration 225

=== Iteration 226/300 ===
Thu Nov 20 12:51:39 PM CET 2025: Starting resource allocation negotiation iteration 226
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.64s (total since entry 6.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.83s (total since entry 9.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 27.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 226 completed successfully
Thu Nov 20 12:52:23 PM CET 2025: Completed iteration 226

=== Iteration 227/300 ===
Thu Nov 20 12:52:23 PM CET 2025: Starting resource allocation negotiation iteration 227
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.34s (total since entry 5.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.73s (total since entry 11.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 32}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 32}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 32.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=28, CPU=32 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=258.03 = surplus=111.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=286.70 = surplus=113.30
âœ… Calculated utility_surplus: {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 111.96866125000003, 'model_b': 113.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 227 completed successfully
Thu Nov 20 12:53:05 PM CET 2025: Completed iteration 227

=== Iteration 228/300 ===
Thu Nov 20 12:53:05 PM CET 2025: Starting resource allocation negotiation iteration 228
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.63s (total since entry 6.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.83s (total since entry 10.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 228 completed successfully
Thu Nov 20 12:53:33 PM CET 2025: Completed iteration 228

=== Iteration 229/300 ===
Thu Nov 20 12:53:33 PM CET 2025: Starting resource allocation negotiation iteration 229
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.57s (total since entry 5.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.13s (total since entry 11.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 229 completed successfully
Thu Nov 20 12:54:11 PM CET 2025: Completed iteration 229

=== Iteration 230/300 ===
Thu Nov 20 12:54:11 PM CET 2025: Starting resource allocation negotiation iteration 230
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.48s (total since entry 9.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.00, marketing=100.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=270.00 = surplus=100.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=300.00 = surplus=100.00
âœ… Calculated utility_surplus: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 100.0, 'model_b': 100.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 230 completed successfully
Thu Nov 20 12:54:33 PM CET 2025: Completed iteration 230

=== Iteration 231/300 ===
Thu Nov 20 12:54:33 PM CET 2025: Starting resource allocation negotiation iteration 231
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.69s (total since entry 6.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.95s (total since entry 9.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 231 completed successfully
Thu Nov 20 12:55:12 PM CET 2025: Completed iteration 231

=== Iteration 232/300 ===
Thu Nov 20 12:55:12 PM CET 2025: Starting resource allocation negotiation iteration 232
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.39s (total since entry 6.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.72s (total since entry 11.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 23\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 23\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 33,\n  "cpu_hours": 23\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=261.96 = surplus=118.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=291.07 = surplus=98.93
âœ… Calculated utility_surplus: {'model_a': 118.03924999999998, 'model_b': 98.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 118.03924999999998, 'model_b': 98.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 232 completed successfully
Thu Nov 20 12:55:50 PM CET 2025: Completed iteration 232

=== Iteration 233/300 ===
Thu Nov 20 12:55:50 PM CET 2025: Starting resource allocation negotiation iteration 233
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.04s (total since entry 9.66s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=26 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=34, CPU=26 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=34, CPU=26
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=428.00, marketing=412.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=141.30, marketing=153.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=428.00 - BATNA=286.70 = surplus=141.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=412.00 - BATNA=258.03 = surplus=153.97
âœ… Calculated utility_surplus: {'model_a': 141.29851250000002, 'model_b': 153.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 428, 'model_b': 412}
Metrics: {'utility_surplus': {'model_a': 141.29851250000002, 'model_b': 153.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 233 completed successfully
Thu Nov 20 12:56:28 PM CET 2025: Completed iteration 233

=== Iteration 234/300 ===
Thu Nov 20 12:56:28 PM CET 2025: Starting resource allocation negotiation iteration 234
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.25s (total since entry 5.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.22s (total since entry 11.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 55, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 55, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 55, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 55, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 55, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 55.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=55, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=163.30, marketing=131.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=258.03 = surplus=131.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=286.70 = surplus=163.30
âœ… Calculated utility_surplus: {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 234 completed successfully
Thu Nov 20 12:57:11 PM CET 2025: Completed iteration 234

=== Iteration 235/300 ===
Thu Nov 20 12:57:11 PM CET 2025: Starting resource allocation negotiation iteration 235
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.12s (total since entry 5.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.04s (total since entry 11.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 235 completed successfully
Thu Nov 20 12:57:45 PM CET 2025: Completed iteration 235

=== Iteration 236/300 ===
Thu Nov 20 12:57:45 PM CET 2025: Starting resource allocation negotiation iteration 236
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.37s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=148.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=261.96 = surplus=148.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=430.00 - BATNA=291.07 = surplus=138.93
âœ… Calculated utility_surplus: {'model_a': 148.03924999999998, 'model_b': 138.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 430, 'model_a': 410}
Metrics: {'utility_surplus': {'model_a': 148.03924999999998, 'model_b': 138.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 236 completed successfully
Thu Nov 20 12:58:23 PM CET 2025: Completed iteration 236

=== Iteration 237/300 ===
Thu Nov 20 12:58:23 PM CET 2025: Starting resource allocation negotiation iteration 237
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.79s (total since entry 9.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=23 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=418.00, marketing=394.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=126.93, marketing=132.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=418.00 - BATNA=291.07 = surplus=126.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=394.00 - BATNA=261.96 = surplus=132.04
âœ… Calculated utility_surplus: {'model_a': 126.9325, 'model_b': 132.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 418, 'model_b': 394}
Metrics: {'utility_surplus': {'model_a': 126.9325, 'model_b': 132.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 237 completed successfully
Thu Nov 20 12:58:56 PM CET 2025: Completed iteration 237

=== Iteration 238/300 ===
Thu Nov 20 12:58:56 PM CET 2025: Starting resource allocation negotiation iteration 238
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.77s (total since entry 10.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 42,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 42,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 42,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 42, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 42, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 42.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=42, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 238 completed successfully
Thu Nov 20 12:59:33 PM CET 2025: Completed iteration 238

=== Iteration 239/300 ===
Thu Nov 20 12:59:33 PM CET 2025: Starting resource allocation negotiation iteration 239
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 239 completed successfully
Thu Nov 20 01:00:10 PM CET 2025: Completed iteration 239

=== Iteration 240/300 ===
Thu Nov 20 01:00:10 PM CET 2025: Starting resource allocation negotiation iteration 240
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.72s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.55s (total since entry 6.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 240 completed successfully
Thu Nov 20 01:00:45 PM CET 2025: Completed iteration 240

=== Iteration 241/300 ===
Thu Nov 20 01:00:45 PM CET 2025: Starting resource allocation negotiation iteration 241
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.41s (total since entry 4.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.24s (total since entry 11.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=370.00, marketing=330.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=83.30, marketing=71.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=330.00 - BATNA=258.03 = surplus=71.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=286.70 = surplus=83.30
âœ… Calculated utility_surplus: {'model_a': 71.96866125000003, 'model_b': 83.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 370, 'model_a': 330}
Metrics: {'utility_surplus': {'model_a': 71.96866125000003, 'model_b': 83.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 241 completed successfully
Thu Nov 20 01:01:29 PM CET 2025: Completed iteration 241

=== Iteration 242/300 ===
Thu Nov 20 01:01:29 PM CET 2025: Starting resource allocation negotiation iteration 242
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.74s (total since entry 5.28s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.03s (total since entry 11.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=440.00 - BATNA=286.70 = surplus=153.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=258.03 = surplus=141.97
âœ… Calculated utility_surplus: {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 440, 'model_b': 400}
Metrics: {'utility_surplus': {'model_a': 153.29851250000002, 'model_b': 141.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 242 completed successfully
Thu Nov 20 01:02:01 PM CET 2025: Completed iteration 242

=== Iteration 243/300 ===
Thu Nov 20 01:02:01 PM CET 2025: Starting resource allocation negotiation iteration 243
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.75s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 5.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.34s (total since entry 10.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 50.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=50 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 243 completed successfully
Thu Nov 20 01:02:44 PM CET 2025: Completed iteration 243

=== Iteration 244/300 ===
Thu Nov 20 01:02:44 PM CET 2025: Starting resource allocation negotiation iteration 244
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.89s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.81s (total since entry 9.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 244 completed successfully
Thu Nov 20 01:03:22 PM CET 2025: Completed iteration 244

=== Iteration 245/300 ===
Thu Nov 20 01:03:22 PM CET 2025: Starting resource allocation negotiation iteration 245
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 245 completed successfully
Thu Nov 20 01:03:53 PM CET 2025: Completed iteration 245

=== Iteration 246/300 ===
Thu Nov 20 01:03:53 PM CET 2025: Starting resource allocation negotiation iteration 246
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.23s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 18}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 18}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 18.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=18 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=153.30, marketing=141.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=258.03 = surplus=141.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=286.70 = surplus=153.30
âœ… Calculated utility_surplus: {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 141.96866125000003, 'model_b': 153.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 246 completed successfully
Thu Nov 20 01:04:23 PM CET 2025: Completed iteration 246

=== Iteration 247/300 ===
Thu Nov 20 01:04:23 PM CET 2025: Starting resource allocation negotiation iteration 247
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.31s (total since entry 5.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.05s (total since entry 11.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 18}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 18}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 18.0}
ğŸ’¡ Player model_b made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=18 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 39, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 39, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 39.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=39, CPU=23 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=30, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=360.00, marketing=340.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=73.30, marketing=81.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=340.00 - BATNA=258.03 = surplus=81.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=286.70 = surplus=73.30
âœ… Calculated utility_surplus: {'model_a': 81.96866125000003, 'model_b': 73.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 360, 'model_a': 340}
Metrics: {'utility_surplus': {'model_a': 81.96866125000003, 'model_b': 73.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 247 completed successfully
Thu Nov 20 01:05:07 PM CET 2025: Completed iteration 247

=== Iteration 248/300 ===
Thu Nov 20 01:05:07 PM CET 2025: Starting resource allocation negotiation iteration 248
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.80s (total since entry 5.34s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.31s (total since entry 11.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 20,\n  "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 20,\n  "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 20,\n  "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 15\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 248 completed successfully
Thu Nov 20 01:05:41 PM CET 2025: Completed iteration 248

=== Iteration 249/300 ===
Thu Nov 20 01:05:41 PM CET 2025: Starting resource allocation negotiation iteration 249
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=448.00, marketing=434.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=161.30, marketing=175.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=434.00 - BATNA=258.03 = surplus=175.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=448.00 - BATNA=286.70 = surplus=161.30
âœ… Calculated utility_surplus: {'model_a': 175.96866125000003, 'model_b': 161.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 448, 'model_a': 434}
Metrics: {'utility_surplus': {'model_a': 175.96866125000003, 'model_b': 161.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 249 completed successfully
Thu Nov 20 01:06:21 PM CET 2025: Completed iteration 249

=== Iteration 250/300 ===
Thu Nov 20 01:06:21 PM CET 2025: Starting resource allocation negotiation iteration 250
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.05s (total since entry 9.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=410.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=143.30, marketing=151.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=258.03 = surplus=151.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=430.00 - BATNA=286.70 = surplus=143.30
âœ… Calculated utility_surplus: {'model_a': 151.96866125000003, 'model_b': 143.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 430, 'model_a': 410}
Metrics: {'utility_surplus': {'model_a': 151.96866125000003, 'model_b': 143.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 250 completed successfully
Thu Nov 20 01:07:00 PM CET 2025: Completed iteration 250

=== Iteration 251/300 ===
Thu Nov 20 01:07:00 PM CET 2025: Starting resource allocation negotiation iteration 251
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 5.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=30 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=50, CPU=10 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=50, CPU=10
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=460.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=173.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=460.00 - BATNA=286.70 = surplus=173.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 173.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 460, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 173.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 251 completed successfully
Thu Nov 20 01:07:50 PM CET 2025: Completed iteration 251

=== Iteration 252/300 ===
Thu Nov 20 01:07:50 PM CET 2025: Starting resource allocation negotiation iteration 252
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 252 completed successfully
Thu Nov 20 01:08:21 PM CET 2025: Completed iteration 252

=== Iteration 253/300 ===
Thu Nov 20 01:08:21 PM CET 2025: Starting resource allocation negotiation iteration 253
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 5.00s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.73s (total since entry 9.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=30, CPU=25 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=94.50, marketing=114.05
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=265.95 = surplus=114.05
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=295.50 = surplus=94.50
âœ… Calculated utility_surplus: {'model_a': 114.05000000000001, 'model_b': 94.5}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 50.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 114.05000000000001, 'model_b': 94.5}, 'risk_minimization': {'model_a': 100.0, 'model_b': 50.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 253 completed successfully
Thu Nov 20 01:08:48 PM CET 2025: Completed iteration 253

=== Iteration 254/300 ===
Thu Nov 20 01:08:48 PM CET 2025: Starting resource allocation negotiation iteration 254
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.36s (total since entry 6.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.03s (total since entry 11.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=60, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=488.00, marketing=464.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=196.93, marketing=202.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=464.00 - BATNA=261.96 = surplus=202.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=488.00 - BATNA=291.07 = surplus=196.93
âœ… Calculated utility_surplus: {'model_a': 202.03924999999998, 'model_b': 196.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 488, 'model_a': 464}
Metrics: {'utility_surplus': {'model_a': 202.03924999999998, 'model_b': 196.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 254 completed successfully
Thu Nov 20 01:09:27 PM CET 2025: Completed iteration 254

=== Iteration 255/300 ===
Thu Nov 20 01:09:27 PM CET 2025: Starting resource allocation negotiation iteration 255
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.88s (total since entry 5.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.09s (total since entry 11.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=265.95 = surplus=104.05
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=295.50 = surplus=104.50
âœ… Calculated utility_surplus: {'model_a': 104.05000000000001, 'model_b': 104.5}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 104.05000000000001, 'model_b': 104.5}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 255 completed successfully
Thu Nov 20 01:09:57 PM CET 2025: Completed iteration 255

=== Iteration 256/300 ===
Thu Nov 20 01:09:57 PM CET 2025: Starting resource allocation negotiation iteration 256
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.45s (total since entry 4.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.24s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 256 completed successfully
Thu Nov 20 01:10:27 PM CET 2025: Completed iteration 256

=== Iteration 257/300 ===
Thu Nov 20 01:10:27 PM CET 2025: Starting resource allocation negotiation iteration 257
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.85s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.47s (total since entry 6.33s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=24 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=33, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=37, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=33, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=396.00, marketing=374.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=109.30, marketing=115.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=396.00 - BATNA=286.70 = surplus=109.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=374.00 - BATNA=258.03 = surplus=115.97
âœ… Calculated utility_surplus: {'model_a': 109.29851250000002, 'model_b': 115.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 396, 'model_b': 374}
Metrics: {'utility_surplus': {'model_a': 109.29851250000002, 'model_b': 115.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 257 completed successfully
Thu Nov 20 01:11:10 PM CET 2025: Completed iteration 257

=== Iteration 258/300 ===
Thu Nov 20 01:11:10 PM CET 2025: Starting resource allocation negotiation iteration 258
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.93s (total since entry 5.61s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=360.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=98.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=360.00 - BATNA=261.96 = surplus=98.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=410.00 - BATNA=291.07 = surplus=118.93
âœ… Calculated utility_surplus: {'model_a': 98.03924999999998, 'model_b': 118.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 410, 'model_a': 360}
Metrics: {'utility_surplus': {'model_a': 98.03924999999998, 'model_b': 118.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 258 completed successfully
Thu Nov 20 01:11:48 PM CET 2025: Completed iteration 258

=== Iteration 259/300 ===
Thu Nov 20 01:11:48 PM CET 2025: Starting resource allocation negotiation iteration 259
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.62s (total since entry 6.29s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.11s (total since entry 9.65s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=35 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=402.00, marketing=396.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=110.93, marketing=134.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=402.00 - BATNA=291.07 = surplus=110.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=396.00 - BATNA=261.96 = surplus=134.04
âœ… Calculated utility_surplus: {'model_a': 110.9325, 'model_b': 134.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 402, 'model_b': 396}
Metrics: {'utility_surplus': {'model_a': 110.9325, 'model_b': 134.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 259 completed successfully
Thu Nov 20 01:12:19 PM CET 2025: Completed iteration 259

=== Iteration 260/300 ===
Thu Nov 20 01:12:19 PM CET 2025: Starting resource allocation negotiation iteration 260
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.92s (total since entry 5.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.04s (total since entry 11.57s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=32, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=32, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=418.00, marketing=408.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=131.30, marketing=149.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=418.00 - BATNA=286.70 = surplus=131.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=408.00 - BATNA=258.03 = surplus=149.97
âœ… Calculated utility_surplus: {'model_a': 131.29851250000002, 'model_b': 149.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 418, 'model_b': 408}
Metrics: {'utility_surplus': {'model_a': 131.29851250000002, 'model_b': 149.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 260 completed successfully
Thu Nov 20 01:13:03 PM CET 2025: Completed iteration 260

=== Iteration 261/300 ===
Thu Nov 20 01:13:03 PM CET 2025: Starting resource allocation negotiation iteration 261
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.90s (total since entry 5.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.33s (total since entry 11.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 261 completed successfully
Thu Nov 20 01:13:35 PM CET 2025: Completed iteration 261

=== Iteration 262/300 ===
Thu Nov 20 01:13:35 PM CET 2025: Starting resource allocation negotiation iteration 262
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.40s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.83s (total since entry 11.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=60, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=448.00, marketing=434.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=156.93, marketing=172.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=434.00 - BATNA=261.96 = surplus=172.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=448.00 - BATNA=291.07 = surplus=156.93
âœ… Calculated utility_surplus: {'model_a': 172.03924999999998, 'model_b': 156.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 448, 'model_a': 434}
Metrics: {'utility_surplus': {'model_a': 172.03924999999998, 'model_b': 156.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 262 completed successfully
Thu Nov 20 01:14:14 PM CET 2025: Completed iteration 262

=== Iteration 263/300 ===
Thu Nov 20 01:14:14 PM CET 2025: Starting resource allocation negotiation iteration 263
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.89s (total since entry 5.43s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.31s (total since entry 11.84s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 60, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 60, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 60.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=60, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=50, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=50, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=50, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=50, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=520.00, marketing=460.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=233.30, marketing=201.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=460.00 - BATNA=258.03 = surplus=201.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=520.00 - BATNA=286.70 = surplus=233.30
âœ… Calculated utility_surplus: {'model_a': 201.96866125000003, 'model_b': 233.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 520, 'model_a': 460}
Metrics: {'utility_surplus': {'model_a': 201.96866125000003, 'model_b': 233.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 263 completed successfully
Thu Nov 20 01:14:46 PM CET 2025: Completed iteration 263

=== Iteration 264/300 ===
Thu Nov 20 01:14:46 PM CET 2025: Starting resource allocation negotiation iteration 264
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.41s (total since entry 4.95s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.49s (total since entry 9.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=40, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=140.00, marketing=130.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=270.00 = surplus=130.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=300.00 = surplus=140.00
âœ… Calculated utility_surplus: {'model_a': 130.0, 'model_b': 140.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 130.0, 'model_b': 140.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 264 completed successfully
Thu Nov 20 01:15:08 PM CET 2025: Completed iteration 264

=== Iteration 265/300 ===
Thu Nov 20 01:15:08 PM CET 2025: Starting resource allocation negotiation iteration 265
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.65s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.71s (total since entry 6.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.88s (total since entry 9.40s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 2: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 2
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=295.50, marketing=265.95
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.50, marketing=104.05
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=295.50 = surplus=104.50
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=265.95 = surplus=104.05
âœ… Calculated utility_surplus: {'model_a': 104.5, 'model_b': 104.05000000000001}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 104.5, 'model_b': 104.05000000000001}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 265 completed successfully
Thu Nov 20 01:15:35 PM CET 2025: Completed iteration 265

=== Iteration 266/300 ===
Thu Nov 20 01:15:35 PM CET 2025: Starting resource allocation negotiation iteration 266
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.57s (total since entry 5.10s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.44s (total since entry 11.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=132.93, marketing=154.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=424.00 - BATNA=291.07 = surplus=132.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=416.00 - BATNA=261.96 = surplus=154.04
âœ… Calculated utility_surplus: {'model_a': 132.9325, 'model_b': 154.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 424, 'model_b': 416}
Metrics: {'utility_surplus': {'model_a': 132.9325, 'model_b': 154.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 266 completed successfully
Thu Nov 20 01:16:13 PM CET 2025: Completed iteration 266

=== Iteration 267/300 ===
Thu Nov 20 01:16:13 PM CET 2025: Starting resource allocation negotiation iteration 267
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.69s (total since entry 10.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 50.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=50 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=40, CPU=15 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=410.00, marketing=360.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=118.93, marketing=98.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=410.00 - BATNA=291.07 = surplus=118.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=360.00 - BATNA=261.96 = surplus=98.04
âœ… Calculated utility_surplus: {'model_a': 118.9325, 'model_b': 98.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 410, 'model_b': 360}
Metrics: {'utility_surplus': {'model_a': 118.9325, 'model_b': 98.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 267 completed successfully
Thu Nov 20 01:16:47 PM CET 2025: Completed iteration 267

=== Iteration 268/300 ===
Thu Nov 20 01:16:47 PM CET 2025: Starting resource allocation negotiation iteration 268
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.50s (total since entry 5.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.26s (total since entry 9.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 268 completed successfully
Thu Nov 20 01:17:30 PM CET 2025: Completed iteration 268

=== Iteration 269/300 ===
Thu Nov 20 01:17:30 PM CET 2025: Starting resource allocation negotiation iteration 269
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.22s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 25,\n    "cpu_hours": 35\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 25,\n    "cpu_hours": 35\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 25,\n    "cpu_hours": 35\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=286.70 = surplus=125.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=258.03 = surplus=127.97
âœ… Calculated utility_surplus: {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 269 completed successfully
Thu Nov 20 01:18:08 PM CET 2025: Completed iteration 269

=== Iteration 270/300 ===
Thu Nov 20 01:18:08 PM CET 2025: Starting resource allocation negotiation iteration 270
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.94s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.74s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 27.5\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27.5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=27.5 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=163.30, marketing=131.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=450.00 - BATNA=286.70 = surplus=163.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=258.03 = surplus=131.97
âœ… Calculated utility_surplus: {'model_a': 163.29851250000002, 'model_b': 131.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 450, 'model_b': 390}
Metrics: {'utility_surplus': {'model_a': 163.29851250000002, 'model_b': 131.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 270 completed successfully
Thu Nov 20 01:18:44 PM CET 2025: Completed iteration 270

=== Iteration 271/300 ===
Thu Nov 20 01:18:44 PM CET 2025: Starting resource allocation negotiation iteration 271
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.62s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.39s (total since entry 6.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.78s (total since entry 11.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 50}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 50}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 50.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=50 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 70}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 70}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 70}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 70}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 70}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 70.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=70 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 271 completed successfully
Thu Nov 20 01:19:20 PM CET 2025: Completed iteration 271

=== Iteration 272/300 ===
Thu Nov 20 01:19:20 PM CET 2025: Starting resource allocation negotiation iteration 272
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.60s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.29s (total since entry 5.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.30s (total since entry 11.82s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=28 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=488.00, marketing=464.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=196.93, marketing=202.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=464.00 - BATNA=261.96 = surplus=202.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=488.00 - BATNA=291.07 = surplus=196.93
âœ… Calculated utility_surplus: {'model_a': 202.03924999999998, 'model_b': 196.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 488, 'model_a': 464}
Metrics: {'utility_surplus': {'model_a': 202.03924999999998, 'model_b': 196.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 272 completed successfully
Thu Nov 20 01:19:57 PM CET 2025: Completed iteration 272

=== Iteration 273/300 ===
Thu Nov 20 01:19:57 PM CET 2025: Starting resource allocation negotiation iteration 273
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.22s (total since entry 5.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.37s (total since entry 11.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=120.93, marketing=124.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=291.07 = surplus=120.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=261.96 = surplus=124.04
âœ… Calculated utility_surplus: {'model_a': 120.9325, 'model_b': 124.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 120.9325, 'model_b': 124.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 273 completed successfully
Thu Nov 20 01:20:40 PM CET 2025: Completed iteration 273

=== Iteration 274/300 ===
Thu Nov 20 01:20:40 PM CET 2025: Starting resource allocation negotiation iteration 274
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.64s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.43s (total since entry 6.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.54s (total since entry 11.07s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 38, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 38, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 38.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=38, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 274 completed successfully
Thu Nov 20 01:21:20 PM CET 2025: Completed iteration 274

=== Iteration 275/300 ===
Thu Nov 20 01:21:20 PM CET 2025: Starting resource allocation negotiation iteration 275
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.73s (total since entry 6.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.58s (total since entry 12.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 26.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=26 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Cleaned response: ''
ğŸ” [DEBUG] [model_b] Raw LLM response: ''
âŒ [DEBUG] [model_b] Could not parse action from response: 
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 275 completed successfully
Thu Nov 20 01:22:05 PM CET 2025: Completed iteration 275

=== Iteration 276/300 ===
Thu Nov 20 01:22:05 PM CET 2025: Starting resource allocation negotiation iteration 276
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.53s (total since entry 6.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.72s (total since entry 12.16s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=30 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=30 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=30
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=460.00, marketing=450.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=168.93, marketing=188.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=460.00 - BATNA=291.07 = surplus=168.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=261.96 = surplus=188.04
âœ… Calculated utility_surplus: {'model_a': 168.9325, 'model_b': 188.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 460, 'model_b': 450}
Metrics: {'utility_surplus': {'model_a': 168.9325, 'model_b': 188.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 276 completed successfully
Thu Nov 20 01:22:41 PM CET 2025: Completed iteration 276

=== Iteration 277/300 ===
Thu Nov 20 01:22:41 PM CET 2025: Starting resource allocation negotiation iteration 277
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.88s (total since entry 11.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 27\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 27\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 27\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=27 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=442.00, marketing=426.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=155.30, marketing=167.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=426.00 - BATNA=258.03 = surplus=167.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=442.00 - BATNA=286.70 = surplus=155.30
âœ… Calculated utility_surplus: {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 442, 'model_a': 426}
Metrics: {'utility_surplus': {'model_a': 167.96866125000003, 'model_b': 155.29851250000002}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 277 completed successfully
Thu Nov 20 01:23:22 PM CET 2025: Completed iteration 277

=== Iteration 278/300 ===
Thu Nov 20 01:23:22 PM CET 2025: Starting resource allocation negotiation iteration 278
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.88s (total since entry 10.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 70, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 30.0}
âš ï¸ Player model_b offer violates constraints: GPU=70, CPU=30
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 0}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 0}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 0}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 0}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 0}
[DEBUG] Validation error: Invalid action type 'offer' for game 'resource_allocation'
[DEBUG] Raw response: {"type": "offer", "gpu_hours": 50, "cpu_hours": 0}
âš ï¸ [DEBUG] [model_b] Pydantic validation failed, using original: Action validation failed: Invalid action type 'offer' for game 'resource_allocation'
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 70,\n    "cpu_hours": 10\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 70, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 70.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=70, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=50, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=45, CPU=15 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=15
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=450.00, marketing=390.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=163.30, marketing=131.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=258.03 = surplus=131.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=450.00 - BATNA=286.70 = surplus=163.30
âœ… Calculated utility_surplus: {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 450, 'model_a': 390}
Metrics: {'utility_surplus': {'model_a': 131.96866125000003, 'model_b': 163.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 278 completed successfully
Thu Nov 20 01:24:03 PM CET 2025: Completed iteration 278

=== Iteration 279/300 ===
Thu Nov 20 01:24:03 PM CET 2025: Starting resource allocation negotiation iteration 279
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.39s (total since entry 4.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.59s (total since entry 12.04s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 279 completed successfully
Thu Nov 20 01:24:42 PM CET 2025: Completed iteration 279

=== Iteration 280/300 ===
Thu Nov 20 01:24:42 PM CET 2025: Starting resource allocation negotiation iteration 280
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.64s (total since entry 5.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.44s (total since entry 11.97s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 30,\n  "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 29, "cpu_hours": 26}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 29, 'cpu_hours': 26}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 29.0, 'cpu_hours': 26.0}
âš ï¸ Player model_a tried to make offer but exceeded proposal limit (4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=380.00 - BATNA=258.03 = surplus=121.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=390.00 - BATNA=286.70 = surplus=103.30
âœ… Calculated utility_surplus: {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 390, 'model_a': 380}
Metrics: {'utility_surplus': {'model_a': 121.96866125000003, 'model_b': 103.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 280 completed successfully
Thu Nov 20 01:25:22 PM CET 2025: Completed iteration 280

=== Iteration 281/300 ===
Thu Nov 20 01:25:22 PM CET 2025: Starting resource allocation negotiation iteration 281
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.47s (total since entry 5.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.01s (total since entry 10.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 40,\n    "cpu_hours": 15\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 281 completed successfully
Thu Nov 20 01:25:57 PM CET 2025: Completed iteration 281

=== Iteration 282/300 ===
Thu Nov 20 01:25:57 PM CET 2025: Starting resource allocation negotiation iteration 282
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.53s (total since entry 5.08s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.32s (total since entry 11.76s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 17}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 17}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 17}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 17}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 17}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 17.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=10 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=17 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=35, CPU=17 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=17
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=382.00, marketing=346.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=90.93, marketing=84.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=382.00 - BATNA=291.07 = surplus=90.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=346.00 - BATNA=261.96 = surplus=84.04
âœ… Calculated utility_surplus: {'model_a': 90.9325, 'model_b': 84.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 382, 'model_b': 346}
Metrics: {'utility_surplus': {'model_a': 90.9325, 'model_b': 84.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 282 completed successfully
Thu Nov 20 01:26:34 PM CET 2025: Completed iteration 282

=== Iteration 283/300 ===
Thu Nov 20 01:26:34 PM CET 2025: Starting resource allocation negotiation iteration 283
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.84s (total since entry 5.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.39s (total since entry 11.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 30\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 30\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 40,\n  "cpu_hours": 30\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=15 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=45, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=45, CPU=20 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=45, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=480.00, marketing=430.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=193.30, marketing=171.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=286.70 = surplus=193.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=430.00 - BATNA=258.03 = surplus=171.97
âœ… Calculated utility_surplus: {'model_a': 193.29851250000002, 'model_b': 171.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 480, 'model_b': 430}
Metrics: {'utility_surplus': {'model_a': 193.29851250000002, 'model_b': 171.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 283 completed successfully
Thu Nov 20 01:27:12 PM CET 2025: Completed iteration 283

=== Iteration 284/300 ===
Thu Nov 20 01:27:13 PM CET 2025: Starting resource allocation negotiation iteration 284
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.80s (total since entry 9.25s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 27}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 27}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 27.0}
ğŸ’¡ Player model_a made offer GPU=37, CPU=23 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=27 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 284 completed successfully
Thu Nov 20 01:27:43 PM CET 2025: Completed iteration 284

=== Iteration 285/300 ===
Thu Nov 20 01:27:43 PM CET 2025: Starting resource allocation negotiation iteration 285
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.05s (total since entry 5.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
3, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.77s (total since entry 9.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 22}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 22}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 34, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 34, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 34.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=32, CPU=22 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=34, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 23}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 23}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 23.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 36, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 36, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 36.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=33, CPU=23 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=36, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=33, CPU=23 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=33, CPU=23
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=402.00, marketing=382.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=115.30, marketing=123.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=402.00 - BATNA=286.70 = surplus=115.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=382.00 - BATNA=258.03 = surplus=123.97
âœ… Calculated utility_surplus: {'model_a': 115.29851250000002, 'model_b': 123.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 402, 'model_b': 382}
Metrics: {'utility_surplus': {'model_a': 115.29851250000002, 'model_b': 123.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 285 completed successfully
Thu Nov 20 01:28:21 PM CET 2025: Completed iteration 285

=== Iteration 286/300 ===
Thu Nov 20 01:28:21 PM CET 2025: Starting resource allocation negotiation iteration 286
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.40s (total since entry 6.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.64s (total since entry 11.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=98.93, marketing=118.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=291.07 = surplus=98.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=261.96 = surplus=118.04
âœ… Calculated utility_surplus: {'model_a': 98.9325, 'model_b': 118.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 98.9325, 'model_b': 118.03924999999998}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 286 completed successfully
Thu Nov 20 01:29:00 PM CET 2025: Completed iteration 286

=== Iteration 287/300 ===
Thu Nov 20 01:29:00 PM CET 2025: Starting resource allocation negotiation iteration 287
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.99s (total since entry 5.52s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.49s (total since entry 12.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=261.96 = surplus=108.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=291.07 = surplus=108.93
âœ… Calculated utility_surplus: {'model_a': 108.03924999999998, 'model_b': 108.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 108.03924999999998, 'model_b': 108.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 287 completed successfully
Thu Nov 20 01:29:34 PM CET 2025: Completed iteration 287

=== Iteration 288/300 ===
Thu Nov 20 01:29:34 PM CET 2025: Starting resource allocation negotiation iteration 288
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.38s (total since entry 4.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.86s (total since entry 9.30s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 30,\n    "cpu_hours": 25\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=258.03 = surplus=157.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=424.00 - BATNA=286.70 = surplus=137.30
âœ… Calculated utility_surplus: {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 424, 'model_a': 416}
Metrics: {'utility_surplus': {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 288 completed successfully
Thu Nov 20 01:30:10 PM CET 2025: Completed iteration 288

=== Iteration 289/300 ===
Thu Nov 20 01:30:10 PM CET 2025: Starting resource allocation negotiation iteration 289
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.76s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 1: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 1
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=300.00, marketing=270.00
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=100.00, marketing=100.00
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=370.00 - BATNA=270.00 = surplus=100.00
ğŸ’° [UTILITY SURPLUS] model_b: utility=400.00 - BATNA=300.00 = surplus=100.00
âœ… Calculated utility_surplus: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 1
Final utilities: {'model_b': 400, 'model_a': 370}
Metrics: {'utility_surplus': {'model_a': 100.0, 'model_b': 100.0}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 289 completed successfully
Thu Nov 20 01:30:32 PM CET 2025: Completed iteration 289

=== Iteration 290/300 ===
Thu Nov 20 01:30:32 PM CET 2025: Starting resource allocation negotiation iteration 290
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.41s (total since entry 4.96s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.21s (total since entry 11.66s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n  "type": "offer",\n  "gpu_hours": 35,\n  "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=22 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=412.00, marketing=386.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=125.30, marketing=127.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=412.00 - BATNA=286.70 = surplus=125.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=258.03 = surplus=127.97
âœ… Calculated utility_surplus: {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 412, 'model_b': 386}
Metrics: {'utility_surplus': {'model_a': 125.29851250000002, 'model_b': 127.96866125000003}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 290 completed successfully
Thu Nov 20 01:31:09 PM CET 2025: Completed iteration 290

=== Iteration 291/300 ===
Thu Nov 20 01:31:09 PM CET 2025: Starting resource allocation negotiation iteration 291
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.91s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.42s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.78s (total since entry 9.20s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 4/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=113.30, marketing=111.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=286.70 = surplus=113.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=258.03 = surplus=111.97
âœ… Calculated utility_surplus: {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 113.29851250000002, 'model_b': 111.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 291 completed successfully
Thu Nov 20 01:31:48 PM CET 2025: Completed iteration 291

=== Iteration 292/300 ===
Thu Nov 20 01:31:48 PM CET 2025: Starting resource allocation negotiation iteration 292
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.60s (total since entry 5.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.75s (total since entry 9.19s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 45, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 45, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 45.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22.5 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=45, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=40, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=25 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=30 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=30
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=500.00, marketing=480.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=208.93, marketing=218.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=261.96 = surplus=218.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=500.00 - BATNA=291.07 = surplus=208.93
âœ… Calculated utility_surplus: {'model_a': 218.03924999999998, 'model_b': 208.9325}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 500, 'model_a': 480}
Metrics: {'utility_surplus': {'model_a': 218.03924999999998, 'model_b': 208.9325}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 292 completed successfully
Thu Nov 20 01:32:20 PM CET 2025: Completed iteration 292

=== Iteration 293/300 ===
Thu Nov 20 01:32:20 PM CET 2025: Starting resource allocation negotiation iteration 293
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.67s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.72s (total since entry 6.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.99s (total since entry 9.51s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 28, "cpu_hours": 27}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 28, 'cpu_hours': 27}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 28.0, 'cpu_hours': 27.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=28, CPU=27 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 24.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=28, CPU=27 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=28, CPU=27
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=386.00, marketing=384.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=94.93, marketing=122.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=384.00 - BATNA=261.96 = surplus=122.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=386.00 - BATNA=291.07 = surplus=94.93
âœ… Calculated utility_surplus: {'model_a': 122.03924999999998, 'model_b': 94.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 386, 'model_a': 384}
Metrics: {'utility_surplus': {'model_a': 122.03924999999998, 'model_b': 94.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 293 completed successfully
Thu Nov 20 01:32:55 PM CET 2025: Completed iteration 293

=== Iteration 294/300 ===
Thu Nov 20 01:32:55 PM CET 2025: Starting resource allocation negotiation iteration 294
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.93s (total since entry 5.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.34s (total since entry 11.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22.5}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22.5}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.5}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=40, CPU=20 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=40, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=440.00, marketing=400.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=148.93, marketing=138.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=261.96 = surplus=138.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=440.00 - BATNA=291.07 = surplus=148.93
âœ… Calculated utility_surplus: {'model_a': 138.03924999999998, 'model_b': 148.9325}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 440, 'model_a': 400}
Metrics: {'utility_surplus': {'model_a': 138.03924999999998, 'model_b': 148.9325}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 294 completed successfully
Thu Nov 20 01:33:27 PM CET 2025: Completed iteration 294

=== Iteration 295/300 ===
Thu Nov 20 01:33:27 PM CET 2025: Starting resource allocation negotiation iteration 295
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.35s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.81s (total since entry 9.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 40.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=40 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{\n    "type": "offer",\n    "gpu_hours": 35,\n    "cpu_hours": 20\n}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 3/4)
ğŸ‰ CONVERGENCE! Both players offered GPU=35, CPU=20 - Creating automatic agreement!
ğŸ” [RESOURCE DEBUG] Agreement in round 3: GPU=35, CPU=20
ğŸ” [BATNA DEBUG] Using BATNA from round 3
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=400.00, marketing=370.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=108.93, marketing=108.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=400.00 - BATNA=291.07 = surplus=108.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=370.00 - BATNA=261.96 = surplus=108.04
âœ… Calculated utility_surplus: {'model_a': 108.9325, 'model_b': 108.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 400, 'model_b': 370}
Metrics: {'utility_surplus': {'model_a': 108.9325, 'model_b': 108.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 295 completed successfully
Thu Nov 20 01:33:55 PM CET 2025: Completed iteration 295

=== Iteration 296/300 ===
Thu Nov 20 01:33:55 PM CET 2025: Starting resource allocation negotiation iteration 296
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.72s (total since entry 6.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.80s (total since entry 11.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 35}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 35}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 35.0}
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=40, CPU=35 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 50, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 50, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 50.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_a made offer GPU=50, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 4/4)
ğŸ’¡ Player model_b made offer GPU=35, CPU=15 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=32, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=32, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=424.00, marketing=416.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=137.30, marketing=157.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=416.00 - BATNA=258.03 = surplus=157.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=424.00 - BATNA=286.70 = surplus=137.30
âœ… Calculated utility_surplus: {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 75.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 424, 'model_a': 416}
Metrics: {'utility_surplus': {'model_a': 157.96866125000003, 'model_b': 137.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 75.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 296 completed successfully
Thu Nov 20 01:34:40 PM CET 2025: Completed iteration 296

=== Iteration 297/300 ===
Thu Nov 20 01:34:40 PM CET 2025: Starting resource allocation negotiation iteration 297
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.41s (total since entry 6.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.89s (total since entry 11.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_b made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 25, "cpu_hours": 35}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 25, 'cpu_hours': 35}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 25.0, 'cpu_hours': 35.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ’¡ Player model_b made offer GPU=25, CPU=35 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 30}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 30}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 30.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 23}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 23}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 23.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=30 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=23 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 31, "cpu_hours": 24}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 31, 'cpu_hours': 24}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 31.0, 'cpu_hours': 24.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=31, CPU=24 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_a accepted offer of GPU=30, CPU=25 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=30, CPU=25
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=390.00, marketing=380.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=103.30, marketing=121.97
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=390.00 - BATNA=286.70 = surplus=103.30
ğŸ’° [UTILITY SURPLUS] model_b: utility=380.00 - BATNA=258.03 = surplus=121.97
âœ… Calculated utility_surplus: {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}
âœ… Calculated risk_minimization: {'model_a': 75.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_a': 390, 'model_b': 380}
Metrics: {'utility_surplus': {'model_a': 103.29851250000002, 'model_b': 121.96866125000003}, 'risk_minimization': {'model_a': 75.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 297 completed successfully
Thu Nov 20 01:35:19 PM CET 2025: Completed iteration 297

=== Iteration 298/300 ===
Thu Nov 20 01:35:19 PM CET 2025: Starting resource allocation negotiation iteration 298
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.48s (total since entry 6.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.49s (total since entry 12.02s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_a = DEVELOPMENT, model_b = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 15}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 15}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 15.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=20 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=15 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 32, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 32, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 32.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=20 (proposal 2/4)
ğŸ’¡ Player model_a made offer GPU=32, CPU=28 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 33, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 33, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 33.0, 'cpu_hours': 22.0}
ğŸ’¡ Player model_b made offer GPU=35, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=33, CPU=22 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=33, CPU=22 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=33, CPU=22
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=396.00, marketing=374.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=104.93, marketing=112.04
ğŸ² [ROLE DEBUG] Development=model_a, Marketing=model_b
ğŸ’° [UTILITY SURPLUS] model_a: utility=396.00 - BATNA=291.07 = surplus=104.93
ğŸ’° [UTILITY SURPLUS] model_b: utility=374.00 - BATNA=261.96 = surplus=112.04
âœ… Calculated utility_surplus: {'model_a': 104.9325, 'model_b': 112.03924999999998}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] Qwen/Qwen2.5-7B beat meta-llama/Llama-3.2-3B-Instruct (player model_b won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_a': 396, 'model_b': 374}
Metrics: {'utility_surplus': {'model_a': 104.9325, 'model_b': 112.03924999999998}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 298 completed successfully
Thu Nov 20 01:35:55 PM CET 2025: Completed iteration 298

=== Iteration 299/300 ===
Thu Nov 20 01:35:55 PM CET 2025: Starting resource allocation negotiation iteration 299
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.36s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.90s (total since entry 9.34s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 30}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 30}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 30.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=30 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 22}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 22}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 22.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 10}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 10}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 10.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=22 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=40, CPU=10 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 37, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 37, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 37.0, 'cpu_hours': 20.0}
ğŸ’¡ Player model_a made offer GPU=30, CPU=25 (proposal 3/4)
ğŸ’¡ Player model_b made offer GPU=37, CPU=20 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 30, "cpu_hours": 25}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 30, 'cpu_hours': 25}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 30.0, 'cpu_hours': 25.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 28}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 28}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 28.0}
ğŸ’¡ Player model_b made offer GPU=30, CPU=25 (proposal 4/4)
ğŸ’¡ Player model_a made offer GPU=35, CPU=28 (proposal 4/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=35, CPU=28 (made in round 4)
ğŸ” [RESOURCE DEBUG] Agreement in round 5: GPU=35, CPU=28
ğŸ” [BATNA DEBUG] Using BATNA from offer round 4 (offer made), agreement in round 5
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=286.70, marketing=258.03
ğŸ” [UTILITY DEBUG] Utilities: development=448.00, marketing=434.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=161.30, marketing=175.97
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=434.00 - BATNA=258.03 = surplus=175.97
ğŸ’° [UTILITY SURPLUS] model_b: utility=448.00 - BATNA=286.70 = surplus=161.30
âœ… Calculated utility_surplus: {'model_a': 175.96866125000003, 'model_b': 161.29851250000002}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 5
Final utilities: {'model_b': 448, 'model_a': 434}
Metrics: {'utility_surplus': {'model_a': 175.96866125000003, 'model_b': 161.29851250000002}, 'risk_minimization': {'model_a': 100.0, 'model_b': 100.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 299 completed successfully
Thu Nov 20 01:36:37 PM CET 2025: Completed iteration 299

=== Iteration 300/300 ===
Thu Nov 20 01:36:37 PM CET 2025: Starting resource allocation negotiation iteration 300
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Resource Allocation Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_b': {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'Qwen/Qwen2.5-7B', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.7, 'max_length': 16, 'do_sample': True, 'trust_remote_code': True, 'load_in_8bit': False, 'api_token': '...'}}
ğŸ“ Registered model: model_b (new instance for Qwen/Qwen2.5-7B)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 16, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': False, 'load_in_8bit': True, 'api_token': '...'}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.2-3B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.90s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: Qwen/Qwen2.5-7B)
Loading Qwen/Qwen2.5-7B...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='Qwen/Qwen2.5-7B'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 8.82s (total since entry 9.26s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 21415
âœ… Qwen/Qwen2.5-7B loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ² [ROLE ASSIGNMENT] model_b = DEVELOPMENT, model_a = MARKETING
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 60}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 60}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 60.0}
âš ï¸ Player model_a offer violates constraints: GPU=40, CPU=60
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 10}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 10}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 10.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 5, "cpu_hours": 5}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 5, 'cpu_hours': 5}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 5.0, 'cpu_hours': 5.0}
ğŸ’¡ Player model_a made offer GPU=20, CPU=10 (proposal 1/4)
ğŸ’¡ Player model_b made offer GPU=5, CPU=5 (proposal 1/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 35, "cpu_hours": 15}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 35, 'cpu_hours': 15}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 35.0, 'cpu_hours': 15.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 10, "cpu_hours": 40}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 10, "cpu_hours": 40}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 10, "cpu_hours": 40}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 10, 'cpu_hours': 40}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 10, 'cpu_hours': 40}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 10.0, 'cpu_hours': 40.0}
ğŸ’¡ Player model_a made offer GPU=35, CPU=15 (proposal 2/4)
ğŸ’¡ Player model_b made offer GPU=10, CPU=40 (proposal 2/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "offer", "gpu_hours": 40, "cpu_hours": 20}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 40, 'cpu_hours': 20}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 40.0, 'cpu_hours': 20.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 45}'
ğŸ” [DEBUG] [model_a] Raw LLM response: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 45}'
ğŸ§¹ [DEBUG] [model_a] Extracted JSON from start: '{"type": "offer", "gpu_hours": 20, "cpu_hours": 45}'
âœ… [DEBUG] [model_a] Successfully parsed JSON from start: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 45}
[DEBUG] Parsed JSON: {'type': 'offer', 'gpu_hours': 20, 'cpu_hours': 45}
âœ… [DEBUG] [model_a] Pydantic validation successful: {'type': 'propose', 'gpu_hours': 20.0, 'cpu_hours': 45.0}
ğŸ’¡ Player model_b made offer GPU=40, CPU=20 (proposal 3/4)
ğŸ’¡ Player model_a made offer GPU=20, CPU=45 (proposal 3/4)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] [model_b] Raw LLM response: '{"type": "accept"}'
ğŸ§¹ [DEBUG] [model_b] Extracted JSON from start: '{"type": "accept"}'
âœ… [DEBUG] [model_b] Successfully parsed JSON from start: {'type': 'accept'}
[DEBUG] Parsed JSON: {'type': 'accept'}
âœ… [DEBUG] [model_b] Pydantic validation successful: {'type': 'accept'}
âœ… Player model_b accepted offer of GPU=20, CPU=45 (made in round 3)
ğŸ” [RESOURCE DEBUG] Agreement in round 4: GPU=20, CPU=45
ğŸ” [BATNA DEBUG] Using BATNA from offer round 3 (offer made), agreement in round 4
ğŸ” [BATNA DEBUG] Config BATNAs: development=300, marketing=270
ğŸ” [BATNA DEBUG] Decay rates: development=0.015, marketing=0.015
ğŸ” [BATNA DEBUG] Calculated BATNAs: development=291.07, marketing=261.96
ğŸ” [UTILITY DEBUG] Utilities: development=430.00, marketing=480.00
ğŸ” [SURPLUS DEBUG] Surpluses: development=138.93, marketing=218.04
ğŸ² [ROLE DEBUG] Development=model_b, Marketing=model_a
ğŸ’° [UTILITY SURPLUS] model_a: utility=480.00 - BATNA=261.96 = surplus=218.04
ğŸ’° [UTILITY SURPLUS] model_b: utility=430.00 - BATNA=291.07 = surplus=138.93
âœ… Calculated utility_surplus: {'model_a': 218.03924999999998, 'model_b': 138.9325}
âœ… Calculated risk_minimization: {'model_a': 66.66666666666666, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ† [LLM WINNER] meta-llama/Llama-3.2-3B-Instruct beat Qwen/Qwen2.5-7B (player model_a won)
ğŸ¤– [MODEL MAPPING] model_a=Llama-3.2-3B-Instruct, model_b=Qwen2.5-7B
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.2-3B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  Qwen/Qwen2.5-7B unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: Qwen/Qwen2.5-7B)
Agreement reached: True
Agreement round: 4
Final utilities: {'model_b': 430, 'model_a': 480}
Metrics: {'utility_surplus': {'model_a': 218.03924999999998, 'model_b': 138.9325}, 'risk_minimization': {'model_a': 66.66666666666666, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 300 completed successfully
Thu Nov 20 01:37:17 PM CET 2025: Completed iteration 300

=== Resource Allocation Game Batch Complete ===
Successful iterations: 300/300
Check output logs for negotiation results and final utilities
