=== Integrative Negotiations Game - Targeted Node Run ===
Job ID: 1754637, Node: jn012
=== Environment Check ===
CUDA: True, GPU: NVIDIA L40
NVIDIA L40, 49140 MiB, 1 MiB
=== Verifying Torch Scope Fix ===
âœ… Torch scope fix is present
=== Verifying Integrative Negotiations Game Updates ===
âœ… Integrative negotiations game found
=== Running Integrative Negotiations Game - 10 Iterations ===
Game: IT Team vs Marketing Team office space negotiation
Issues: Server Room Size, Meeting Access, Cleaning Responsibility, Branding Visibility

=== Iteration 1/10 ===
Mon Sep 29 04:42:24 PM CEST 2025: Starting integrative negotiations iteration 1
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 34.95s (total since entry 35.46s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 14.85s (total since entry 15.31s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 1 completed successfully
Mon Sep 29 04:43:33 PM CEST 2025: Completed iteration 1

=== Iteration 2/10 ===
Mon Sep 29 04:43:33 PM CEST 2025: Starting integrative negotiations iteration 2
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.82s (total since entry 12.32s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.96s (total since entry 5.41s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 2 completed successfully
Mon Sep 29 04:44:03 PM CEST 2025: Completed iteration 2

=== Iteration 3/10 ===
Mon Sep 29 04:44:03 PM CEST 2025: Starting integrative negotiations iteration 3
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.24s (total since entry 11.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
16, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.57s (total since entry 5.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
23, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 3 completed successfully
Mon Sep 29 04:44:33 PM CEST 2025: Completed iteration 3

=== Iteration 4/10 ===
Mon Sep 29 04:44:33 PM CEST 2025: Starting integrative negotiations iteration 4
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.24s (total since entry 11.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.63s (total since entry 5.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 4 completed successfully
Mon Sep 29 04:45:02 PM CEST 2025: Completed iteration 4

=== Iteration 5/10 ===
Mon Sep 29 04:45:02 PM CEST 2025: Starting integrative negotiations iteration 5
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.26s (total since entry 11.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.60s (total since entry 5.05s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
29, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 5 completed successfully
Mon Sep 29 04:45:32 PM CEST 2025: Completed iteration 5

=== Iteration 6/10 ===
Mon Sep 29 04:45:32 PM CEST 2025: Starting integrative negotiations iteration 6
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.30s (total since entry 11.78s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
23, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.69s (total since entry 5.14s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
18, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 6 completed successfully
Mon Sep 29 04:46:01 PM CEST 2025: Completed iteration 6

=== Iteration 7/10 ===
Mon Sep 29 04:46:01 PM CEST 2025: Starting integrative negotiations iteration 7
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.46s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.11s (total since entry 11.59s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.58s (total since entry 5.03s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
9, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 7 completed successfully
Mon Sep 29 04:46:30 PM CEST 2025: Completed iteration 7

=== Iteration 8/10 ===
Mon Sep 29 04:46:30 PM CEST 2025: Starting integrative negotiations iteration 8
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.24s (total since entry 11.72s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.44s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.64s (total since entry 5.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 8 completed successfully
Mon Sep 29 04:47:00 PM CEST 2025: Completed iteration 8

=== Iteration 9/10 ===
Mon Sep 29 04:47:00 PM CEST 2025: Starting integrative negotiations iteration 9
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.43s (total since entry 11.92s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.43s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.58s (total since entry 5.01s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
6, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 9 completed successfully
Mon Sep 29 04:47:30 PM CEST 2025: Completed iteration 9

=== Iteration 10/10 ===
Mon Sep 29 04:47:30 PM CEST 2025: Starting integrative negotiations iteration 10
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.22s (total since entry 11.73s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
5, 17823
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.45s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.64s (total since entry 5.09s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 22697
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 4,\n        "cleaning": "Shared",\n        "branding": "Moderate"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type":"propose","proposal":{"server_room":100,"meeting_access":7,"cleaning":"Outsourced","branding":"Prominent"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n  "type": "accept"\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n  "type": "accept"\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
âš ï¸  Model model_a shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_b (shared name: meta-llama/Llama-3.2-3B-Instruct)
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 36.0, 'model_b': 34.0}
Metrics: {'utility_surplus': {'model_a': 1.7000000000000028, 'model_b': 4.600000000000001}, 'risk_minimization': {'model_a': 0.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 10 completed successfully
Mon Sep 29 04:47:59 PM CEST 2025: Completed iteration 10

=== Integrative Negotiations Game Batch Complete ===
Successful iterations: 10/10
Check output logs for negotiation results and final utilities
