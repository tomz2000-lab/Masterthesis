=== Company Car Game - 10 Iterations ===
Job ID: 1754218, Node: jn120
=== Environment Check ===
CUDA: True, GPU: NVIDIA L40
NVIDIA L40, 49140 MiB, 1 MiB
=== Verifying Torch Scope Fix ===
âœ… Torch scope fix is present
=== Running Company Car Game - 10 Iterations ===
Game: Employee vs Manager company car negotiation

=== Iteration 1/10 ===
Mon Sep 29 03:59:37 PM CEST 2025: Starting company car negotiation iteration 1 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.58s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 14.90s (total since entry 15.50s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.57s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 30.96s (total since entry 31.53s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 1 completed successfully
Mon Sep 29 04:00:48 PM CEST 2025: Completed iteration 1

=== Iteration 2/10 ===
Mon Sep 29 04:00:48 PM CEST 2025: Starting company car negotiation iteration 2 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 5.12s (total since entry 5.68s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 11.05s (total since entry 11.56s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 2 completed successfully
Mon Sep 29 04:01:22 PM CEST 2025: Completed iteration 2

=== Iteration 3/10 ===
Mon Sep 29 04:01:22 PM CEST 2025: Starting company car negotiation iteration 3 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.70s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.87s (total since entry 5.58s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.99s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 3 completed successfully
Mon Sep 29 04:01:55 PM CEST 2025: Completed iteration 3

=== Iteration 4/10 ===
Mon Sep 29 04:01:55 PM CEST 2025: Starting company car negotiation iteration 4 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.70s (total since entry 5.27s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.16s (total since entry 10.81s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 4 completed successfully
Mon Sep 29 04:02:29 PM CEST 2025: Completed iteration 4

=== Iteration 5/10 ===
Mon Sep 29 04:02:29 PM CEST 2025: Starting company car negotiation iteration 5 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.55s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.92s (total since entry 5.47s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
10, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.66s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.22s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 5 completed successfully
Mon Sep 29 04:03:02 PM CEST 2025: Completed iteration 5

=== Iteration 6/10 ===
Mon Sep 29 04:03:02 PM CEST 2025: Starting company car negotiation iteration 6 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.68s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.90s (total since entry 5.60s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
8, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.86s (total since entry 11.35s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 6 completed successfully
Mon Sep 29 04:03:35 PM CEST 2025: Completed iteration 6

=== Iteration 7/10 ===
Mon Sep 29 04:03:35 PM CEST 2025: Starting company car negotiation iteration 7 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.81s (total since entry 5.38s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.63s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.25s (total since entry 10.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 7 completed successfully
Mon Sep 29 04:04:09 PM CEST 2025: Completed iteration 7

=== Iteration 8/10 ===
Mon Sep 29 04:04:09 PM CEST 2025: Starting company car negotiation iteration 8 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.54s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.94s (total since entry 5.49s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.47s (total since entry 10.99s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 8 completed successfully
Mon Sep 29 04:04:42 PM CEST 2025: Completed iteration 8

=== Iteration 9/10 ===
Mon Sep 29 04:04:42 PM CEST 2025: Starting company car negotiation iteration 9 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.87s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.86s (total since entry 5.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.38s (total since entry 10.87s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 9 completed successfully
Mon Sep 29 04:05:16 PM CEST 2025: Completed iteration 9

=== Iteration 10/10 ===
Mon Sep 29 04:05:16 PM CEST 2025: Starting company car negotiation iteration 10 on node jn120
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b', 'model_c']

=== Running Single Company Car Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b', 'model_c']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is ENABLED (consider TORCHDYNAMO_DISABLE=1 if hanging)
ğŸ”§ DISABLING TorchDynamo to prevent potential hanging...
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.79s (total since entry 5.36s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.82s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 10.11s (total since entry 10.93s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
13, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 40000}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 40000}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 40000}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 40000.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38510}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38510}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38510}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38510.0}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "offer", "price": 38224}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "offer", "price": 38224}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'offer', 'price': 38224}
âœ… [DEBUG] Pydantic validation successful: {'type': 'offer', 'price': 38224.0}
âœ… Calculated utility_surplus: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
âœ… Calculated risk_minimization: {'model_a': 0.0, 'model_b': 66.66666666666666}
âœ… Calculated deadline_sensitivity: {'model_a': 0.0, 'model_b': 0.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 0.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 3
Final utilities: {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}
Metrics: {'utility_surplus': {'model_a': 4033.5999999999985, 'model_b': 0.09999999999854481}, 'risk_minimization': {'model_a': 0.0, 'model_b': 66.66666666666666}, 'deadline_sensitivity': {'model_a': 0.0, 'model_b': 0.0}, 'feasibility': {'model_a': 1.0, 'model_b': 0.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
âœ… Iteration 10 completed successfully
Mon Sep 29 04:05:48 PM CEST 2025: Completed iteration 10

=== Company Car Game Batch Complete ===
Successful iterations: 10/10
âœ… ALL ITERATIONS SUCCESSFUL: Platform working perfectly on node jn120
=== Company Car Run Complete ===
