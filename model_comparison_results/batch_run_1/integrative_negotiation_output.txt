=== Integrative Negotiations Game - Targeted Node Run ===
Job ID: 1754226, Node: jn101
=== Environment Check ===
CUDA: True, GPU: NVIDIA L40
NVIDIA L40, 49140 MiB, 1 MiB
=== Verifying Torch Scope Fix ===
âœ… Torch scope fix is present
=== Verifying Integrative Negotiations Game Updates ===
âœ… Integrative negotiations game found
=== Running Integrative Negotiations Game - 10 Iterations ===
Game: IT Team vs Marketing Team office space negotiation
Issues: Server Room Size, Meeting Access, Cleaning Responsibility, Branding Visibility

=== Iteration 1/10 ===
Mon Sep 29 04:00:15 PM CEST 2025: Starting integrative negotiations iteration 1
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.56s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.91s (total since entry 10.48s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 23.63s (total since entry 24.13s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 1 completed successfully
Mon Sep 29 04:01:12 PM CEST 2025: Completed iteration 1

=== Iteration 2/10 ===
Mon Sep 29 04:01:12 PM CEST 2025: Starting integrative negotiations iteration 2
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.64s (total since entry 5.17s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 15.71s (total since entry 16.21s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 2 completed successfully
Mon Sep 29 04:01:52 PM CEST 2025: Completed iteration 2

=== Iteration 3/10 ===
Mon Sep 29 04:01:52 PM CEST 2025: Starting integrative negotiations iteration 3
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.37s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.21s (total since entry 9.69s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 3 completed successfully
Mon Sep 29 04:02:23 PM CEST 2025: Completed iteration 3

=== Iteration 4/10 ===
Mon Sep 29 04:02:23 PM CEST 2025: Starting integrative negotiations iteration 4
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.86s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.23s (total since entry 9.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 4 completed successfully
Mon Sep 29 04:02:55 PM CEST 2025: Completed iteration 4

=== Iteration 5/10 ===
Mon Sep 29 04:02:55 PM CEST 2025: Starting integrative negotiations iteration 5
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
2, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.47s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.22s (total since entry 9.70s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 5 completed successfully
Mon Sep 29 04:03:27 PM CEST 2025: Completed iteration 5

=== Iteration 6/10 ===
Mon Sep 29 04:03:27 PM CEST 2025: Starting integrative negotiations iteration 6
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.71s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.34s (total since entry 5.06s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.49s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.25s (total since entry 9.74s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 6 completed successfully
Mon Sep 29 04:03:58 PM CEST 2025: Completed iteration 6

=== Iteration 7/10 ===
Mon Sep 29 04:03:58 PM CEST 2025: Starting integrative negotiations iteration 7
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.23s (total since entry 9.71s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
4, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 7 completed successfully
Mon Sep 29 04:04:30 PM CEST 2025: Completed iteration 7

=== Iteration 8/10 ===
Mon Sep 29 04:04:30 PM CEST 2025: Starting integrative negotiations iteration 8
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.60s (total since entry 5.12s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.52s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.23s (total since entry 9.75s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 8 completed successfully
Mon Sep 29 04:05:02 PM CEST 2025: Completed iteration 8

=== Iteration 9/10 ===
Mon Sep 29 04:05:02 PM CEST 2025: Starting integrative negotiations iteration 9
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.53s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.33s (total since entry 4.88s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
1, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.50s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.20s (total since entry 9.70s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 9 completed successfully
Mon Sep 29 04:05:34 PM CEST 2025: Completed iteration 9

=== Iteration 10/10 ===
Mon Sep 29 04:05:34 PM CEST 2025: Starting integrative negotiations iteration 10
ğŸ”§ About to run Python command...
=== Negotiation Platform ===
Available models: ['model_a', 'model_b', 'model_c']
Using models: ['model_a', 'model_b']

=== Running Single Integrative Negotiations Negotiation ===
[DEBUG] LLMManager received model_configs: {'model_a': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_b': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}, 'model_c': {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}}
[DEBUG] Type of model_configs: <class 'dict'>
[DEBUG] Keys in model_configs: ['model_a', 'model_b', 'model_c']
[DEBUG] Attempting to register 3 models...
[DEBUG] Processing model: model_a
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_a (new instance for meta-llama/Llama-3.2-3B-Instruct)
[DEBUG] Processing model: model_b
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model: model_b (new instance for meta-llama/Llama-3.1-8B-Instruct)
[DEBUG] Processing model: model_c
[DEBUG] Model config: {'type': 'huggingface', 'model_name': 'meta-llama/Llama-3.1-8B-Instruct', 'config': {'device': 'cuda', 'device_map': 'auto', 'temperature': 0.0, 'max_length': 1024, 'do_sample': False, 'pad_token_id': 50256, 'trust_remote_code': True, 'load_in_8bit': False}}
ğŸ“ Registered model alias: model_c -> meta-llama/Llama-3.1-8B-Instruct
ğŸ“Š Registered metric: utility_surplus - Utility Surplus
ğŸ“Š Registered metric: risk_minimization - Risk Minimization
ğŸ“Š Registered metric: deadline_sensitivity - Deadline Sensitivity
ğŸ“Š Registered metric: feasibility - Feasibility
ğŸ”„ Models will be loaded on-demand: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Loading model: model_a
ğŸš€ Loading model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
Loading meta-llama/Llama-3.2-3B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.51s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.2-3B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 4.32s (total since entry 4.85s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 7381
âœ… meta-llama/Llama-3.2-3B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_a]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loading model: model_b
ğŸš€ Loading model: model_b (shared name: meta-llama/Llama-3.1-8B-Instruct)
Loading meta-llama/Llama-3.1-8B-Instruct...
ğŸ”§ TorchDynamo is DISABLED via environment variable
ğŸ”§ Environment: transformers=4.53.2, torch=2.7.1+cu126
ğŸ”§ CUDA available: True, device_count: 1
ğŸ” Starting tokenizer.from_pretrained()
âœ… Tokenizer loaded in 0.48s
ğŸ”§ Building model kwargs...
ğŸ”§ Using configured device_map: auto
âœ… Model kwargs built in 0.00s
ï¿½ Model kwargs keys: ['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map']
ğŸ”§ device_map: auto
ğŸ”§ torch_dtype: torch.float16
ğŸ”§ low_cpu_mem_usage: True
ğŸ”§ trust_remote_code: True
ï¿½ğŸ” Starting AutoModelForCausalLM.from_pretrained() with model_name='meta-llama/Llama-3.1-8B-Instruct'
ğŸ”§ About to call: AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', **['token', 'torch_dtype', 'trust_remote_code', 'low_cpu_mem_usage', 'device_map'])
ğŸ”§ Starting model load (optimized settings)...
âœ… Model loaded successfully in 9.22s (total since entry 9.70s)
ğŸ”§ [INFO] First model parameter device after load: cuda:0
ğŸ”§ [GPU] nvidia-smi output:
0, 23951
âœ… meta-llama/Llama-3.1-8B-Instruct loaded successfully
ğŸ”„ [DEBUG] Loaded model type: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Loaded model is None: False
ğŸ”„ [DEBUG] stored in loaded_agents[model_b]: <class 'negotiation_platform.models.hf_model_wrapper.HuggingFaceModelWrapper'>
ğŸ”„ [DEBUG] Final loaded_agents keys: ['model_a', 'model_b']
ğŸ”„ [DEBUG] Players list: ['model_a', 'model_b']
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "propose", "proposal": {"server_room": 100, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ” [DEBUG] Round 1 - no previous proposals to check
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
ğŸ” [DEBUG] Raw LLM response: '{\n    "type": "propose",\n    "proposal": {\n        "server_room": 100,\n        "meeting_access": 7,\n        "cleaning": "Outsourced",\n        "branding": "Prominent"\n    }\n}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
âœ… [DEBUG] Pydantic validation successful: {'type': 'propose', 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_b: {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}
ğŸ¯ [GUIDANCE] Player model_a (IT): Proposal {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}
ğŸ¯ [GUIDANCE] Utility: 33.0, BATNA: 34.3
ğŸ¯ [GUIDANCE] Decision: COUNTER-OFFER (below your BATNA)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON after prefix removal: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "counter-offer", "proposal": {"server_room": 120, "meeting_access": 4, "cleaning": "Shared", "branding": "Moderate"}}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'counter-offer', 'proposal': {'server_room': 120, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
âš ï¸ [DEBUG] Pydantic validation failed, using original: Action validation failed: Invalid action type 'counter-offer' for game 'integrative_negotiations'
ğŸ” [DEBUG] Looking for opponent proposal in round_1
ğŸ” [DEBUG] Available round_proposals keys: ['round_1']
ğŸ” [DEBUG] Previous round data: {'model_a': {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}, 'model_b': {'player': 'model_b', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 7, 'cleaning': 'Outsourced', 'branding': 'Prominent'}}}
ğŸ” [DEBUG] Found opponent proposal from model_a: {'player': 'model_a', 'round': 1, 'proposal': {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}}
ğŸ¯ [GUIDANCE] Player model_b (Marketing): Proposal {'server_room': 100, 'meeting_access': 4, 'cleaning': 'Shared', 'branding': 'Moderate'}
ğŸ¯ [GUIDANCE] Utility: 34.0, BATNA: 29.4
ğŸ¯ [GUIDANCE] Decision: ACCEPT IT (acceptable)
ğŸ”§ [DEBUG] Model device: cuda:0
ğŸ”§ [DEBUG] Moved inputs to device: cuda:0
ğŸ§¹ [DEBUG] Extracted JSON from start: '{"type": "accept"}'
ğŸ” [DEBUG] Raw LLM response: '{"type": "accept"}'
âœ… [DEBUG] Successfully parsed JSON from start: {'type': 'accept'}
âœ… [DEBUG] Pydantic validation successful: {'type': 'accept'}
âœ… Calculated utility_surplus: {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}
âœ… Calculated risk_minimization: {'model_a': 100.0, 'model_b': 0.0}
âœ… Calculated deadline_sensitivity: {'model_a': 100.0, 'model_b': 100.0}
âœ… Calculated feasibility: {'model_a': 1.0, 'model_b': 1.0}
ğŸ—‘ï¸  meta-llama/Llama-3.2-3B-Instruct unloaded
ğŸ—‘ï¸  Unloaded model: model_a (shared name: meta-llama/Llama-3.2-3B-Instruct)
âš ï¸  Model model_b shares wrapper 'meta-llama/Llama-3.1-8B-Instruct' with aliases ['model_c']; skipping unload to avoid breaking aliases
Agreement reached: True
Agreement round: 2
Final utilities: {'model_a': 24.0, 'model_b': 31.0}
Metrics: {'utility_surplus': {'model_a': -10.299999999999997, 'model_b': 1.6000000000000014}, 'risk_minimization': {'model_a': 100.0, 'model_b': 0.0}, 'deadline_sensitivity': {'model_a': 100.0, 'model_b': 100.0}, 'feasibility': {'model_a': 1.0, 'model_b': 1.0}}
ğŸ”„ Keeping models loaded for potential reuse

Platform completed successfully!
ğŸ”§ Python command exit code: 0
âœ… Iteration 10 completed successfully
Mon Sep 29 04:06:05 PM CEST 2025: Completed iteration 10

=== Integrative Negotiations Game Batch Complete ===
Successful iterations: 10/10
Check output logs for negotiation results and final utilities
